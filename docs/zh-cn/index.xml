<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Razon Yang</title>
    <link>https://xuzhijvn.github.io/zh-cn/</link>
    <description>Recent content on Razon Yang</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Razon Yang. All Rights Reserved.</copyright>
    <lastBuildDate>Fri, 27 Aug 2021 11:15:10 +0800</lastBuildDate><atom:link href="https://xuzhijvn.github.io/zh-cn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1.1 一些概念</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.1-%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/1.1-%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.1-%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/1.1-%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/</guid>
      <description>1.1.1 什么是云原生？ 云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。
 什么是云原生架构？ 采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。
什么是云原生应用？ 在架构设计、开发方式、部署维护等各个阶段和方面都基于云的特点建设的应用。
 1.1.2 什么是容器编排？ 容器编排是指自动化容器的部署、管理、扩展和联网。
 k8s周边：
 k3s: 轻量化k8s k9s: kubectl命令的封装 KubeOperator : 国产k8s发行版 Kubesphere : k8s企业级别增强（多云、多集群） kubeedge : 边缘计算 Kubeless : 面向serverless的k8s   1.1.3 什么是服务网格？ 1.1.4 k8s/istio与云原生的关系？ </description>
    </item>
    
    <item>
      <title>1.2.1 k8s功能</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.1-k8s%E5%8A%9F%E8%83%BD/k8s%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.1-k8s%E5%8A%9F%E8%83%BD/k8s%E5%8A%9F%E8%83%BD/</guid>
      <description>Kubernetes提供了一个可弹性运行分布式系统的框架。Kubernetes 会满足您的扩展要求、故障转移、部署模式等。具体如下：
 Service discovery and load balancing，服务发现和负载均衡，通过DNS实现内部解析，service实现负载均衡 Storage orchestration，存储编排，通过plungin的形式支持多种存储，如本地，nfs，ceph，公有云快存储等 Automated rollouts and rollbacks，自动发布与回滚，通过匹配当前状态与目标状态一致，更新失败时可回滚 Automatic bin packing，自动资源调度，可以设置pod调度的所需（requests）资源和限制资源（limits） Self-healing，内置的健康检查策略，自动发现和处理集群内的异常，更换，需重启的pod节点 Secret and configuration management，密钥和配置管理，对于敏感信息如密码，账号的那个通过secret存储，应用的配置文件通过configmap存储，避免将配置文件固定在镜像中，增加容器编排的灵活性 Batch execution，批处理执行，通过job和cronjob提供单次批处理任务和循环计划任务功能的实现 Horizontal scaling，横向扩展功能，包含有HPA和AS，即应用的基于CPU利用率的弹性伸缩和基于平台级的弹性伸缩，如自动增加node和删除nodes节点。   使用kubectl autoscale命令来创建一个 HPA 对象：
$ kubectl autoscale deployment wordpress --namespace kube-example --cpu-percent=20 --min=3 --max=6 horizontalpodautoscaler.autoscaling/hpa-demo autoscaled $ kubectl get hpa -n kube-example NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE wordpress Deployment/wordpress &amp;lt;unknown&amp;gt;/20% 3 6 0 13s 此命令创建了一个关联资源 wordpress 的 HPA，最小的 Pod 副本数为3，最大为6。HPA 会根据设定的 cpu 使用率（20%）动态的增加或者减少 Pod 数量。</description>
    </item>
    
    <item>
      <title>300. 最长递增子序列</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/300.-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/300.-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/</guid>
      <description>class Solution { public int lengthOfLIS(int[] nums) { if (nums == null || nums.length == 0){return 0;} int[] dp = new int[nums.length]; Arrays.fill(dp,1); for (int i = 0; i &amp;lt; nums.length; i++) { for (int j = 0; j &amp;lt; i; j++) { if(nums[i] &amp;gt; nums[j] &amp;amp;&amp;amp; dp[j] + 1 &amp;gt; dp[i]){ dp[i] = dp[j] + 1; } } } return Arrays.stream(dp).max().getAsInt(); } } 300. 最长递增子序列</description>
    </item>
    
    <item>
      <title>400 Bad Request</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/400-bad-request/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/400-bad-request/</guid>
      <description>需要设置 proxy_set_header Host $host:$server_port;
location ^~/gateway/ { proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; proxy_pass http://k8s_yshop-gateway-svc/; } </description>
    </item>
    
    <item>
      <title>5种IO模型</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/5%E7%A7%8Dio%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/5%E7%A7%8Dio%E6%A8%A1%E5%9E%8B/</guid>
      <description>1. 前言 W. Richard Stevens在 《Unix Network Programming Volume 1 3rd Edition - The Sockets Networking API》文中的6.2 I/O Models小节中对如下5中IO模型进行了详细的阐述：
 blocking I/O nonblocking I/O I/O multiplexing (select and poll) signal driven I/O (SIGIO) asynchronous I/O (the POSIX aio_functions)  由signal driven IO在实际中并不常用，所以主要介绍其余四种IO Model。
再说一下IO发生时涉及的对象和步骤。对于一个network IO (这里我们以read举例)，它会涉及到两个系统对象，一个是调用这个IO的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历两个阶段：
 等待数据准备 (Waiting for the data to be ready)（将数据从网卡读到内核） 将数据从内核拷贝到进程中(Copying the data from the kernel to the process)  记住这两点很重要，因为这些IO模型的区别就是在两个阶段上各有不同的情况。
2. 概念说明 在进行解释之前，首先要说明几个概念：</description>
    </item>
    
    <item>
      <title>API接口安全设计</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/api%E6%8E%A5%E5%8F%A3%E5%AE%89%E5%85%A8%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/api%E6%8E%A5%E5%8F%A3%E5%AE%89%E5%85%A8%E8%AE%BE%E8%AE%A1/</guid>
      <description> 验签：防接口被肆意调用  client对request签名，server对请求进行验签。
鉴权：防调用没有权限的接口  token鉴权，实际上验签、鉴权通常是一同处理的，例如：OAuth2
验证请求的timestamp：防盗链  当前时间 - timestamp &amp;gt; 阈值，则说明该请求已经失效。
nonce随机数：防重复提交  上述验证timestamp的过程中存在一个问题，例如：阈值 = 60s，那么60s内发生重复提交怎么办？
解决办法是：server端保存带随机数的请求（随机数+原请求 = 新请求，通常是redis保存）。
接口幂等性保证（唯一主键、乐观锁）  幂等性：以相同的请求调用这个接口一次和调用这个接口多次，对系统产生的影响是相同的。
恶意的重复提交：请求的timestamp、nonce较上一次没变，这种情况通过redis保存带随机数的请求可以杜绝。
用户误操作导致的重复提交：前端控制+幂等设计。
幂等与重复提交的区别在于：幂等是在重复提交已经发生了的情况下，如何保证多次调用接口的结果一致，重复提交在前，幂等保证在后。
参考链接： 如何保证接口的幂等性说说API的防重放机制Java生鲜电商平台-API接口设计之token、timestamp、sign 具体架构与实现（APP/小程序，传输安全）阿里一面：如何保证API接口数据安全？实现接口幂等性的几种方案</description>
    </item>
    
    <item>
      <title>AQS</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/aqs/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/aqs/</guid>
      <description>Java并发之AQS详解并发之AQS原理(二) CLH队列与Node解析从ReentrantLock的实现看AQS的原理及应用</description>
    </item>
    
    <item>
      <title>Async</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/async/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/async/</guid>
      <description>Spring Boot系列二 Spring @Async异步线程池用法总结springboot利用@Async提升API接口并发能力</description>
    </item>
    
    <item>
      <title>BeanFactoryPostProcessor和BeanPostProcessor</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/beanfactorypostprocessor%E5%92%8Cbeanpostprocessor/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/beanfactorypostprocessor%E5%92%8Cbeanpostprocessor/</guid>
      <description>BeanFactoryPostProcessor和BeanPostProcessor，这两个接口，都是Spring初始化bean时对外暴露的扩展点。两个接口名称看起来很相似，但作用及使用场景却不同，分析如下：
1、BeanFactoryPostProcessor接口 该接口的定义如下：
public interface BeanFactoryPostProcessor {
/** * Modify the application context&amp;#39;s internal bean factory after its standard * initialization. All bean definitions will have been loaded, but no beans * will have been instantiated yet. This allows for overriding or adding * properties even to eager-initializing beans. * @param beanFactory the bean factory used by the application context * @throws org.springframework.beans.BeansException in case of errors */ void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; } 实现该接口，可以在spring的bean创建之前，修改bean的定义属性。也就是说，Spring允许BeanFactoryPostProcessor在容器实例化任何其它bean之前读取配置元数据，并可以根据需要进行修改，例如可以把bean的scope从singleton改为prototype，也可以把property的值给修改掉。可以同时配置多个BeanFactoryPostProcessor，并通过设置&amp;rsquo;order&amp;rsquo;属性来控制各个BeanFactoryPostProcessor的执行次序。</description>
    </item>
    
    <item>
      <title>bean生命周期</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid>
      <description>实例化 -&amp;gt; 属性赋值 -&amp;gt; 初始化 -&amp;gt; 销毁 参考链接 请别再问Spring Bean的生命周期了！Spring Bean的生命周期（非常详细）</description>
    </item>
    
    <item>
      <title>cache和buffer</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/system/cache%E5%92%8Cbuffer/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/system/cache%E5%92%8Cbuffer/</guid>
      <description>首先我们使用 free -m 查看系统内存的使用情况：
可以看出，系统内存为 16G，Swap 内存 16G，mem free 虽然显示为 1118，因缓存的存在，不能认为系统目前内剩下这么多内存。而应该把 buffers、cached 的也算上，即 free+cached+buffers=1118+7110+430＝8658，总内存再减去 8658＝7314，与 buffers/cache 行中对应 free 列的 7312 和 8659 基本一致。
 这里顺便介绍一下Swap，了解Swap更多详情请移步 👉 Swap交换分区概念  Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。
 从Linux kernel version 2.4开始，buffer/cache合并（详情见 👉 Linux IO的buffer cache和page cache合并的原因），所以同样使用 free -m 看到的是：
[root@k8s-node3 ~]# free -m total used free shared buff/cache available Mem: 7820 6577 147 30 1095 907 Swap: 0 0 0  使用 hostnamectl 或者 cat /proc/version 可以查看内核版本：</description>
    </item>
    
    <item>
      <title>Caffeine</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/caffeine/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/caffeine/</guid>
      <description>1. LRU 和 LFU 的缺点  LRU 实现简单，在一般情况下能够表现出很好的命中率，是一个“性价比”很高的算法，平时也很常用。虽然 LRU 对突发性的稀疏流量（sparse bursts）表现很好，但同时也会产生缓存污染，举例来说，如果偶然性的要对全量数据进行遍历，那么“历史访问记录”就会被刷走，造成污染。 如果数据的分布在一段时间内是固定的话，那么 LFU 可以达到最高的命中率。但是 LFU 有两个缺点，第一，它需要给每个记录项维护频率信息，每次访问都需要更新，这是个巨大的开销；第二，对突发性的稀疏流量无力，因为前期经常访问的记录已经占用了缓存，偶然的流量不太可能会被保留下来，而且过去的一些大量被访问的记录在将来也不一定会使用上，这样就一直把“坑”占着了。  无论 LRU 还是 LFU 都有其各自的缺点，不过，现在已经有很多针对其缺点而改良、优化出来的变种算法。
2. TinyLFU TinyLFU 就是其中一个优化算法，它是专门为了解决 LFU 上述提到的两个问题而被设计出来的。
解决第一个问题是采用了 Count–Min Sketch 算法。
解决第二个问题是让记录尽量保持相对的“新鲜”（Freshness Mechanism），并且当有新的记录插入时，可以让它跟老的记录进行“PK”，输者就会被淘汰，这样一些老的、不再需要的记录就会被剔除。
2.1 统计频率 Count–Min Sketch 算法 如何对一个 key 进行统计，但又可以节省空间呢？（不是简单的使用HashMap，这太消耗内存了），注意哦，不需要精确的统计，只需要一个近似值就可以了，怎么样，这样场景是不是很熟悉，如果你是老司机，或许已经联想到布隆过滤器（Bloom Filter）的应用了。
没错，将要介绍的 Count–Min Sketch 的原理跟 Bloom Filter 一样，只不过 Bloom Filter 只有 0 和 1 的值，那么你可以把 Count–Min Sketch 看作是“数值”版的 Bloom Filter。
2.2 保新机制 为了让缓存保持“新鲜”，剔除掉过往频率很高但之后不经常的缓存，Caffeine 有一个 Freshness Mechanism。做法很简答，就是当整体的统计计数（当前所有记录的频率统计之和，这个数值内部维护）达到某一个值时，那么所有记录的频率统计除以 2。
3. Window Tiny LFU Caffeine 通过测试发现 TinyLFU 在面对突发性的稀疏流量（sparse bursts）时表现很差，因为新的记录（new items）还没来得及建立足够的频率就被剔除出去了，这就使得命中率下降。</description>
    </item>
    
    <item>
      <title>char、varchar、text区别</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/charvarchartext%E5%8C%BA%E5%88%AB/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/charvarchartext%E5%8C%BA%E5%88%AB/</guid>
      <description>参考 MySQL之char、varchar和text的设计</description>
    </item>
    
    <item>
      <title>ClickHouse</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/clickhouse/clickhouse/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/clickhouse/clickhouse/</guid>
      <description>列式存储优势   当查询语句只涉及部分列时，只需要扫描相关的列
  每一列的数据都是相同类型的，彼此间相关性更大，对列数据压缩的效率较高
  参考 “行式存储”和“列式存储”的区别什么是ClickHouse？</description>
    </item>
    
    <item>
      <title>CompletableFuture使用详解</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/completablefuture%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/completablefuture%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/</guid>
      <description>1、 runAsync 和 supplyAsync方法 CompletableFuture 提供了四个静态方法来创建一个异步操作。
public static CompletableFuture&amp;lt;Void&amp;gt; runAsync(Runnable runnable) public static CompletableFuture&amp;lt;Void&amp;gt; runAsync(Runnable runnable, Executor executor) public static &amp;lt;U&amp;gt; CompletableFuture&amp;lt;U&amp;gt; supplyAsync(Supplier&amp;lt;U&amp;gt; supplier) public static &amp;lt;U&amp;gt; CompletableFuture&amp;lt;U&amp;gt; supplyAsync(Supplier&amp;lt;U&amp;gt; supplier, Executor executor) 没有指定Executor的方法会使用ForkJoinPool.commonPool() 作为它的线程池执行异步代码。如果指定线程池，则使用指定的线程池运行。以下所有的方法都类同。
 runAsync方法不支持返回值。 supplyAsync可以支持返回值。  示例 //无返回值 public static void runAsync() throws Exception { CompletableFuture&amp;lt;Void&amp;gt; future = CompletableFuture.runAsync(() -&amp;gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { } System.out.println(&amp;#34;run end ...&amp;#34;); }); future.get(); } //有返回值 public static void supplyAsync() throws Exception { CompletableFuture&amp;lt;Long&amp;gt; future = CompletableFuture.</description>
    </item>
    
    <item>
      <title>CPU缓存行</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/cpu%E7%BC%93%E5%AD%98%E8%A1%8C/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/cpu%E7%BC%93%E5%AD%98%E8%A1%8C/</guid>
      <description>JAVA 拾遗 — CPU Cache 与缓存行既然CPU有缓存一致性协议（MESI），为什么JMM还需要volatile关键字？CPU缓存行Java内存访问重排序的研究</description>
    </item>
    
    <item>
      <title>dockerfile-maven-plugin使用</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/maven/dockerfile-maven-plugin%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/maven/dockerfile-maven-plugin%E4%BD%BF%E7%94%A8/</guid>
      <description>&amp;lt;properties&amp;gt; &amp;lt;docker.image.prefix&amp;gt;anaham-docker.pkg.coding.net/cereshop/ceres&amp;lt;/docker.image.prefix&amp;gt; &amp;lt;anaham-docker.username&amp;gt;用户名&amp;lt;/anaham-docker.username&amp;gt; &amp;lt;anaham-docker.password&amp;gt;密码&amp;lt;/anaham-docker.password&amp;gt; &amp;lt;/properties&amp;gt; &amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;repackage&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;!-- docker打包插件 --&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;com.spotify&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;dockerfile-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;${dockerfile-maven-plugin.version}&amp;lt;/version&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;username&amp;gt;${anaham-docker.username}&amp;lt;/username&amp;gt; &amp;lt;password&amp;gt;${anaham-docker.password}&amp;lt;/password&amp;gt; &amp;lt;repository&amp;gt;${docker.image.prefix}/${project.artifactId}&amp;lt;/repository&amp;gt; &amp;lt;tag&amp;gt;${ceres.version}&amp;lt;/tag&amp;gt; &amp;lt;!-- 不指定tag默认为latest --&amp;gt; &amp;lt;buildArgs&amp;gt; &amp;lt;JAR_FILE&amp;gt;target/${project.build.finalName}.jar&amp;lt;/JAR_FILE&amp;gt; &amp;lt;/buildArgs&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt; &amp;lt;/build&amp;gt;  构建镜像  mvn clean package -Dmaven.计算机科学.skip=true dockerfile:build -Ddockerfile.tag=latest 因为上面pom.xml已经指定了tag，也可以直接使用：mvn dockerfile:build 会优先选择pom.xml配置的tag
上传镜像  mvn dockerfile:push -Ddockerfile.username=[镜像仓库账号] -Ddockerfile.password=[镜像仓库密码] 因为上面pom.xml已经指定了username和password，也可以直接使用：mvn dockerfile:push</description>
    </item>
    
    <item>
      <title>ES综述</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/es/es%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/es/es%E7%BB%BC%E8%BF%B0/</guid>
      <description>1. ES 的分布式架构原理 ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 ES 进程实例，组成了一个 ES 集群。
默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。
1.1 ES数据结构 ES 中存储数据的基本单位是索引，比如说你现在要在 ES 中存储一些订单数据，你就应该在 ES 中创建一个索引 order_idx ，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。
index -&amp;gt; type -&amp;gt; mapping -&amp;gt; document -&amp;gt; field。1.2 ES高可用 类似kafka将一个topic分成多个partition，es将一个index分成多个shard。
 每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。
 kafka的每个partition有多个replica副本，所有副本选出一个leader（依赖zookeeper完成选举）负责所有的读写请求。
ES 的每个shard也可以配置多个replica副本，每个 shard 都有一个 primary shard ，负责写入数据，但是还有几个 replica shard 。 primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。</description>
    </item>
    
    <item>
      <title>GitHub 443</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/git/github-443/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/git/github-443/</guid>
      <description>我们真是一个神奇的国度，连github都要封禁。
最近github https无法接入：
fatal: unable to access &amp;#39;https://github.com/xuzhijvn/tony-demo.git/&amp;#39;: LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 试了n种方式都不行：
  切换SSR代理节点
  切换SSR代理模式到全局
  退出SSR
  使用蓝灯代理
  设置git代理（61885是我蓝灯代理的端口）
  git config --global --list git config --global https.proxy &amp;#39;127.0.0.1:61885&amp;#39; git config --global http.proxy &amp;#39;127.0.0.1:61885&amp;#39; git config --global --list 禁用git代理  git config --global --list git config --global --unset http.proxy git config --global --list networksetup -setv6off Wi-Fi  以上方法全部不好使，折腾了一下午心态崩了💔💔</description>
    </item>
    
    <item>
      <title>Golang的协程调度器原理及GMP设计思想</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/go/golang%E7%9A%84%E5%8D%8F%E7%A8%8B%E8%B0%83%E5%BA%A6%E5%99%A8%E5%8E%9F%E7%90%86%E5%8F%8Agmp%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/go/golang%E7%9A%84%E5%8D%8F%E7%A8%8B%E8%B0%83%E5%BA%A6%E5%99%A8%E5%8E%9F%E7%90%86%E5%8F%8Agmp%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/</guid>
      <description>本节为重点章节 本章节含视频版:
  一、Golang“调度器”的由来？ (1) 单进程时代不需要调度器 我们知道，一切的软件都是跑在操作系统上，真正用来干活(计算)的是CPU。早期的操作系统每个程序就是一个进程，直到一个程序运行完，才能进行下一个进程，就是“单进程时代”
一切的程序只能串行发生。
早期的单进程操作系统，面临2个问题：
1.单一的执行流程，计算机只能一个任务一个任务处理。
2.进程阻塞所带来的CPU时间浪费。
那么能不能有多个进程来宏观一起来执行多个任务呢？
后来操作系统就具有了最早的并发能力：多进程并发，当一个进程阻塞的时候，切换到另外等待执行的进程，这样就能尽量把CPU利用起来，CPU就不浪费了。
(2)多进程/线程时代有了调度器需求 在多进程/多线程的操作系统中，就解决了阻塞的问题，因为一个进程阻塞cpu可以立刻切换到其他进程中去执行，而且调度cpu的算法可以保证在运行的进程都可以被分配到cpu的运行时间片。这样从宏观来看，似乎多个进程是在同时被运行。
但新的问题就又出现了，进程拥有太多的资源，进程的创建、切换、销毁，都会占用很长的时间，CPU虽然利用起来了，但如果进程过多，CPU有很大的一部分都被用来进行进程调度了。
怎么才能提高CPU的利用率呢？
但是对于Linux操作系统来讲，cpu对进程的态度和线程的态度是一样的。
很明显，CPU调度切换的是进程和线程。尽管线程看起来很美好，但实际上多线程开发设计会变得更加复杂，要考虑很多同步竞争等问题，如锁、竞争冲突等。
(3)协程来提高CPU利用率 多进程、多线程已经提高了系统的并发能力，但是在当今互联网高并发场景下，为每个任务都创建一个线程是不现实的，因为会消耗大量的内存(进程虚拟内存会占用4GB[32位操作系统], 而线程也要大约4MB)。
大量的进程/线程出现了新的问题
 高内存占用 调度的高消耗CPU  好了，然后工程师们就发现，其实一个线程分为“内核态“线程和”用户态“线程。
一个“用户态线程”必须要绑定一个“内核态线程”，但是CPU并不知道有“用户态线程”的存在，它只知道它运行的是一个“内核态线程”(Linux的PCB进程控制块)。
这样，我们再去细化去分类一下，内核线程依然叫“线程(thread)”，用户线程叫“协程(co-routine)&amp;quot;.
看到这里，我们就要开脑洞了，既然一个协程(co-routine)可以绑定一个线程(thread)，那么能不能多个协程(co-routine)绑定一个或者多个线程(thread)上呢。
之后，我们就看到了有3中协程和线程的映射关系：
 N:1关系  N个协程绑定1个线程，优点就是协程在用户态线程即完成切换，不会陷入到内核态，这种切换非常的轻量快速。但也有很大的缺点，1个进程的所有协程都绑定在1个线程上
缺点：
 某个程序用不了硬件的多核加速能力 一旦某协程阻塞，造成线程阻塞，本进程的其他协程都无法执行了，根本就没有并发的能力了。   1:1 关系  1个协程绑定1个线程，这种最容易实现。协程的调度都由CPU完成了，不存在N:1缺点，
缺点：
 协程的创建、删除和切换的代价都由CPU完成，有点略显昂贵了。   M:N关系  M个协程绑定1个线程，是N:1和1:1类型的结合，克服了以上2种模型的缺点，但实现起来最为复杂。
协程跟线程是有区别的，线程由CPU调度是抢占式的，协程由用户态调度是协作式的，一个协程让出CPU后，才执行下一个协程。
(4)Go语言的协程goroutine Go为了提供更容易使用的并发方法，使用了goroutine和channel。goroutine来自协程的概念，让一组可复用的函数运行在一组线程之上，即使有协程阻塞，该线程的其他协程也可以被runtime调度，转移到其他可运行的线程上。最关键的是，程序员看不到这些底层的细节，这就降低了编程的难度，提供了更容易的并发。
Go中，协程被称为goroutine，它非常轻量，一个goroutine只占几KB，并且这几KB就足够goroutine运行完，这就能在有限的内存空间内支持大量goroutine，支持了更多的并发。虽然一个goroutine的栈只占几KB，但实际是可伸缩的，如果需要更多内容，runtime会自动为goroutine分配。
Goroutine特点：
 占用内存更小（几kb） 调度更灵活(runtime调度)  (5)被废弃的goroutine调度器 好了，既然我们知道了协程和线程的关系，那么最关键的一点就是调度协程的调度器的实现了。
Go目前使用的调度器是2012年重新设计的，因为之前的调度器性能存在问题，所以使用4年就被废弃了，那么我们先来分析一下被废弃的调度器是如何运作的？
 大部分文章都是会用G来表示Goroutine，用M来表示线程，那么我们也会用这种表达的对应关系。</description>
    </item>
    
    <item>
      <title>go多版本切换</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/go/go%E5%A4%9A%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/go/go%E5%A4%9A%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2/</guid>
      <description>方法一：调整PATH export PATH=/usr/local/go/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin 方法二：调整软链接(未验证) //查看当前软链接指向cd /usr/local/bin ls -trl | grep go//调整软链接ln -snf /usr/local/Cellar/go/1.16.3/bin go</description>
    </item>
    
    <item>
      <title>HashMap和HashTable区别</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/hashmap%E5%92%8Chashtable%E5%8C%BA%E5%88%AB/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/hashmap%E5%92%8Chashtable%E5%8C%BA%E5%88%AB/</guid>
      <description>1、HashMap是HashTable的轻量级版本， HashTable是线程安全的，其方法都被synchronized关键同步
2、 HashMap 把 Hashtable 的 contains 方法去掉了，改成 containsValue 和 containsKey。因为 contains 方法容易让人引起误解。
3、 HashMap允许将 null 作为一个 entry 的 key 或者 value，而 Hashtable 不允许。
4、HashTable 继承自 Dictionary 类，而 HashMap 是 Java1.2 引进的 Map interface 的一个实现。
5、Hashtable 和 HashMap 采用的 hash/rehash 算法都大概一样，所以性能不会有很大的差异。</description>
    </item>
    
    <item>
      <title>http2</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/http/http2/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/http/http2/</guid>
      <description>http/2在http/1系列的基础上优化了通信效率，主要得益于如下几点改进：
一、 多路复用的单一长连接 1.单一长连接 在HTTP/2中，客户端向某个域名的服务器请求页面的过程中，只会创建一条TCP连接，即使这页面可能包含上百个资源。 单一的连接应该是HTTP2的主要优势，单一的连接能减少TCP握手（还有ssl握手的开销）带来的时延 。HTTP2中用一条单一的长连接，避免了创建多个TCP连接带来的网络开销，提高了吞吐量。
2.多路复用 HTTP2虽然只有一条TCP连接，但是在逻辑上分成了很多stream。HTTP2把要传输的信息分割成一个个二进制帧，首部信息会被封装到HEADER Frame，相应的request body就放到DATA Frame,一个帧你可以看成路上的一辆车,只要给这些车编号，让1号车都走1号门出，2号车都走2号门出，就把不同的http请求或者响应区分开来了。但是，这里要求同一个请求或者响应的帧必须是有有序的，要保证FIFO的，但是不同的请求或者响应帧可以互相穿插。这就是HTTP2的多路复用，是不是充分利用了网络带宽，是不是提高了并发度？
二、头部压缩和二进制格式 http1.x一直都是plain text,对此我只能想到一个优点，便于阅读和debug。但是，现在很多都走https，SSL也把plain text变成了二进制，那这个优点也没了。 于是HTTP2搞了个HPACK压缩来压缩头部，减少报文大小(调试这样的协议将需要curl这样的工具，要进一步地分析网络数据流需要类似Wireshark的http2解析器)。
三、服务端推送Server Push 这个功能通常被称作“缓存推送”。主要的思想是：当一个客户端请求资源X，而服务器知道它很可能也需要资源Z的情况下，服务器可以在客户端发送请求前，主动将资源Z推送给客户端。这个功能帮助客户端将Z放进缓存以备将来之需。
总结  单一的长连接，减少了SSL握手的开销 头部被压缩，减少了数据传输量 多路复用能大幅提高传输效率，不用等待上一个请求的响应 不用像http1.x那样把多个文件或者资源弄成一个文件或者资源（http1.x常见的优化手段），这时候，缓存就能更容易命中啊（http1.x里面你揉成一团的东西怎么命中缓存？）  参考 HTTP/2 相比 1.0 有哪些重大改进？HTTP 2 的新特性你 get 了吗？HTTP/2 服务器推送（Server Push）教程</description>
    </item>
    
    <item>
      <title>io.lettuce.core.RedisCommandTimeoutException: Command timed out after 5 second(s)</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/exception/io.lettuce.core.rediscommandtimeoutexception_-comm/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/exception/io.lettuce.core.rediscommandtimeoutexception_-comm/</guid>
      <description>lettuce连接redis报错io.lettuce.core.RedisCommandTimeoutException: Command timed out after 5 second(s)，我的spring.redis.timeout = 5000。
解决办法：
1、登陆redis容器 2、输入redis-cli进入redis控制台 3、设置 CONFIG SET timeout &amp;quot;60&amp;quot; 4、设置 CONFIG SET tcp-keepalive &amp;quot;300&amp;quot;
参考链接：Redis 配置</description>
    </item>
    
    <item>
      <title>java agent</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java-agent/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java-agent/</guid>
      <description>**开发者可以构建一个独立于应用程序的代理程序（Agent），用来监测和协助运行在 JVM 上的程序，甚至能够替换和修改某些类的定义。**有了这样的功能，开发者就可以实现更为灵活的运行时虚拟机监控和 Java 类操作了，这样的特性实际上提供了 一种虚拟机级别支持的 AOP 实现方式，使得开发者无需对 JDK 做任何升级和改动，就可以实现某些 AOP 的功能了。
1.1 JVM启动前静态Instrument 未完待续。。。</description>
    </item>
    
    <item>
      <title>Java switch表达式支持的数据类型</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java-switch%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%94%AF%E6%8C%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java-switch%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%94%AF%E6%8C%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</guid>
      <description>switch(expression){ case value : //语句  break; //可选  case value : //语句  break; //可选  //你可以有任意数量的case语句  default : //可选  //语句 } 这里的 expression 支持：
1、基本数据类型：byte, short, char, int
2、包装数据类型：Byte, Short, Character, Integer
3、枚举类型：Enum
4、 字符串类型：String（Jdk 7+ 开始支持）
为什么不支持long、float、double数据类型？
switch 底层是使用 int 型 来进行判断的，即使是枚举、String类型，最终也是转变成 int 型。由于 long、float、double 型表示范围大于 int 型，因此不支持 long、float、double 类型。 （String类型最终是转成了int类型的hashCode；枚举最终转成了枚举对象的定义顺序，即 ordinal值）
下面举一个使用包装类型和枚举的，其实也不难，注意只能用在 switch 块里面
// 使用包装类型 Integer value = 5; switch (value) { case 3: System.</description>
    </item>
    
    <item>
      <title>Java协程</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%8D%8F%E7%A8%8B/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%8D%8F%E7%A8%8B/</guid>
      <description>参考 为什么 Java 坚持多线程不选择协程？Kotlin 协程真的比 Java 线程更高效吗？硬核系列 | 深入剖析 Java 协程Golang 的 协程调度机制 与 GOMAXPROCS 性能调优</description>
    </item>
    
    <item>
      <title>Java反射</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%8F%8D%E5%B0%84/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%8F%8D%E5%B0%84/</guid>
      <description>反射可以在程序运行过程中动态的构造类、获取类的全部信息、调用类型方法。但是，为什么我们要这么做呢？需要构造类，new就好了，需要访问类成员变量、调用方法，直接访问、调用就好了，为什么要通过一大堆反射代码去实现呢？
通常，class在编译期间就确定，JVM在运行时通过类加载器加载确定的class。如果在运行时才确定需要加载什么类，就需要利用java反射。java反射使得程序更加灵活，类似spring的框架将类以全限定名的形成配置在配置文件，然后再通过反射实例化。
参考：
https://blog.csdn.net/Appleyk/article/details/77879073</description>
    </item>
    
    <item>
      <title>Java堆外内存溢出</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/</guid>
      <description>前段时间在做一个实时人脸抓拍项目的时候，遇到了一个堆外内存OOM的问题，现在把思路好好整理一下。
项目中用opencv通过rtsp协议，实时的读取通用网络摄像头的视频帧。因为项目中多处用到了org.opencv.core.Mat这个对象，而Mat对象的构造是通过调用native方法实现的，也就是说构造Mat对象的时候，会在堆外分配内存：
//  // C++: Mat::Mat()  //  // javadoc: Mat::Mat()  public Mat() { nativeObj = n_Mat(); return; } // C++: Mat::Mat()  private static native long n_Mat(); 堆外分配的内存不受JVM的内存管理。由于又没有主动调用Mat.realse()去释放堆外内存，导致堆外内存OOM。
其实解决的办法很简单，可以在Mat对象使用完毕后直接调用Mat.realse()释放堆外内存。（没有试过，本人使用的下面的方式）
但是，Mat对象充斥着整个项目，要跟踪Mat对象的生命周期显得有点复杂，而且因为太多地方使用了Mat对象，很有可能遗漏调用Mat.realse()释放内存。因此，还是想把这部分内存的释放交由JVM来做，具体的方式是：定期的调用System.gc()执行垃圾回收（很多人说System.gc()只是建议JVM执行垃圾回收，并不是命令，是否执行取决去JVM自己，但是，经我实测，每次调用System.gc()都会触发垃圾回收。），JVM在垃圾回收前会执行每个**空java对象（null）**的finalize()方法，而Mat对象的finalize()方法正好实现了释放内存的逻辑：
@Override protected void finalize() throws Throwable { n_delete(nativeObj); super.finalize(); } // native support for java finalize()  private static native void n_delete(long nativeObj); 因为会定时的调用System.gc()触发Full GC, 而Full GC的之前会调用那些不再被引用的Mat对象的finalize()方法释放它的堆外内存，所以间接的实现了由JVM释放堆外内存的目的。
但是，这种做法并不好，因为通过System.gc()强制定期执行Full GC，势必会影响java应用本身。
为何一定要复制到DirectByteBuffer来读写（系统调用）？
GC会回收无用对象，同时还会进行碎片整理，移动对象在内存中的位置，来减少内存碎片。DirectByteBuffer不受GC控制。如果不用DirectByteBuffer而是用HeapByteBuffer，如果在调用系统调用时，发生了GC，导致HeapByteBuffer内存位置发生了变化，但是内核态并不能感知到这个变化导致系统调用读取或者写入错误的数据。所以一定要通过不受GC影响的DirectByteBuffer来进行IO系统调用。
假设我们要从网络中读入一段数据，再把这段数据发送出去的话，采用Non-direct ByteBuffer的流程是这样的：
网络 –&amp;gt; 临时的DirectByteBuffer –&amp;gt; 应用 Non-direct ByteBuffer –&amp;gt; 临时的Direct ByteBuffer –&amp;gt; 网络</description>
    </item>
    
    <item>
      <title>Java如何绑定线程到指定CPU上执行</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%A6%82%E4%BD%95%E7%BB%91%E5%AE%9A%E7%BA%BF%E7%A8%8B%E5%88%B0%E6%8C%87%E5%AE%9Acpu%E4%B8%8A%E6%89%A7%E8%A1%8C/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%A6%82%E4%BD%95%E7%BB%91%E5%AE%9A%E7%BA%BF%E7%A8%8B%E5%88%B0%E6%8C%87%E5%AE%9Acpu%E4%B8%8A%E6%89%A7%E8%A1%8C/</guid>
      <description>不知道你是啥感觉，但是我第一次看到这个问题的时候，我是懵逼的。
而且它还是一个面试题。
我懵逼倒不是因为我不知道答案，而是恰好我之前在非常机缘巧合的情况下知道了答案。
我感觉非常的冷门，作为一个考察候选者的知识点出现在面试环节中不太合适，除非是候选者主动提起做过这样的优化。
而且怕就怕面试官也是恰巧在某个书上或者博客中知道这个东西，稍微的看了一下，以为自己学到了绝世武功，然后拿出去考别人。
这样不合适。
说回这个题目。
正常来说，其实应该是属于考察操作系统的知识点范畴。
但是面试官呢又特定的加了“在 Java 中如何实现”。
那我们就聊聊这个问题。
Java线程 在聊如何绑定之前，先铺垫一个相关的背景知识：Java线程的实现。
其实我们都知道 Thread 类的大部分方法都是 native 方法：
在 Java 中一个方法被声明为 native 方法，绝大部分情况下说明这个方法没有或者不能使用平台无关的手段来实现。
说明需要操作的是很底层的东西了，已经脱离了 Java 语言层面的范畴。
抛开 Java 语言这个大前提，实现线程主要是有三种方式：
 1.使用内核线程实现（1:1实现） 2.使用用户线程实现（1:N实现） 3.使用用户线程加轻量级进程混合实现（N:M实现）
 这三种实现方案，在《深入理解Java虚拟机》的 12.4 小节有详细的描述，有兴趣的同学可以去仔细的翻阅一下。
总之，你要知道的是虽然有这三种不同的线程模型，但是 Java 作为上层应用，其实是感知不到这三种模型之间的区别的。
JVM 规范里面也没有规定，必须使用哪一种模型。
因为操作系统支持是怎样的线程模型，很大程度上决定了运行在上面的 Java 虚拟机的线程怎样去映射，但是这一点在不同的平台上很难达成一致。
所以JVM 规范里面没有、也不好去规定 Java 线程需要使用哪种线程模型来实现。
同时关于本文要讨论的话题，我在知乎上也找到了类似的问题：
 https://www.zhihu.com/question/64072646/answer/216184631 这里面有一个R大的回答，大家可以看看一下。
他也是先从线程模型的角度铺垫了一下。
我这里主要说一下使用内核线程实现（1:1实现）的这个模型。
因为我们用的最多的 HotSpot 虚拟机，就是采用 1:1 模型来实现 Java 线程的。
这是个啥意思呢？
说人话就是一个 Java 线程是直接映射为一个操作系统原生线程的，中间没有额外的间接结构。HotSpot 虚拟机也不干涉线程的调度，这事全权交给底下的操作系统去做。
顶多就是设置一个线程优先级，操作系统来调度的时候给个建议。</description>
    </item>
    
    <item>
      <title>Java安装指南</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/</guid>
      <description>1. 下载JDK 进入Oracle 官方网站下载合适的 JDK 版本，准备安装。
2. 创建目录 执行如下命令，在 /usr/ 目录下创建 java 目录。
mkdir /usr/java cd /usr/java 将下载的文件 jdk-8u151-linux-x64.tar.gz 复制到 /usr/java/ 目录下。
3. 解压 JDK tar -zxvf jdk-8u151-linux-x64.tar.gz 4. 设置环境变量 set java environment JAVA_HOME=/usr/java/jdk1.8.0_151 JRE_HOME=/usr/java/jdk1.8.0_151/jre CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin export JAVA_HOME JRE_HOME CLASS_PATH PATH 使修改生效：
source /etc/profile 5. 测试 执行如下命令进行测试。
java -version 若显示 Java 版本信息，则说明 JDK 安装成功：
java version &amp;#34;1.8.0_151&amp;#34; Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.</description>
    </item>
    
    <item>
      <title>Java对象的hashCode方法</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%AF%B9%E8%B1%A1%E7%9A%84hashcode%E6%96%B9%E6%B3%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/java%E5%AF%B9%E8%B1%A1%E7%9A%84hashcode%E6%96%B9%E6%B3%95/</guid>
      <description>java中的Object类的hashCode方法是一个native方法，查看native源码过于困难，所以暂且认为 Object类的hashCode生成规则是：hash(对象的内存地址+一些其他信息)
java中String类的 hashCode方法 比较直观，源码如下：
public int hashCode() { int h = hash; if (h == 0 &amp;amp;&amp;amp; value.length &amp;gt; 0) { char val[] = value; for (int i = 0; i &amp;lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 生成规则：s[0]*31^(n-1) + s[1]*31^(n-2) + &amp;hellip; + s[n-1]
为什么是素数31？
素数：根据素数的特点，一个数与素数相乘，得到结果只能被1、这个数、素数本身整除。因此，按照 s[0]*31^(n-1) + s[1]*31^(n-2) + &amp;hellip; + s[n-1] 生成的hashCode越不容易发生碰撞。
31：哈希计算速度快。可用移位和减法来代替乘法。现代的VM可以自动完成这种优化，如31 * i = (i &amp;laquo; 5) - i。</description>
    </item>
    
    <item>
      <title>JIT即时编译</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/jit%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/jit%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/</guid>
      <description>1. 编译器与解释器 不做特别说明的话，我们讲的，
编译器：程序运行前将其编译成机器码的程序
解释器：程序运作中逐行解释源码得到结果的程序
 特别注意的是，解释器也是一个程序，输入源码，输出结果，并没有显示的将源码转换成机器码的过程 解释器与 JIT 无论是编译器还是解释器，从 源码 到结果都需要将源码经过：词法分析 -&amp;gt; 语法分析 -&amp;gt; 语义分析 处理，
一个比较简单的编译器的处理步骤看起来：
编译流程：源码 [字符流]- 词法分析 -&amp;gt; 单词（token）流- 语法分析 -&amp;gt; 语法树 / 抽象语法树- 语义分析 -&amp;gt; 标注了属性的抽象语法树- 代码生成 -&amp;gt; 目标代码执行流程：目标代码- 操作系统/硬件 -&amp;gt; 执行结果狭义的解释器处理步骤看起来：
解释执行流程：源码 [字符流]- 需要做词法分析+语法分析+类型检查的字符流解释器 -&amp;gt; 执行结果 特别注意的是，解释器真正的输入往往并直接是源码，使用解释器实现的编程语言实现里，通常：
 至少会在解释执行前做完语法分析，然后通过树解释器来实现解释执行； 兼顾易于实现、跨平台、执行效率这几点，会选择使用字节码解释器实现解释执行。  为什么大多数解释器都将AST转化成字节码再用虚拟机执行，而不是直接解释AST？ 2. 解释型语言 很多资料会说，Python、Ruby、JavaScript都是“解释型语言”，是通过解释器来实现的。这么说其实很容易引起误解：语言一般只会定义其抽象语义，而不会强制性要求采用某种实现方式。
例如说C一般被认为是“编译型语言”，但C的解释器也是存在的，例如Ch。同样，C++也有解释器版本的实现，例如Cint。
一般被称为“解释型语言”的是主流实现为解释器的语言，但并不是说它就无法编译。例如说经常被认为是“解释型语言”的Scheme就有好几种编译器实现，其中率先支持R6RS规范的大部分内容的是Ikarus，支持在x86上编译Scheme；它最终不是生成某种虚拟机的字节码，而是直接生成x86机器码。</description>
    </item>
    
    <item>
      <title>LRU和LFU</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/lru%E5%92%8Clfu/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/lru%E5%92%8Clfu/</guid>
      <description>LRU和LFU都是内存管理的页面置换算法。
LRU，即：最近最少使用淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面。
LFU，即：最不经常使用淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内，使用次数最少的页面。
案例：
假设LFU方法的时期T为10分钟，访问如下页面所花的时间正好为10分钟，内存块大小为3。
若所需页面顺序依次如下：
2 1 2 1 2 3 4
&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-&amp;gt;
当需要使用页面4时，内存块中存储着1、2、3，内存块中没有页面4，就会发生缺页中断，而且此时内存块已满，需要进行页面置换。
若按LRU算法，应替换掉页面1。因为页面1是最长时间没有被使用的了，页面2和3都在它后面被使用过。
若按LFU算法，应换页面3。因为在这段时间内，页面1被访问了2次，页面2被访问了3次，而页面3只被访问了1次，一段时间内被访问的次数最少。
可见LRU关键是看页面最后一次被使用到发生调度的时间长短，而LFU关键是看一定时间段内页面被使用的频率!
参考链接： LRU和LFU的区别</description>
    </item>
    
    <item>
      <title>maven基础</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/maven/maven%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/maven/maven%E5%9F%BA%E7%A1%80/</guid>
      <description>1. 问题 项目开发过程中，经常会遇到jar冲突，然后maven根据自己的规则进行冲突解决，导致项目在运行的过程中报错。
1、maven自动解决依赖冲突的规则是什么？
2、如何查看当前项目的maven的依赖树？
3、如何从依赖树中找到自己预期的版本，是被那个jar给覆盖了？
4、如何人工进行依赖冲突解决，达到使用目的？
2. 解决问题 2.1 maven自动解决依赖冲突的规则是什么？ 2.1.1 第一原则：路径最近者优先 项目A有如下的依赖关系：
A-&amp;gt;B-&amp;gt;C-&amp;gt;X(1.0)
A-&amp;gt;D-&amp;gt;X(2.0)
则该例子中，X的版本是2.0
2.1.2 第二原则：路径相等，先声明者优先 项目A有如下的依赖关系：
A-&amp;gt;B-&amp;gt;Y(1.0)
A-&amp;gt;C-&amp;gt;Y(2.0)
若pom文件中B的依赖坐标先于C进行声明，则最终Y的版本为1.0
2.2 如何查看当前项目的maven依赖树？ //进入项目的pom.xml文件的目录下，运行如下命令//这个是正常依赖的树mvn dependency:tree//这个命令是查看maven是如何解决依赖冲突的依赖树mvn -Dverbose dependency:tree//如果想将依赖树打印到指定文件中，则命令如下mvn -Dverbose dependency:tree -Doutput=/Users/shangxiaofei/sxfoutput.txt3. 如何从依赖树中找到自己预期的版本，是被那个jar给覆盖了？ 例子：
递归依赖的关系列的算是比较清楚了，每行都是一个jar包，根据缩进可以看到依赖的关系。
最后写着compile的就是编译成功的。
最后写着omitted for duplicate的就是有jar包被重复依赖了，但是jar包的版本是一样的。
最后写着omitted for conflict with xxxx的，说明和别的jar包版本冲突了，而该行的jar包不会被引入。比如上面有一行最后写着omitted for conflict with 3.4.6，那么该行的zookeeper:jar:3.4.8不会被引入，会引入3.4.6版本
最后写着version managed from 2.3 ;omitted for duplicate ,表示最终使用commons-pool2最终会使用2.4.2，拒绝使用中声明的2.3版本
最后写着version managed from 1.16.8 ;表示最终使用lombok:jar:1.16.22版本
4. 如何人工进行依赖冲突解决，达到使用目的？ 解决重复依赖和冲突的方法：</description>
    </item>
    
    <item>
      <title>Merkle Patricia Tree</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/merkle-patricia-treempt/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/merkle-patricia-treempt/</guid>
      <description>区块链一个重要的亮点就是防篡改，那么它是怎么做到防篡改的呢？其中一个重要的知识点就是Merkle Patricia Tree(MPT)，本篇就来解析下何为MPT。
MPT是一种加密认证的数据结构，它融合了Merkle树和Patricia Trie树(基数树/压缩前缀树)两种数据类型的优点。
则在介绍MPT树之前先介绍下Merkle树(默克尔树)、Trie树(前缀树)和Patricia Trie(基数树/压缩前缀树)，介绍Trie树是因为Patricia Trie是基于Trie树衍化来的。
Trie树 Trie树又称前缀树或字典树，是一种检索树，使用一个有序的树结构存储一个动态数据集或者关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙相对于当前节点都有相同的前缀，而根节点为空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。
Trie树中，key是从树根到对应value得真实的路径。即从根节点开始，key中的每个字符会标识走那个子节点从而到达相应value。Value被存储在叶子节点，是每条路径的终止。假如key来自一个包含N个字符的字母表，那么树中的每个节点都可能会有多达N个孩子，树的最大深度是key的最大长度。看个例子画个图就了然了。 例子：关键字集合{“a”, “to”, “tea”, “ted”, “i”, “in”, “inn”}，此集合转为Trie树为
Trie树
不理想情况下，数据集中存在一个很长的key，而这个key与其它key又没有太多的公共前缀，这就造成整个树的深度会加大，需要存储多个节点，存储比较稀疏而且极不平衡。 例子：关键字集合{“algori”, “to”, “tea”, “ted”, “i”, “in”, “inn”}，此集合转为Trie树为
稀疏Trie树
Patricia Trie树 既然Trie树在某些情况下存储空间利用率不高，那就给压缩下，然后就出现了Patricia Trie树。
Patricia Trie树是一种空间使用率经过优化的Trie树。与Trie树不同的是，Patricia Trie 里如果存在一个父节点只有一个子节点，那么这个父节点将与其子节点合并。这样压缩存储可以减少Trie树中不必要的深度，大大加快搜索节点速度。 如下图所示
Patricia Trie树
Merkle树 Merkle树是由计算机科学家Ralph Merkle在很多年前提出的，并以他本人的名字来命名，是一种树形数据结构，可以是二叉树，也可以是多叉树。 它由若干叶节点、中间节点和一个根节点构成。最下面的叶节点包含基础数据，每个中间节点是它子节点的散列，根节点是它的子节点的散列，代表了Merkle树的根部。
由于Merkle树是自底向上构建的，而且除叶子结点之外的其它节点都是其子节点的散列，这样每个节点的值发生变化都会一层一层的向上反映，最终在根节点上表现出来。也就是说只要对比两个Merkle树的根节点是否相等就能得到两份数据集是否一样，而且还可以验证Merkle树的某个分支。
比特币使用Merkle树存储一个区块中的所有交易信息，一是为了防篡改，因为散列是向上的，伪造任何一个节点都会引起上层节点的改动，最终导致根节点的变化。二是为了允许区块的数据可以零散的传送，即节点可以从一个节点下载区块头，从另外的源下载与其相关的树的其他部分，而依然能够确认所有的数据都是正确的。
看下图的例子，首先将L1-L4四个单元数据散列化，然后将散列值存储至相应的叶子节点。这些节点是Hash0-0, Hash0-1, Hash1-0, Hash1-1，然后将相邻两个节点的散列值合并成一个字符串，然后计算这个字符串的散列，得到的就是这两个节点的父节点的散列值。
Merkle树
在比特币网络中，merkle树被用来归纳一个区块中的所有交易，同时生成整个交易集合的数字指纹。此外，由于merkle树的存在，使得在比特币这种公链的场景下，扩展一种“轻节点”实现简单支付验证变成可能。
知道了Merkle树在比特币中的应用，那么他是怎么构成呢？现在就简单看下其构成。它由一组叶节点、一组中间节点和一个根节点构成。最下面的叶节点包含基础数据，每个中间节点是它的子节点的散列，根节点是它的子节点的散列，代表了Merkle树的根部 。
Merkle树具有下列特性:
 每个数据集对应一个唯一合法的根散列值。 很容易更新、添加或者删除树节点，以及生成新的根散列值 。 不改变根散列值的话就没有办法修改树的任何部分，所以如果根散列值被包括在签名的文档或有效区块中，就可以保证这棵树的正确性。 任何人可以只提供一个到特定节点的分支，并通过密码学方法证明拥有对应内容的节点确实在树里 。  Merkle Patricia Tree 叨叨了那么多，本篇的主角终于出来了。 Merkle Patricia Tree结合了Merkle树和Patricia树的特点，并针对以太坊的使用场景进行了一些改进。
首先，为了保证树的加密安全，每个节点通过它的散列值被引用，则根节点是一层一层散列向上收敛而得，被称为整棵树的加密签名，如果一棵给定 Trie树的根散列值是公开的，那么所有人都可以提供一种证明，即通过提供每步向上的路径证明特定的key是否含有特定的值。在当前的以太坊版本中，MPT存储在LevelDB数据库中。</description>
    </item>
    
    <item>
      <title>MongoDB综述</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mongodb/mongodb%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mongodb/mongodb%E7%BB%BC%E8%BF%B0/</guid>
      <description>MongoDB综述 参考 项目实战 MongoDB快速入门，掌握这些刚刚好！mall整合Mongodb实现文档操作</description>
    </item>
    
    <item>
      <title>MQ消息最终一致性解决方案</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mq/mq%E6%B6%88%E6%81%AF%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mq/mq%E6%B6%88%E6%81%AF%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</guid>
      <description>随着分布式服务架构的流行与普及，原来在单体应用中执行的多个逻辑操作，现在被拆分成了多个服务之间的远程调用。虽然服务化为我们的系统带来了水平伸缩的能力，然而随之而来挑战就是分布式事务问题，多个服务之间使用自己单独维护的数据库，它们彼此之间不在同一个事务中，假如A执行成功了，B执行却失败了，而A的事务此时已经提交，无法回滚，那么最终就会导致两边数据不一致性的问题；尽管很早之前就有基于两阶段提交的XA分布式事务，但是这类方案因为需要资源的全局锁定，导致性能极差；因此后面就逐渐衍生出了消息最终一致性、TCC等柔性事务的分布式事务方案，本文主要分析的是基于消息的最终一致性方案。
0. 简单RPC处理存在的一致性问题 在正式开始讲述正题之前，我们先看一下，不依赖任何分布式事务手段，单纯将本地业务逻辑和远程调用逻辑放在同一个本地事务中会有什么问题。
我们以订单创建为例，订单系统先创建订单(本地事务)，然后RPC调用库存扣减服务。
@Transactionnal public void processOrder() { try{ // 订单处理(业务操作)  orderService.process(); // 库存扣减（RPC远程调用）  storageService.deduction(); }catch(Exception e){ 事务回滚; } } 如果库存服务由于DB数据量比较大，导致处理超时，订单服务在出现超时异常后，直接回滚本地事务，从而导致订单服务这边没数据，而库存服务那边数据却已经写入了，最终导致两边业务数据的不一致。
即使不存在 “DB数据量比较大” 这种特殊情况，也一定会存在因为网络抖动，订单服务调用库存服务超时而本地回滚，但是库存服务实际操作成功的情况。
其根本的原因就在于：远程调用，结果最终可能为成功、失败、超时；而对于超时的情况，处理方最终的结果可能是成功，也可能是失败，调用方是无法知晓的。
1. 普通消息的处理流程  消息生成者发送消息 MQ收到消息，将消息进行持久化，在存储中新增一条记录 返回ACK给生产者 MQ push 消息给对应的消费者，然后等待消费者返回ACK 如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息,重复执行4、5、6步骤 MQ删除消息  1.2 普通消息处理存在的一致性问题 我们还是以订单创建为例，订单系统先创建订单(本地事务)，再发送消息给下游处理；如果订单创建成功，然而消息没有发送出去，那么下游所有系统都无法感知到这个事件，会出现脏数据；
public void processOrder() { // 订单处理(业务操作)  orderService.process(); // 发送订单处理成功消息(发送消息)  sendBizMsg (); } 如果先发送订单消息，再创建订单；那么就有可能消息发送成功，但是在订单创建的时候却失败了，此时下游系统却认为这个订单已经创建，也会出现脏数据。
public void processOrder() { // 发送订单处理成功消息(发送消息)  sendBizMsg (); // 订单处理(业务操作)  orderService.</description>
    </item>
    
    <item>
      <title>MQ综述</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mq/mq%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mq/mq%E7%BB%BC%E8%BF%B0/</guid>
      <description>1. 概念  Broker Producer Consumer Topic Queue Message  2. 模式 2.1. 点对点 PTP 点对点: 使用 Queue 作为通信载体
消息生产者生产消息发送到 Queue 中，然后消息消费者从 Queue 中取出并且消费消息。消息被消费以后，Queue 中不再存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费
2.2. 发布/订阅 Pub/Sub 发布订阅(广播): 使用 Topic 作为通信载体
消息生产者(发布)将消息发布到 Topic 中，同时有多个消息消费者(订阅)消费该消息。和点对点方式不同，发布到 Topic 的消息会被所有订阅者消费
总结 Queue 实现了负载均衡，将 Producer 生产的消息发送到消息队列中，由多个消费者消费。但一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有一个可用的消费者
Topic 实现了发布和订阅，当你发布一个消息，所有订阅这个 Topic 的服务都能得到这个消息，所以从1到N个订阅者都能得到一个消息的拷贝
3. 协议 3.1. AMQP协议 AMQP 即 Advanced Message Queuing Protocol ，一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。兼容 JMS。RabbitMQ 就是基于 AMQP 协议实现的。
优点：可靠、通用
 JMS（JAVA Message Service,java 消息服务）是 java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS（JAVA Message Service，Java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。</description>
    </item>
    
    <item>
      <title>MySQL Integer类型与INT(11)</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/mysql-integer%E7%B1%BB%E5%9E%8B%E4%B8%8Eint11/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/mysql-integer%E7%B1%BB%E5%9E%8B%E4%B8%8Eint11/</guid>
      <description>1.介绍 Integer类型，即整数类型，MySQL支持的整数类型有TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT。
1.1 空间和范围 每种整数类型所需的存储空间和范围如下：
   类型 字节 最小值(有符号) 最大值(有符号) 最小值(无符号) 最大值(无符号)     TINYINT 1 -128 127 0 255   SMALLINT 2 -32768 32767 0 65535   MEDIUMINT 3 -8388608 8388607 0 16777215   INT 4 -2147483648 2147483647 0 4294967295   BIGINT 8 $-2^{63}$(-9223372036854775808) $2^{63}-1$(9223372036854775807) 0 $2^{64}-1$(18446744073709551615)    2. INT(11) 2.1 数字是否限制长度？ id INT(11) NOT NULL AUTO_INCREMENT, 在一些建表语句会出现上面 int(11) 的类型，那么其代表什么意思呢？</description>
    </item>
    
    <item>
      <title>MySQL前缀索引</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/mysql%E5%89%8D%E7%BC%80%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/mysql%E5%89%8D%E7%BC%80%E7%B4%A2%E5%BC%95/</guid>
      <description>参考 MySQL 前缀索引前缀索引，一种优化索引大小的解决方案</description>
    </item>
    
    <item>
      <title>MySQL日志</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/mysql%E6%97%A5%E5%BF%97/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/mysql%E6%97%A5%E5%BF%97/</guid>
      <description>日志是mysql数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。作为开发，我们重点需要关注的是二进制日志(binlog)和事务日志(包括redo log和undo log)，本文接下来会详细介绍这三种日志。其他几种日志见 👉 [玩转MySQL之八]MySQL日志分类及简介1. binlog binlog用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog是mysql的逻辑日志，并且由Server层进行记录，使用任何存储引擎的mysql数据库都会记录binlog日志。
 逻辑日志：可以简单理解为记录的就是sql语句。
  物理日志：因为mysql数据最终是保存在数据页中的，物理日志记录的就是数据页变更。
 binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。
1.1 binlog使用场景 在实际应用中，binlog的主要使用场景有两个，分别是主从复制和数据恢复。
 主从复制：在Master端开启binlog，然后将binlog发送到各个Slave端，Slave端重放binlog从而达到主从数据一致。 数据恢复：通过使用mysqlbinlog工具来恢复数据。  1.2 binlog刷盘时机 对于InnoDB存储引擎而言，只有在事务提交时才会记录biglog，此时记录还在内存中，那么biglog是什么时候刷到磁盘中的呢？mysql通过sync_binlog参数控制biglog的刷盘时机，取值范围是0-N：
 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次commit的时候都要将binlog写入磁盘； N：每N个事务，才会将binlog写入磁盘。  从上面可以看出，sync_binlog最安全的是设置是1，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。
1.3 binlog日志格式 binlog日志有三种格式，分别为STATMENT、ROW和MIXED。
 在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。
  STATMENT 基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、slepp()等。 ROW 基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题； 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨 MIXED 基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog  2. redo log 2.1 为什么需要redo log 我们都知道，事务的四大特性里面有一个是持久性，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态。那么mysql是如何保证持久性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：
 因为Innodb是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！  因此mysql设计了redo log，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。MySQL实战45讲中将redo log比作临时记账的粉板，实际数据页比作账本。</description>
    </item>
    
    <item>
      <title>nginx授权登陆报403问题</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/nginx%E6%8E%88%E6%9D%83%E7%99%BB%E9%99%86%E6%8A%A5403%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/nginx%E6%8E%88%E6%9D%83%E7%99%BB%E9%99%86%E6%8A%A5403%E9%97%AE%E9%A2%98/</guid>
      <description>在给nginx加上授权模块后，再访问应用报403访问禁止的错误。
一开始是这样：
1. 生成密码文件 printf &amp;#34;yourusername:$(openssl passwd -apr1)&amp;#34; &amp;gt; /etc/nginx/passwords 2. nginx配置 server { # ... auth_basic &amp;#34;Protected&amp;#34;; auth_basic_user_file passwords; # ... } [v_error]auth_basic_user_file 后面跟的是相对路径，这样配置很容易导致nginx找不到文件，因此改成绝对路径就万事大吉了：[/v_error]
server{	listen 443 ssl; server_name netdata.6and.ltd; #listen [::]:81 default_server ipv6only=on; #ssl on; ssl_certificate httpssl/1_netdata.6and.ltd_bundle.crt; ssl_certificate_key httpssl/2_netdata.6and.ltd.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #index index.html index.htm index.php; #root /home/wwwroot/; #error_page 404 /404.html; #include enable-php.conf; auth_basic &amp;#34;Protected&amp;#34;; auth_basic_user_file /usr/local/nginx/passwords; location / { proxy_pass http://127.</description>
    </item>
    
    <item>
      <title>OAuth2</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/oauth2/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/oauth2/</guid>
      <description>OAuth（开放授权）是一个开放标准，允许用户授权第三方移动应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容，OAuth2.0是OAuth协议的延续版本，但不向后兼容OAuth 1.0即完全废止了OAuth1.0。
客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。OAuth 2.0定义了四种授权方式。
 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials）  1、名词定义 在详细讲解OAuth 2.0之前，需要了解几个专用名词。
（1） Third-party application：第三方应用程序，本文中又称&amp;quot;客户端&amp;quot;（client），即上一节例子中的&amp;quot;云冲印&amp;quot;。
（2）HTTP service：HTTP服务提供商，本文中简称&amp;quot;服务提供商&amp;quot;，即上一节例子中的Google。
（3）Resource Owner：资源所有者，本文中又称&amp;quot;用户&amp;quot;（user）。
（4）User Agent：用户代理，本文中就是指浏览器。
（5）Authorization server：认证服务器，即服务提供商专门用来处理认证的服务器。
（6）Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。
知道了上面这些名词，就不难理解，OAuth的作用就是让&amp;quot;客户端&amp;quot;安全可控地获取&amp;quot;用户&amp;quot;的授权，与&amp;quot;服务商提供商&amp;quot;进行互动。
2、4种模式区别 授权码模式没什么好讲的，应用最广泛，是标准的OAuth2模式；
简化模式适合没有后端的应用，也就无法安全的存储client id 和 client secret 例如只有一个页面的h5调查问卷等等；
密码模式是用户把账号密码直接告诉客户端，非常不安全，适用于遗留项目升级为oauth2的适配方案、客户端是自家的应用；
客户端模式直接根据client的id和密钥即可获取token，无需用户参与，适用于后端服务调后端服务；
参考链接： 理解OAuth 2.0oauth2四种授权方式小结[简易图解]『 OAuth2.0』 『进阶』 授权模式总结OAuth2.0的四种授权模式</description>
    </item>
    
    <item>
      <title>Oauth2的授权码模式为什么返回token之前先返回code，而不是直接返回token？</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/oauth2%E7%9A%84%E6%8E%88%E6%9D%83%E7%A0%81%E6%A8%A1%E5%BC%8F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%94%E5%9B%9Etoken%E4%B9%8B%E5%89%8D%E5%85%88%E8%BF%94%E5%9B%9Ecode%E8%80%8C%E4%B8%8D%E6%98%AF%E7%9B%B4%E6%8E%A5%E8%BF%94%E5%9B%9Etoken/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/oauth2%E7%9A%84%E6%8E%88%E6%9D%83%E7%A0%81%E6%A8%A1%E5%BC%8F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%94%E5%9B%9Etoken%E4%B9%8B%E5%89%8D%E5%85%88%E8%BF%94%E5%9B%9Ecode%E8%80%8C%E4%B8%8D%E6%98%AF%E7%9B%B4%E6%8E%A5%E8%BF%94%E5%9B%9Etoken/</guid>
      <description>所谓第三方登录，实质就是 OAuth 授权。用户想要登录 A 网站，A 网站让用户提供第三方网站的数据，证明自己的身份。获取第三方网站的身份数据，就需要 OAuth 授权。
举例来说，A 网站允许 GitHub 登录，背后就是下面的流程。
  A 网站让用户跳转到 GitHub。 GitHub 要求用户登录，然后询问&amp;quot;A 网站要求获得 xx 权限，你是否同意？&amp;quot; 用户同意，GitHub 就会重定向回 A 网站，同时发回一个授权码。 A 网站使用授权码，向 GitHub 请求令牌。 GitHub 返回令牌. A 网站使用令牌，向 GitHub 请求用户数据。   那为什么不直接返回token？而要中间经过code再倒腾一遍呢？知乎上有位大哥回答得挺好。
问题：
 为什么oauth2中的授权码模式 在获取token之前非要先到资源服务器获取一个code 然后才使用资源服务器的code去资源服务器去申请token?
看了很多资料说是因为 用户在确认授权之后 资源服务器会跳转到我们指定的一个回调url, 如果直接返回token的话，谁都可以在浏览器中看到这个token 那就没有安全性可言了
但是我有个想法不知是否可行 那就是为什么 资源服务器非要跳转到第三方站点给的回调url呢？ 我的url如果是个接口 资源服务器完全可以不通过浏览器跳转 而是直接回调我的接口 直接吧token给我的服务器， 然后我的服务器存储好token之后 自己决定如何跳转不就行了？ 这样岂不是比授权模式简单也比隐式模式安全
 回答：
 首先，从产品交互上，我们需要浏览器跳转到“认证服务器”，让用户明确表态同不同意“第三方站点”的授权请求。这个时候，浏览器访问的地址已经到“认证服务器”去了，不跳转回来的话，网页不在“第三方站点”的控制中，怎么进行授权成功后的下一步交互呢？
授权码模式的安全考量，是基于产品交互能完成的前提下，考虑如何不在浏览器这种暴露 url 的环境里做到安全的。
如果你想表达的是“认证服务器”跳转回来，但是不带 code，而是通过 Server 对 Server 将 token 直接给“第三方服务器”。</description>
    </item>
    
    <item>
      <title>OOM实战</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/oom%E5%AE%9E%E6%88%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/oom%E5%AE%9E%E6%88%98/</guid>
      <description>《深入理解Java虚拟机》中将OOM划分为: Java堆溢出、虚拟机栈和本地方法栈溢出、方法区和运行时常量池溢出、本机直接内存溢出
1. Java堆溢出 /** * JDK1.6/JDK1.8 * * Java堆内存溢出异常测试 * * VM Args: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError * * @author xuzhijun.online * @date 2019年4月22日 */ public class HeapOOM { static class OOMObject{ } public static void main(String[] args) { List&amp;lt;OOMObject&amp;gt; list = new ArrayList&amp;lt;OOMObject&amp;gt;(); while(true) { list.add(new OOMObject()); } } } 运行结果：
java.lang.OutOfMemoryError: Java heap space Dumping heap to java_pid3404.hprof ... Heap dump file created [22045981 bytes in 0.663 secs] 处理方法：</description>
    </item>
    
    <item>
      <title>openresty最佳实践</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/openresty/openresty%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/openresty/openresty%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</guid>
      <description>推荐先阅读下面文章，以储备基础知识。
方志朋openresty系列：openresty最佳案例案例-汇总黑马程序员：java自学进阶高性能web平台openresty简介</description>
    </item>
    
    <item>
      <title>pbft</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/pbft/pbft/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/pbft/pbft/</guid>
      <description>1999年Castro和Liskov在《操作系统设计与实现》上发表论文Practical Byzantine Fault Tolerance。之后Castro和Liskov修改了之前论文的部分细节，2001年将修改后的论文Practical Byzantine Fault Tolerance and Proactive Recovery发表于《ACM Transactions on Computer Systems (TOCS)》。之后发表的这篇论文对之前的论文进行了部分优化，pre-prepare消息不再包含请求消息（只包含消息摘要），client不只把请求发给主节点（也发给从节点）等。fabric中的pbft实现也是基于2001年的论文，因此建议大家直接看2001年发表的论文。但是因为，网上对1999年的论文讲解比较多，本文也以1999年的论文形成总结。
0. 背景 拜占庭问题（Byzantine Problem） 又叫拜占庭将军（Byzantine Generals Problem） 问题，讨论的是允许存在少数节点作恶（消息可能被伪造） 场景下的如何达成共识问题。拜占庭容错（Byzantine Fault Tolerant，BFT）讨论的是容忍拜占庭错误的共识算法。
 两将军问题  拜占庭问题之前，学术界就已经存在两将军问题的讨论（《Some constraints and tradeofis in the design of network communications》 ，1975年）：两个将军要通过信使来达成进攻还是撤退的约定，但信使可能迷路或被敌军阻拦（消息丢失或伪造），如何达成一致？根据FLP不可能原理，这个问题无通用解。
 拜占庭问题  拜占庭问题最早由 Leslie Lamport 等学者于 1982 年在论文《The Byzantine Generals Problem》中正式提出，是用来解释异步系统中共识问题的一个虚构模型。它是分布式领域中最复杂、最严格的容错模型。在该模型下，系统不会对集群中的节点做任何的限制，它们可以向其他节点发送随机数据、错误数据，也可以选择不响应其他节点的请求，这些无法预测的行为使得容错这一问题变得更加复杂。
拜占庭是古代东罗马帝国的首都，由于地域宽广，假设其守卫边境的多个将军（系统中的多个节点） 需要通过信使来传递消息，达成某些一致决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将向不同的将军发送不同的消息，试图干扰共识的达成。拜占庭问题即讨论在此情况下，如何让忠诚的将军们能达成行动的一致。
在大多数的分布式系统中，拜占庭的场景并不多见。然而在特定场景下存在意义，例如允许匿名参与的系统（如比特币） ，或是出现欺诈可能造成巨大损失的情况。
 拜占庭容错算法  拜占庭容错算法（Byzantine Fault Tolerant）是面向拜占庭问题的容错算法，解决的是在网络通信可靠，但节点可能故障和作恶情况下如何达成共识。
拜占庭容错算法最早的讨论可以追溯到Leslie Lamport等人1982年发表的论文《The Byzantine Generals Problem》，之后出现了大量的改进工作，代表性成果包括《Optimal Asynchronous Byzantine Agreement》（1992年）、《Fully Polynomial Byzantine Agreement for n&amp;gt;3t Processors in t+1 Rounds》（1998年）等。长期以来，拜占庭问题的解决方案都存在运行过慢，或复杂度过高的问题，直到“实用拜占庭容错算法”（Practical Byzantine Fault Tolerance，PBFT）算法的提出。</description>
    </item>
    
    <item>
      <title>post和get</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/http/post%E5%92%8Cget/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/http/post%E5%92%8Cget/</guid>
      <description>RFC标准区别 根据RFC定义的http标准，区别如下：
 GET 用于获取信息，是无副作用的，是幂等的，且可缓存 POST 用于修改服务器上的数据，有副作用，非幂等，不可缓存  实际区别 GET 和 POST 方法没有实质区别，只是报文格式不同。
GET 和 POST 只是 HTTP 协议中两种请求方式，而 HTTP 协议是基于 TCP/IP 的应用层协议，无论 GET 还是 POST，用的都是同一个传输层协议，所以在传输上，没有区别。
你可以在GET请求的body里面发参数，也可以在POST请求的url后面跟参数。
现实中，之所以会有诸如：GET请求有长度限制这样的差异，是与浏览器和服务端实现细节有关，与http协议本身无关，不是http协议规定的这些差异。
其他区别 有些文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。
HTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。
所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。</description>
    </item>
    
    <item>
      <title>Redisson原理</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/redisson%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/redisson%E5%8E%9F%E7%90%86/</guid>
      <description>写在前面 在了解分布式锁具体实现方案之前，我们应该先思考一下使用分布式锁必须要考虑的一些问题。
 互斥性：在任意时刻，只能有一个进程持有锁。 防死锁：即使有一个进程在持有锁的期间崩溃而未能主动释放锁，要有其他方式去释放锁从而保证其他进程能获取到锁。 加锁和解锁的必须是同一个进程。 锁的续期问题。  常见的分布式锁实现方案  基于 Redis 实现分布式锁 基于 Zookeeper 实现分布式锁  本文采用第一种方案，也就是基于 Redis 的分布式锁实现方案。
Redis 实现分布式锁主要步骤  指定一个 key 作为锁标记，存入 Redis 中，指定一个 唯一的用户标识 作为 value。 当 key 不存在时才能设置值，确保同一时间只有一个客户端进程获得锁，满足 互斥性 特性。 设置一个过期时间，防止因系统异常导致没能删除这个 key，满足 防死锁 特性。 当处理完业务之后需要清除这个 key 来释放锁，清除 key 时需要校验 value 值，需要满足 只有加锁的人才能释放锁 。   特别注意：以上实现步骤考虑到了使用分布式锁需要考虑的互斥性、防死锁、加锁和解锁必须为同一个进程等问题，但是锁的续期无法实现。所以，博主采用 Redisson 实现 Redis 的分布式锁，借助 Redisson 的 WatchDog 机制 能够很好的解决锁续期的问题，同样 Redisson 也是 Redis 官方推荐分布式锁实现方案，实现起来较为简单。
 Redisson 实现分布式锁  具体实现代码已经上传到博主的仓库，需要的朋友可以在公众号内回复 【分布式锁代码】 获取码云或 GitHub 项目下载地址。</description>
    </item>
    
    <item>
      <title>redis分布式锁和zk分布式锁的对比</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%92%8Czk%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%92%8Czk%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AF%B9%E6%AF%94/</guid>
      <description>redis 分布式锁和 zk 分布式锁的对比  redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。  另外一点就是，如果是 Redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。
Redis 分布式锁大家没发现好麻烦吗？遍历上锁，计算时间等等&amp;hellip;&amp;hellip;zk 的分布式锁语义清晰实现简单。
所以先不分析太多的东西，就说这两点，我个人实践认为 zk 的分布式锁比 Redis 的分布式锁牢靠、而且模型简单易用。
参考 一般实现分布式锁都有哪些方式？使用 Redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？</description>
    </item>
    
    <item>
      <title>Redis配置指南</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/redis%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/redis%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/</guid>
      <description>1. 安装 $ wget http://download.redis.io/releases/redis-5.0.8.tar.gz $ tar xzf redis-5.0.8.tar.gz $ cd redis-5.0.8 $ make 2.配置  注释掉bind 127.0.0.1 protected-mode yes requirepass xxxpassword daemonize yes  3. 启动 cd src ./redis-server 或者
cd src ./redis-server ../redis.conf 4. 停止 cd src ./redis-cli auth xxxpassword shutdown exit 5. 卸载 find / -name &amp;#34;redis*&amp;#34; | xargs rm -rf 6.远程连接 window连接远程redis:
redis-cli -h 193.112.37.xxx -p 6379 -a xxxpassword </description>
    </item>
    
    <item>
      <title>Spring Security第一次登录失败，第二次登录成功</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/exception/spring-security%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%99%BB%E5%BD%95%E5%A4%B1%E8%B4%A5%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%99%BB%E5%BD%95%E6%88%90%E5%8A%9F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/exception/spring-security%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%99%BB%E5%BD%95%E5%A4%B1%E8%B4%A5%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%99%BB%E5%BD%95%E6%88%90%E5%8A%9F/</guid>
      <description>当我第一次登录时我会得到{&amp;quot; timestamp&amp;quot;：1481719982036，&amp;quot; status&amp;quot;：999，&amp;quot; error&amp;quot;：&amp;quot; None&amp;quot;，&amp;quot; message&amp;quot;：&amp;quot;无可用消息&amp;quot;}，但第二次还可以。
解决办法：填写如下配置到application.properties
spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration参考链接：java:Spring Security第一次登录失败，第二次登录成功</description>
    </item>
    
    <item>
      <title>SpringAOP方法内部调用不生效</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/springaop%E6%96%B9%E6%B3%95%E5%86%85%E9%83%A8%E8%B0%83%E7%94%A8%E4%B8%8D%E7%94%9F%E6%95%88/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/springaop%E6%96%B9%E6%B3%95%E5%86%85%E9%83%A8%E8%B0%83%E7%94%A8%E4%B8%8D%E7%94%9F%E6%95%88/</guid>
      <description>假设一个接口里面有两个方法：
package demo.long; public interface CustomerService { public void doSomething1(); public void doSomething2(); } 接口实现类如下：
package demo.long.impl; import demo.long.CustomerService; public class CustomerServiceImpl implements CustomerService { public void doSomething1() { System.out.println(&amp;#34;CustomerServiceImpl.doSomething1()&amp;#34;); doSomething2(); } public void doSomething2() { System.out.println(&amp;#34;CustomerServiceImpl.doSomething2()&amp;#34;); } } 现在我需要在CustomerService接口的每个方法被调用时都在方法前执行一些逻辑，所以需要配置一个拦截器：
package demo.long; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; @Aspect public class CustomerServiceInterceptor { @Before(&amp;#34;execution(* demo.long..*.*(..))&amp;#34;) public void doBefore() { System.out.println(&amp;#34;do some important things before...&amp;#34;); } } 把Bean加到Spring配置中
&amp;lt;aop:aspectj-autoproxy /&amp;gt; &amp;lt;bean id=&amp;#34;customerService&amp;#34; class=&amp;#34;demo.long.impl.CustomerServiceImpl&amp;#34; /&amp;gt; &amp;lt;bean id=&amp;#34;customerServiceInterceptor&amp;#34; class=&amp;#34;demo.</description>
    </item>
    
    <item>
      <title>SpringBoot使用QQ邮箱发送邮件配置</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/springboot/springboot%E4%BD%BF%E7%94%A8qq%E9%82%AE%E7%AE%B1%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/springboot/springboot%E4%BD%BF%E7%94%A8qq%E9%82%AE%E7%AE%B1%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E9%85%8D%E7%BD%AE/</guid>
      <description>使用SpringBoot Admin 配置QQ邮箱去发送邮件时报错：com.sun.mail.smtp.SMTPSenderFailedException: 501 mail from address must be same as authorization user，我的配置如下：
spring.mail.host: smtp.qq.comspring.mail.username: 发送账号spring.mail.password: qq授权码spring.boot.admin.notify.mail.to: 接收账号后来在网上查到是少了spring.boot.admin.notify.mail.from的配置，貌似只有QQ邮箱才需要额外加上这个设置（本人没有测试过用其他邮箱发送邮件）。所以最终配置如下：
spring.mail.host: smtp.qq.comspring.mail.username: 发送账号spring.mail.password: qq授权码spring.boot.admin.notify.mail.to: 接收账号spring.boot.admin.notify.mail.from: 发送账号参考： Springboot admin 发送邮件失败：com.sun.mail.smtp.SMTPSenderFailedException: 553 Mail from must equal authorized user</description>
    </item>
    
    <item>
      <title>SpringMVC</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/springmvc/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/springmvc/</guid>
      <description>SpringMVC执行流程  用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求调用处理器映射器HandlerMapping。 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 执行处理器Handler(Controller，也叫页面控制器)。 Handler执行完成返回ModelAndView HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 DispatcherServlet响应用户。  组件说明  DispatcherServlet：前端控制器。用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性,系统扩展性提高。由框架实现 HandlerMapping：处理器映射器。HandlerMapping负责根据用户请求的url找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，根据一定的规则去查找,例如：xml配置方式，实现接口方式，注解方式等。由框架实现 Handler：处理器。Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要程序员根据业务需求开发Handler。 HandlAdapter：处理器适配器。通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。由框架实现。 ModelAndView是springmvc的封装对象，将model和view封装在一起。 ViewResolver：视图解析器。ViewResolver负责将处理结果生成View视图，ViewResolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 View: 是springmvc的封装对象，是一个接口, springmvc框架提供了很多的View视图类型，包括：jspview，pdfview,jstlView、freemarkerView、pdfView等。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。  参考链接： SpringMVC执行流程及工作原理Spring MVC【入门】就这一篇！</description>
    </item>
    
    <item>
      <title>Spring事务</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/spring%E4%BA%8B%E5%8A%A1/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/spring%E4%BA%8B%E5%8A%A1/</guid>
      <description>参考链接： 可能是最漂亮的Spring事务管理详解spring 事务的传播机制看这篇就够了Spring 事务管理</description>
    </item>
    
    <item>
      <title>Spring优雅的异常处理</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/spring%E4%BC%98%E9%9B%85%E7%9A%84%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/spring%E4%BC%98%E9%9B%85%E7%9A%84%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</guid>
      <description>SpringBoot的WEB异常捕获，如果是WEB项目的话，可以直接处理Controller中的异常。如果不是WEB项目的话，就需要使用AspectJ来做切面。
1. web项目 package com.test.handler; import lombok.extern.log4j.Log4j2; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.ControllerAdvice; import org.springframework.web.bind.annotation.ExceptionHandler; @ControllerAdvice @Log4j2 public class GlobalExceptionHandler { @ExceptionHandler(value = Exception.class) public String exception(Exception e, Model model){ log.error(&amp;#34;find exception:e={}&amp;#34;,e.getMessage()); model.addAttribute(&amp;#34;mes&amp;#34;,e.getMessage()); return &amp;#34;pages/500&amp;#34;; } } 参考链接：
SpringBootWEB项目和非Web项目的全局异常捕获SpringBoot 处理异常的几种常见姿势使用枚举简单封装一个优雅的 Spring Boot 全局异常处理！2. 非web项目 package com.test.syncbackend.handler; import lombok.extern.log4j.Log4j2; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Pointcut; import org.springframework.stereotype.Component; @Component @Aspect @Log4j2 public class GlobalExceptionHandler { @Pointcut(&amp;#34;execution(* com.test.syncbackend.scheduleds.*.*(..))&amp;#34;) public void pointCut() { } @Around(&amp;#34;pointCut()&amp;#34;) public Object handlerException(ProceedingJoinPoint proceedingJoinPoint) { try { return proceedingJoinPoint.</description>
    </item>
    
    <item>
      <title>SSL/TLS协议</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/ssl-tls%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/ssl-tls%E5%8D%8F%E8%AE%AE/</guid>
      <description>参考链接
http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html</description>
    </item>
    
    <item>
      <title>TCP协议</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/tcp/tcp%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/tcp/tcp%E5%8D%8F%E8%AE%AE/</guid>
      <description>简介 数据在TCP层称为流（Stream），数据分组称为分段（Segment）。作为比较，数据在IP层称为Datagram，数据分组称为分片（Fragment）。 UDP 中分组称为Message。
TCP 数据包的大小 以太网数据包（packet）的大小是固定的，最初是1518字节，后来增加到1522字节。其中， 1500 字节是负载（payload），22字节是头信息（head）。 IP 数据包在以太网数据包的负载里面，它也有自己的头信息，最少需要20字节，所以 IP 数据包的负载最多为1480字节。
（图片说明：IP 数据包在以太网数据包里面，TCP 数据包在 IP 数据包里面。）
TCP 数据包在 IP 数据包的负载里面。它的头信息最少也需要20字节，因此 TCP 数据包的最大负载是 1480 - 20 = 1460 字节。由于 IP 和 TCP 协议往往有额外的头信息，所以 TCP 负载实际为1400字节左右。
因此，一条1500字节的信息需要两个 TCP 数据包。HTTP/2 协议的一大改进， 就是压缩 HTTP 协议的头信息，使得一个 HTTP 请求可以放在一个 TCP 数据包里面，而不是分成多个，这样就提高了速度。
创建连接 TCP用三次握手（或称三路握手，three-way handshake）过程创建一个连接。在连接创建过程中，很多参数要被初始化，例如序号被初始化以保证按序传输和连接的强壮性。 一对终端同时初始化一个它们之间的连接是可能的。但通常是由一端打开一个套接字（socket）然后监听来自另一方的连接，这就是通常所指的被动打开（passive open）。服务器端被被动打开以后，用户端就能开始创建主动打开（active open）。
 客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三次握手的一部分。客户端把这段连接的序号设定为随机数A。 服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK的确认码应为A+1，SYN/ACK包本身又有一个随机产生的序号B。 最后，客户端再发送一个ACK。此时包的序号被设定为A+1，而ACK的确认码则为B+1。当服务端收到这个ACK的时候，就完成了三次握手，并进入了连接创建状态。  注意：三次握手建立连接的时候， SYN/ACK 是一个数据包发送出去的
注意：三次握手建立连接的时候， SYN/ACK 是一个数据包发送出去的
如果服务器端接到了客户端发的SYN后回了SYN-ACK后客户端掉线了，服务器端没有收到客户端回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，服务器端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会断开这个连接。使用三个TCP参数来调整行为：tcp_synack_retries 减少重试次数；tcp_max_syn_backlog，增大SYN连接数；tcp_abort_on_overflow决定超出能力时的行为。</description>
    </item>
    
    <item>
      <title>ThreadLocal</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/threadlocal/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/threadlocal/</guid>
      <description>ThreadLocal 是一个线程的本地变量，也就意味着这个变量是线程独有的，是不能与其他线程共享的，这样就可以避免资源竞争带来的多线程的问题，这种解决多线程的安全问题和lock(这里的lock 指通过synchronized 或者Lock 等实现的锁) 是有本质的区别的:
 lock 的资源是多个线程共享的，所以访问的时候需要加锁。 ThreadLocal 是每个线程都有一个副本，是不需要加锁的。 lock 是通过时间换空间的做法。 ThreadLocal 是典型的通过空间换时间的做法。  当然他们的使用场景也是不同的，关键看你的资源是需要多线程之间共享的还是单线程内部共享的。
ThreadLocal 的使用是非常简单的，看下面的代码：
public class Test { public static void main(String[] args) { ThreadLocal&amp;lt;String&amp;gt; local = new ThreadLocal&amp;lt;&amp;gt;(); //设置值  local.set(&amp;#34;hello word&amp;#34;); //获取刚刚设置的值  System.out.println(local.get()); } } ThreadLocal的数据结构 为什么ThreadLocalMap 采用开放地址法来解决哈希冲突? ThreadLocal 往往存放的数据量不会特别大，这个时候开放地址法简单的结构会显得更省空间（链地址法需要额外的指针空间）
ThreadLocal应用场景 传递参数 ThreadLocal用于传递参数及优势保证线程安全 SimpleDateFormat是线程不安全的，例如下面的写法会报错：
日期转换的一个工具类
public class DateUtil {private static final SimpleDateFormat sdf = new SimpleDateFormat(&amp;quot;yyyy-MM-dd HH:mm:ss&amp;quot;);public static Date parse(String dateStr) {Date date = null;try {date = sdf.</description>
    </item>
    
    <item>
      <title>ThreadLocal线程单例</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/threadlocal%E7%BA%BF%E7%A8%8B%E5%8D%95%E4%BE%8B/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/threadlocal%E7%BA%BF%E7%A8%8B%E5%8D%95%E4%BE%8B/</guid>
      <description>ThreadLocal 保证的是单个线程内部访问的是同一个实例，不同线程访问的不是同一个实例。
package test; public class Singleton { private static final ThreadLocal&amp;lt;Singleton&amp;gt; singleton = new ThreadLocal&amp;lt;Singleton&amp;gt;() { @Override protected Singleton initialValue() { return new Singleton(); } }; public static Singleton getInstance() { return singleton.get(); } private Singleton() { } } package test; public class T implements Runnable { @Override public void run() { Singleton instance = Singleton.getInstance(); System.out.println(instance); } } 测试类：
package test; public class Test { public static void main(String[] args){ System.</description>
    </item>
    
    <item>
      <title>ThreadPoolExecutor</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/threadpoolexecutor/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/threadpoolexecutor/</guid>
      <description>构造方法 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&amp;lt;Runnable&amp;gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &amp;lt; 0 || maximumPoolSize &amp;lt;= 0 || maximumPoolSize &amp;lt; corePoolSize || keepAliveTime &amp;lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/centos-linux-%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/centos-linux-%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4/</guid>
      <description>Centos Linux 清理磁盘空间
 df -hl 查看占比  [root@VM-0-12-centos apioak-document]# df -hl Filesystem Size Used Avail Use% Mounted on devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 1.9G 24K 1.9G 1% /dev/shm tmpfs 1.9G 788K 1.9G 1% /run tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/vda1 50G 48G 0 100% / tmpfs 379M 0 379M 0% /run/user/0 overlay 50G 48G 0 100% /var/lib/docker/overlay2/b3b7c9bcd6a086d488f5ca533ec7fc934f863340e4efa40513c354f3a13c6ccd/merged shm 64M 0 64M 0% /var/lib/docker/containers/76627689f39c84201d3554e95cf7a8ca53bc53f660fbf1127d86c49db2a67591/mounts/shm overlay 50G 48G 0 100% /var/lib/docker/overlay2/f3fac618fb01637e586b483c87d9075d1cb08bee9c9d1f3afffd0a012394b067/merged shm 64M 0 64M 0% /var/lib/docker/containers/36172ef8f5a337d595f41fda4a2e7864b333278a9d2102b700d7f2a6f47c52a8/mounts/shm 在根目录执行du -sh *  [root@VM-0-12-centos /]# du -sh * 0	bin 147M	boot 987M	data 0	dev 6.</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/css%E4%B8%AD%E5%86%85%E8%81%94svg/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/css%E4%B8%AD%E5%86%85%E8%81%94svg/</guid>
      <description>CSS中内联SVG
内联SVG比内联图像的base64效果更好，图片更加保真：
.icon-arrow-down { width: 20px; height: 20px; background: url(&amp;#39;data:image/svg+xml;utf8,&amp;lt;svg version=&amp;#34;1.1&amp;#34; xmlns=&amp;#34;http://www.w3.org/2000/svg&amp;#34; width=&amp;#34;200&amp;#34; height=&amp;#34;200&amp;#34; viewBox=&amp;#34;0 0 200 200&amp;#34;&amp;gt;&amp;lt;path fill=&amp;#34;#00A5E0&amp;#34; d=&amp;#34;M145.659,68.949c-5.101-5.208-13.372-5.208-18.473,0L99.479,97.233 L71.772,68.949c-5.1-5.208-13.371-5.208-18.473,0c-5.099,5.208-5.099,13.648,0,18.857l46.18,47.14l46.181-47.14 C150.759,82.598,150.759,74.157,145.659,68.949z&amp;#34;/&amp;gt;&amp;lt;/svg&amp;gt;&amp;#39;) no-repeat center; background-size: 100%; } </description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/docker-compose-%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/docker-compose-%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/</guid>
      <description>Docker Compose 网络设置
基本概念 默认情况下，Compose 会为我们的应用创建一个网络，服务的每个容器都会加入该网络中。这样，容器就可被该网络中的其他容器访问，不仅如此，该容器还能以服务名称作为 hostname 被其他容器访问。
默认情况下，应用程序的网络名称基于 Compose 的工程名称，而项目名称基于 docker-compose.yml 所在目录的名称。如需修改工程名称，可使用&amp;ndash;project-name 标识或 COMPOSE_PORJECT_NAME 环境变量。
举个例子，假如一个应用程序在名为 myapp 的目录中，并且 docker-compose.yml 如下所示：
version: &amp;#39;2&amp;#39; services: web: build: . ports: - &amp;#34;8000:8000&amp;#34; db: image: postgres 当我们运行 docker-compose up 时，将会执行以下几步：
- 创建一个名为 myapp_default 的网络； - 使用 web 服务的配置创建容器，它以“web”这个名称加入网络 myapp_default； - 使用 db 服务的配置创建容器，它以“db”这个名称加入网络 myapp_default。
容器间可使用服务名称（web 或 db）作为 hostname 相互访问。例如，web 这个服务可使用postgres://db:5432 访问 db 容器。
更新容器 当服务的配置发生更改时，可使用 docker-compose up 命令更新配置。
此时，Compose 会删除旧容器并创建新容器。新容器会以不同的 IP 地址加入网络，名称保持不变。任何指向旧容器的连接都会被关闭，容器会重新找到新容器并连接上去。
links 前文讲过，默认情况下，服务之间可使用服务名称相互访问。links 允许我们定义一个别名，从而使用该别名访问其他服务。举个例子：</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/docker-compose.yml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/docker-compose.yml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid>
      <description>docker-compose.yml文件详解
Compose和Docker兼容性： Compose 文件格式有3个版本,分别为1, 2.x 和 3.x 目前主流的为 3.x 其支持 docker 1.13.0 及其以上的版本 常用参数： version  # 指定 compose 文件的版本 services  # 定义所有的 service 信息, services 下面的第一级别的 key 既是一个 service 的名称 build  # 指定包含构建上下文的路径, 或作为一个对象，该对象具有 context 和指定的 dockerfile 文件以及 args 参数值 context # context: 指定 Dockerfile 文件所在的路径 dockerfile # dockerfile: 指定 context 指定的目录下面的 Dockerfile 的名称(默认为 Dockerfile) args # args: Dockerfile 在 build 过程中需要的参数 (等同于 docker container build --build-arg 的作用) cache_from  # v3.</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/gitbook%E5%AE%89%E8%A3%85/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/gitbook%E5%AE%89%E8%A3%85/</guid>
      <description>gitbook安装
 安装  sudo npm install gitbook -g sudo npm install -g gitbook-cli 验证  gitbook -V 初始化项目  mkdir direName //创建自己的文件夹目录 cd direName //进入到自己的gitbook文件夹目录 gitbook init //初始化gitbook项目 启动  gitbook serve 或者gitbook serve &amp;amp;后台运行，默认在4000端口启动。</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/iptables_-no-chain_target_match-by-that-name/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/iptables_-no-chain_target_match-by-that-name/</guid>
      <description>iptables: No chain/target/match by that name.
重启redis镜像的时候报错如下：
ERROR: for redis Cannot start service redis: driver failed programming external connectivity on endpoint redis (f5211771e5a0ee705edb72f8a8dfbca2503456ab0e8330a32932b029a7c0568d): (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 6379 -j DNAT --to-destination 192.168.80.2:6379 ! -i br-fbefac0273aa: iptables: No chain/target/match by that name. 原因：
docker 服务启动的时候，docker服务会向iptables注册一个链，以便让docker服务管理的containner所暴露的端口之间进行通信。通过命令iptables -L可以查看iptables 链。如果你删除了iptables中的docker链，或者iptables的规则被丢失了（例如重启firewalld，我就是使用了systemctl stop iptables导致链丢失），docker就会报这个错误。
解决办法：
systemctl restart docker 重启docker服务，之后，正确的iptables规则就会被创建出来。
参考：
Docker 启动时报错：iptables:No chain/target/match by the name</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/linux%E9%85%8D%E7%BD%AEgit%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/linux%E9%85%8D%E7%BD%AEgit%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81/</guid>
      <description>Linux配置git账号密码
1. 在~/下， touch创建文件 .git-credentials, 用vim编辑此文件 touch .git-credentials vim .git-credentials 在里面按“i”然后输入： https://{username}:{password}@github.com 比如 https://account:password@github.com 2. 在终端下执行 git config --global credential.helper store 3. 可以看到~/.gitconfig文件，会多了一项 [credential] helper = store </description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/navicat%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5mysql2003-can_t-connect-to-mysql-serve/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/navicat%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5mysql2003-can_t-connect-to-mysql-serve/</guid>
      <description>navicat远程连接mysql，2003 can&amp;rsquo;t connect to mysql server on 10038
首先我们通过
①：netstat -an|grep 3306
来查看mysql默认的端口3306是否开启，允许哪个ip使用，如果你发现，前面有127.0.0.1，就说明，3306端口只能本机ip使用
所以，我们需要
②：打开mysql配置文件vi /etc/mysql/mysql.conf.d/mysqld.cnf
将bind-address = 127.0.0.1注销
③：进入mysql，对远程用户进行授权，
grant all privileges on . to &amp;lsquo;root&amp;rsquo;@&#39;%&#39; identified by &amp;lsquo;xxxxxx&amp;rsquo;;
这里的root 是你远程登录的用户，xxxxxx是你登录使用的密码，然后可以在mysql数据 表中查看到你这个用户已经被添加到user表中
④：service mysql restart</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/net__err_content_length_mismatch-200-ok/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/net__err_content_length_mismatch-200-ok/</guid>
      <description>net::ERR_CONTENT_LENGTH_MISMATCH 200 (OK)
加载静态资源时报错：net::ERR_CONTENT_LENGTH_MISMATCH 200 (OK)
解决办法：调整缓冲区大小 proxy_buffer_size 64k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k;
server { listen 80; server_name tkaid.com www.tkaid.com; location / { proxy_buffer_size 64k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_pass http://k8s_tkb-web-svc; } } 如果仍然报这个错，可以再将值设置得大一点。</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86kibana%E6%8A%A5kibana-did-not-load-properly.chec/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86kibana%E6%8A%A5kibana-did-not-load-properly.chec/</guid>
      <description>nginx反向代理kibana报：Kibana did not load properly.Check the server output for more information.
如题所述，直接访问5601端口不会报错，一旦用ngnix反向代理就报错。
原因：应该是kibana的启动用户没有访问nginx/proxy_temp文件夹的权限，导致部分静态资源无法加载。
 奇怪的是：1. 我的proxy_temp文件下一个文件都没有 2. 我的nginx error.log日志也没有报类似Permission denied的错误。 这里暂且不管。。。
 解决办法:
chmod -R 777 proxy_temphttps://www.cnblogs.com/operationhome/p/9901580.html</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/ubuntu%E4%BF%AE%E6%94%B9ip/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/ubuntu%E4%BF%AE%E6%94%B9ip/</guid>
      <description>Ubuntu修改ip
方式一 这种方式可以修改ip地址，不能访问互联网；虚拟机100必须以这种方式配置（101、102、103是复制100而来）
1、sudo vi /etc/netplan/50-cloud-init.yaml
network: ethernets: enp0s3: addresses: [10.0.2.15/24] dhcp4: true enp0s8: addresses: [192.168.56.100/24] dhcp4: false version: 2 2、重启虚拟机
方式二 这种方式可以修改ip地址，能访问互联网；虚拟机101、102、103以这种方式配置
1、注释50-cloud-init.yaml里面的修改
2、sudo vim /etc/network/interfaces
# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface(NAT) auto enp0s3 iface enp0s3 inet dhcp # 增加的Host-only静态IP设置 (enp0s8 是根据拓扑关系映射的网卡名称（旧规则是eth0,eth1）) # 可以通过 ```ls /sys/class/net```查看，是否为enp0s8 auto enp0s8 iface enp0s8 inet static address 192.</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/ubuntu%E4%BF%AE%E6%94%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/ubuntu%E4%BF%AE%E6%94%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/</guid>
      <description>1、vim /etc/resolv.conf
nameserver 8.8.8.8 nameserver 114.114.114.114 nameserver 1.2.4.8 2、vim /etc/network/interfaces
dns-nameserver 8.8.8.8 dns-nameserver 114.114.114.114 dns-nameserver 1.2.4.8 3、/etc/init.d/networking restart</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/docker-compose%E7%9A%84%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2wordpress/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/docker-compose%E7%9A%84%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2wordpress/</guid>
      <description>docker-compose的方式部署wordpress
 安装docker  sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io  编写docker-compose.yml  mysql: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=wordpress web: image: wordpress links: - mysql environment: - WORDPRESS_DB_PASSWORD=123456 ports: - &amp;#34;0.0.0.0:8080:80&amp;#34; working_dir: /var/www/html volumes: - wordpress:/var/www/html  启动docker  sudo systemctl start docker 运行docker-compose  docker-compose up 参考：
What does &amp;ldquo;local address&amp;rdquo; and &amp;ldquo;foreign address&amp;rdquo; mean in the netstat command result?</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/nginx-%E8%AE%BF%E9%97%AE-.php%E6%96%87%E4%BB%B6%E5%8F%98%E6%88%90%E4%B8%8B%E8%BD%BDchrome-%E6%88%96%E8%80%85%E7%9B%B4%E6%8E%A5%E6%98%BE%E7%A4%BA%E6%BA%90%E7%A0%81edge/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/nginx-%E8%AE%BF%E9%97%AE-.php%E6%96%87%E4%BB%B6%E5%8F%98%E6%88%90%E4%B8%8B%E8%BD%BDchrome-%E6%88%96%E8%80%85%E7%9B%B4%E6%8E%A5%E6%98%BE%E7%A4%BA%E6%BA%90%E7%A0%81edge/</guid>
      <description>nginx 访问 .php文件变成下载(chrome) 或者直接显示源码(edge)
原因：这是因为nginx没有设置好碰到php文件时，要传递到后方的php解释器。
需要在nginx.conf的server{}添加如下内容：
location ~ [^/]\.php(/|$) { #fastcgi_pass remote_php_ip:9000; fastcgi_pass unix:/dev/shm/php-cgi.sock; fastcgi_index index.php; include fastcgi.conf; } </description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/woocommerce_rest_authentication_error/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/woocommerce_rest_authentication_error/</guid>
      <description>woocommerce_rest_authentication_error
nginx配置错误导致woocommerce REST API不可用
It was a bad configuration of try_files in nginx:
WRONG
try_files $uri $uri/ /index.php?q=$uri&amp;amp;$args; CORRECT
try_files $uri $uri/ /index.php$is_args$args; After changing that everything works perfectly ^^
参考： woocommerce_rest_authentication_error on localhost</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress-%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6-curl-error-77%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress-%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6-curl-error-77%E8%A7%A3%E5%86%B3/</guid>
      <description>WordPress 安装插件 cURL error 77解决
第一步：执行命令
yum install ca-certificates 第二步：重启php-fpm
/etc/init.d/php-fpm restart </description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress%E6%94%B9%E9%80%A0%E6%88%90https%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress%E6%94%B9%E9%80%A0%E6%88%90https%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</guid>
      <description>WordPress改造成https的注意事项
WordPress加入ssl后可能出现的网站访问缓慢、样式无法被加载，是由于站点虽然被改造成了https访问，但是 WordPress 代码层面对于一些css、js、图片等静态资源的访问还是http的，所以才会出现这种情况。解决的办法可以改造代码，也可以安装WordPress插件。下面介绍后者的步骤：
1、申请证书、上传证书到服务器、配置服务器（阿里云有免费的证书，并有详细的操作步骤）
2、安装 Really Simple SSL插件，它会将http的请求全都转成https（感谢作者吧）
3、 后台修改wordpress地址和站点地址，如下图所示：
参考链接：https://www.rogoso.info/wordpress-ssl/</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress%E6%96%87%E7%AB%A0%E5%8F%91%E5%B8%83%E5%90%8Enginx%E6%8A%A5404%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress%E6%96%87%E7%AB%A0%E5%8F%91%E5%B8%83%E5%90%8Enginx%E6%8A%A5404%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</guid>
      <description>wordpress文章发布后，nginx报404解决方法
修改nginx.conf文件，在location /节点下添加如下代码：
location / { try_files $uri $uri/ /index.php?q=$uri&amp;amp;$args; } 然后重启nginx即可解决。</description>
    </item>
    
    <item>
      <title>Ubuntu修改域名解析</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress%E6%9B%B4%E6%8D%A2%E5%9F%9F%E5%90%8D/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/wordpress/wordpress%E6%9B%B4%E6%8D%A2%E5%9F%9F%E5%90%8D/</guid>
      <description>WordPress更换域名
在更换的域名过程中遇到很多坑，主要还是我的架构比较特殊的原因，导致跟以往配置不太一样，架构如下：
1. 无法通过nginx转发请求到容器端口 原因：nginx配置不正确
解决：补充缺失的如下配置
add_header X-Frame-Options SAMEORIGIN; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_hide_header X-Frame-Options; 最终类似：
server { listen 443 ssl; listen [::]:443 ssl; include snippets/ssl-params.conf; server_name wptest.your-awesome-domain.com; # domain當然要用自己的，subdomain請隨自己喜好 location / { add_header X-Frame-Options SAMEORIGIN; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_hide_header X-Frame-Options; proxy_pass http://localhost:8000; # 注意這邊跟上面docker-compose設定的port相同 } } 2. 提示“重定向次数过多” 修改wordpress根目录下的wp-config.php：
$_SERVER[&amp;#39;HTTPS&amp;#39;] = &amp;#39;on&amp;#39;; define(&amp;#39;FORCE_SSL_LOGIN&amp;#39;, true); define(&amp;#39;FORCE_SSL_ADMIN&amp;#39;, true); 参考链接： https://softman.</description>
    </item>
    
    <item>
      <title>Zookeeper应用场景</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/</guid>
      <description>Zookeeper应用场景
zookeeper应用场景 大致来说，zookeeper 的使用场景如下，我就举几个简单的，大家能说几个就好了：
 分布式协调 分布式锁 元数据/配置信息管理 HA 高可用性  分布式协调 这个其实是 zookeeper 很经典的一个用法，简单来说，就好比，你 A 系统发送个请求到 mq，然后 B 系统消息消费之后处理了。那 A 系统如何知道 B 系统的处理结果？用 zookeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 系统立马就可以收到通知，完美解决。
分布式锁 举个栗子。对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。
元数据/配置信息管理 zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等很多分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心不也支持 zookeeper 么？
HA 高可用性 这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。
参考 zookeeper 都有哪些使用场景？</description>
    </item>
    
    <item>
      <title>Zookeeper服务注册发现原理</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%8F%91%E7%8E%B0%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%8F%91%E7%8E%B0%E5%8E%9F%E7%90%86/</guid>
      <description>Zookeeper服务注册发现原理 RPC框架中有3个重要的角色：
 注册中心 ：保存所有服务的名字，服务提供者的ip列表，服务消费者的IP列表 服务提供者： 提供跨进程服务 服务消费者： 寻找到指定命名的服务并消费。  Zookeeper用作注册中心 简单来讲，zookeeper可以充当一个服务注册表（Service Registry），让多个服务提供者形成一个集群，让服务消费者通过服务注册表获取具体的服务访问地址（ip+端口）去访问具体的服务提供者。如下图所示：
具体来说，zookeeper就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到zookeeper的某一路径上: /{service}/{version}/{ip:port}, 比如我们的HelloWorldService部署到两台机器，那么zookeeper上就会创建两条目录：分别为/HelloWorldService/1.0.0/100.19.20.01:16888 /HelloWorldService/1.0.0/100.19.20.02:16888。
这么描述有点不好理解，下图更直观，
在zookeeper中，进行服务注册，实际上就是在zookeeper中创建了一个znode节点，该节点存储了该服务的IP、端口、调用方式(协议、序列化方式)等。该节点承担着最重要的职责，它由服务提供者(发布服务时)创建，以供服务消费者获取节点中的信息，从而定位到服务提供者真正网络拓扑位置以及得知如何调用。RPC服务注册、发现过程简述如下：
 服务提供者启动时，会将其服务名称，ip地址注册到配置中心。 服务消费者在第一次调用服务时，会通过注册中心找到相应的服务的IP地址列表，并缓存到本地，以供后续使用。当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从IP列表中取一个服务提供者的服务器调用服务。 当服务提供者的某台服务器宕机或下线时，相应的ip会从服务提供者IP列表中移除。同时，注册中心会将新的服务IP地址列表发送给服务消费者机器，缓存在消费者本机。 当某个服务的所有服务器都下线了，那么这个服务也就下线了。 同样，当服务提供者的某台服务器上线时，注册中心会将新的服务IP地址列表发送给服务消费者机器，缓存在消费者本机。 服务提供方可以根据服务消费者的数量来作为服务下线的依据。  感知服务的下线&amp;amp;上线 zookeeper提供了“心跳检测”功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如100.19.20.02这台机器如果宕机了，那么zookeeper上的路径就会只剩/HelloWorldService/1.0.0/100.19.20.01:16888。
服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。
更为重要的是zookeeper 与生俱来的容错容灾能力（比如leader选举），可以确保服务注册表的高可用性。
使用 zookeeper 作为注册中心时，客户端订阅服务时会向 zookeeper 注册自身；主要是方便对调用方进行统计、管理。但订阅时是否注册 client 不是必要行为，和不同的注册中心实现有关，例如使用 consul 时便没有注册。
参考 Zookeeper用作注册中心的原理8、Zookeeper服务注册与发现原理浅析微服务中Zookeeper的应用及原理</description>
    </item>
    
    <item>
      <title>Zookeeper核心设计理解与实战</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E6%A0%B8%E5%BF%83%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E6%A0%B8%E5%BF%83%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/</guid>
      <description>Zookeeper核心设计理解与实战 一、前言 想起很久以前在某个客户现场，微服务 B 突然无法调用到微服务 A，为了使服务尽快正常恢复，重启了微服务 B 。
但客户不依不饶询问这个问题出现的原因，于是我还大老远从杭州飞到深圳，现场排查问题。
最后的结论是，zk 在某时刻出现主备切换，此时微服务 A（基于 dubbo）需要重新往 zk上注册，但是端口号变了。
但是微服务 B 本地有微服务 A rpc 接口的缓存，缓存里面还是旧的端口，所以调用不到。
解决方法就是，把微服务的 rpc 端口号改成固定的。
虽说原因找到了，但对于 Zookeeper 的理解还是不够深刻，于是重新学习了 Zookeeper 的核心设计，并记录于此文共勉。
二、Zookeeper 核心架构设计 1、Zookeeper 特点 （1）Zookeeper 是一个分布式协调服务，是为了解决多个节点状态不一致的问题，充当中间机构来调停。如果出现了不一致，则把这个不一致的情况写入到 Zookeeper 中，Zookeeper 会返回响应，响应成功，则表示帮你达成了一致。
比如，A、B、C 节点在集群启动时，需要推举出一个主节点，这个时候，A、B、C 只要同时往 Zookeeper 上注册临时节点，谁先注册成功，谁就是主节点。
（2）Zookeeper 虽然是一个集群，但是数据并不是分散存储在各个节点上的，而是每个节点都保存了集群所有的数据。
其中一个节点作为主节点，提供分布式事务的写服务，其他节点和这个节点同步数据，保持和主节点状态一致。
（3）Zookeeper 所有节点的数据状态通过 Zab 协议保持一致。当集群中没有 Leader 节点时，内部会执行选举，选举结束，Follower 和 Leader 执行状态同步；当有 Leader 节点时，Leader 通过 ZAB 协议主导分布式事务的执行，并且所有的事务都是串行执行的。
（4）Zookeeper 的节点个数是不能线性扩展的，节点越多，同步数据的压力越大，执行分布式事务性能越差。推荐3、5、7 这样的数目。
2、Zookeeper 角色的理解 Zookeeper 并没有沿用 Master/Slave 概念，而是引入了 Leader，Follower，Observer 三种角色。</description>
    </item>
    
    <item>
      <title>ZooKeeper的stat结构</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E7%9A%84stat%E7%BB%93%E6%9E%84/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E7%9A%84stat%E7%BB%93%E6%9E%84/</guid>
      <description>ZooKeeper的stat结构 ZooKeeper命名空间中的每个znode都有一个与之关联的stat结构，类似于Unix/Linux文件系统中文件的stat结构。 znode的stat结构中的字段显示如下，各自的含义如下：
 cZxid：这是导致创建znode更改的事务ID。 mZxid：这是最后修改znode更改的事务ID。 pZxid：这是用于添加或删除子节点的znode更改的事务ID。 ctime：表示从1970-01-01T00:00:00Z开始以毫秒为单位的znode创建时间。 mtime：表示从1970-01-01T00:00:00Z开始以毫秒为单位的znode最近修改时间。 dataVersion：表示对该znode的数据所做的更改次数。 cversion：这表示对此znode的子节点进行的更改次数。 aclVersion：表示对此znode的ACL进行更改的次数。 ephemeralOwner：如果znode是ephemeral类型节点，则这是znode所有者的 session ID。 如果znode不是ephemeral节点，则该字段设置为零。 dataLength：这是znode数据字段的长度。 numChildren：这表示znode的子节点的数量。  在ZooKeeper Java shell中，可以使用stat或ls2命令查看znode的stat结构。 具体说明如下：
使用stat命令查看znode的stat结构：
[zk: localhost(CONNECTED) 0] stat /zookeeper cZxid = 0x0 ctime = Thu Jan 01 05:30:00 IST 1970 mZxid = 0x0 mtime = Thu Jan 01 05:30:00 IST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 使用ls2命令查看znode的stat结构：</description>
    </item>
    
    <item>
      <title>ZooKeeper能做什么</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E8%83%BD%E5%81%9A%E4%BB%80%E4%B9%88/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/zookeeper/zookeeper%E8%83%BD%E5%81%9A%E4%BB%80%E4%B9%88/</guid>
      <description>ZooKeeper能做什么？ 什么是ZooKeeper?官网介绍到：ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. 这大概描述了Zookeeper主要可以干哪些事情：配置管理，名字服务，提供分布式同步以及集群管理。
1. 配置管理 配置管理又被称为发布-订阅。
在我们的应用中除了代码外，还有一些就是各种配置。比如数据库连接等。一般我们都是使用配置文件的方式，在代码中引入这些配置文件。但是当我们只有一种配置，只有一台服务器，并且不经常修改的时候，使用配置文件是一个很好的做法，但是如果我们配置非常多，有很多服务器都需要这个配置，而且还可能是动态的话使用配置文件就不是个好主意了。这个时候往往需要寻找一种集中管理配置的方法，我们在这个集中的地方修改了配置，所有对这个配置感兴趣的都可以获得变更。比如我们可以把配置放在数据库里，然后所有需要配置的服务都去这个数据库读取配置。但是，因为很多服务的正常运行都非常依赖这个配置，所以需要这个集中提供配置服务的服务具备很高的可靠性。一般我们可以用一个集群来提供这个配置服务，但是用集群提升可靠性，那如何保证配置在集群中的一致性呢？ 这个时候就需要使用一种实现了一致性协议的服务了。Zookeeper就是这种服务，它使用Zab这种一致性协议来提供一致性。现在有很多开源项目使用Zookeeper来维护配置，比如在HBase中，客户端就是连接一个Zookeeper，获得必要的HBase集群的配置信息，然后才可以进一步操作。还有在开源的消息队列Kafka中，也使用Zookeeper来维护broker的信息。
应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。
2. 命名服务 命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。
阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，点击这里查看Dubbo开源项目。在Dubbo实现中： 服务提供者在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。 服务消费者启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName} /consumers目录下写入自己的URL地址。 注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。 另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。
3. 分布式锁 分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是保持独占（排他锁），另一个是控制时序（共享锁/读锁）。
所谓保持独占（排他锁），就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock节点，最终成功创建的那个客户端也即拥有了这把锁。
控制时序（共享锁/读锁）是指所有试图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里/distribute_lock已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。
4. 集群管理 在分布式的集群中，经常会由于各种原因，比如硬件故障，软件故障，网络问题，有些节点会进进出出。有新的节点加入进来，也有老的节点退出集群。这个时候，集群中其他机器需要感知到这种变化，然后根据这种变化做出对应的决策。
4.1 Master选举 Master选举则是ZooKeeper中最为经典的应用场景了。比如 HDFS 中 Active NameNode 的选举。在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。
利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群Master选举了。成功创建该节点的客户端所在的机器就成为了Master。同时，其他没有成功创建该节点的客户端，都会在该节点上注册一个子节点变更的 Watcher，用于监控当前 Master 机器是否存活，一旦发现当前的Master挂了，那么其他客户端将会重新进行 Master 选举。这样就实现了 Master 的动态选举。
4.2 集群机器监控 这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。这种做法可行，但是存在两个比较明显的问题：
 将会产生一定的时延（受心跳长短限制）; 当集群中的节点发生变更时，其余的节点都需要对维护的集群文件（状态表）进行修改，修改内容多。  利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：</description>
    </item>
    
    <item>
      <title>【转】MySQL索引的工作原理</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E8%BD%ACmysql%E7%B4%A2%E5%BC%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E8%BD%ACmysql%E7%B4%A2%E5%BC%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</guid>
      <description>【转】MySQL索引的工作原理 索引是一种加快查询的数据结构，在 MySQL 中，索引的数据结构选择的是 B+Tree，至于 B+Tree 是什么以及为什么 MySQL 为什么选择 B+Tree 来作为索引，可以去查看公众号的前三篇文章。
  索引数据结构之 B-Tree 与 B+Tree（上篇）  索引数据结构之 B-Tree 与 B+Tree（下篇）  MySQL 为什么不用数组、哈希表、二叉树等数据结构作为索引呢今天主要来聊聊 MySQL 中索引的工作原理，这一部分的知识，在工作中经常被使用到，在面试中也几乎是必问的。所以，不管是面试造火箭，还是工作拧螺丝，掌握索引的工作原理，都是十分有必要的。
  首先需要说明的是，本文的所有讨论均是基于 InnoDB 存储引擎为前提。
示例表 为了方便说明，我们先创建一个示例表。建表语句如下
CREATE TABLE user ( `id` BIGINT ( 11 ) NOT NULL AUTO_INCREMENT, `name` VARCHAR ( 64 ) COMMENT &amp;#39;姓名&amp;#39;, `age` INT ( 4 ) COMMENT &amp;#39;年龄&amp;#39;, PRIMARY KEY ( `id` ), INDEX ( NAME ) ) ENGINE = INNODB COMMENT &amp;#39;用户表&amp;#39;; INSERT INTO `user` ( `name`, `age` ) VALUES ( &amp;#39;AA&amp;#39;, 30 ),( &amp;#39;BB&amp;#39;, 33 ),( &amp;#39;CC&amp;#39;, 31 ),( &amp;#39;DD&amp;#39;, 30 ),( &amp;#39;EE&amp;#39;, 29 ) 复制代码 在上面的 SQL 语句中，创建了一张 user 表，表中有三个字段，id 是主键，name 和 age 分别表示用户的姓名和年龄，同时还为字段 name 创建了一个普通索引。为了方便后面描述，因此还向表中插入了 5 条数据，由于主键 id 是自增的，所以这五行数据的 id 值分为是 1~5。</description>
    </item>
    
    <item>
      <title>【转】redis主从复制</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E8%BD%ACredis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E8%BD%ACredis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</guid>
      <description>一、主从复制概述 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。
默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。
主从复制的作用
主从复制的作用主要包括：
 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。  二、如何使用主从复制 为了更直观的理解主从复制，在介绍其内部原理之前，先说明我们需要如何操作才能开启主从复制。
1. 建立复制 需要注意，主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。
从节点开启主从复制，有3种方式：
（1）配置文件
在从服务器的配置文件中加入：slaveof （2）启动命令
redis-server启动命令后加入 &amp;ndash;slaveof （3）客户端命令
Redis服务器启动后，直接通过客户端执行命令：slaveof ，则该Redis实例成为从节点。
上述3种方式是等效的，下面以客户端命令的方式为例，看一下当执行了slaveof后，Redis主节点和从节点的变化。
2. 实例 准备工作：启动两个节点 方便起见，实验所使用的主从节点是在一台机器上的不同Redis实例，其中主节点监听6379端口，从节点监听6380端口；从节点监听的端口号可以在配置文件中修改：
启动后可以看到：
两个Redis节点启动后（分别称为6379节点和6380节点），默认都是主节点。
建立复制 此时在6380节点执行slaveof命令，使之变为从节点：
观察效果 下面验证一下，在主从复制建立后，主节点的数据会复制到从节点中。
（1）首先在从节点查询一个不存在的key：
（2）然后在主节点中增加这个key：
（3）此时在从节点中再次查询这个key，会发现主节点的操作已经同步至从节点：
（4）然后在主节点删除这个key：
（5）此时在从节点中再次查询这个key，会发现主节点的操作已经同步至从节点：
3. 断开复制 通过slaveof 命令建立主从复制关系以后，可以通过slaveof no one断开。需要注意的是，从节点断开复制后，不会删除已有的数据，只是不再接受主节点新的数据变化。
从节点执行slaveof no one后，打印日志如下所示；可以看出断开复制后，从节点又变回为主节点。
主节点打印日志如下：
三、主从复制的实现原理 上面一节中，介绍了如何操作可以建立主从关系；本小节将介绍主从复制的实现原理。
主从复制过程大体可以分为3个阶段：连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段；下面分别进行介绍。
1. 连接建立阶段 该阶段的主要作用是在主从节点之间建立连接，为数据同步做好准备。
步骤1：保存主节点信息 从节点服务器内部维护了两个字段，即masterhost和masterport字段，用于存储主节点的ip和port信息。
需要注意的是，slaveof是异步命令，从节点完成主节点ip和port的保存后，向发送slaveof命令的客户端直接返回OK**，实际的复制操作在这之后才开始进行。**
这个过程中，可以看到从节点打印日志如下：
步骤2：建立socket连接 从节点每秒1次调用复制定时函数replicationCron()，如果发现了有主节点可以连接，便会根据主节点的ip和port，创建socket连接。如果连接成功，则：
从节点：为该socket建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收RDB文件、接收命令传播等。
主节点：接收到从节点的socket连接后（即accept之后），为该socket创建相应的客户端状态，并将从节点看做是连接到主节点的一个客户端，后面的步骤会以从节点向主节点发送命令请求的形式来进行。
这个过程中，从节点打印日志如下：
步骤3：发送ping命令 从节点成为主节点的客户端之后，发送ping命令进行首次请求，目的是：检查socket连接是否可用，以及主节点当前是否能够处理请求。
从节点发送ping命令后，可能出现3种情况：
（1）返回pong：说明socket连接正常，且主节点当前可以处理请求，复制过程继续。</description>
    </item>
    
    <item>
      <title>【转】你了解的可见性可能是错的</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E8%BD%AC%E4%BD%A0%E4%BA%86%E8%A7%A3%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7%E5%8F%AF%E8%83%BD%E6%98%AF%E9%94%99%E7%9A%84/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E8%BD%AC%E4%BD%A0%E4%BA%86%E8%A7%A3%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7%E5%8F%AF%E8%83%BD%E6%98%AF%E9%94%99%E7%9A%84/</guid>
      <description>背景 这篇文章最开始再我的群里面有讨论过，当时想写的这篇文章的，但是因为一些时间的关系所以便没有写。最近阅读微信文章的时候发现了一篇零度写的一篇文章《分享一道阿里Java并发面试题》，对于有关Java并发性技术的文章我一般还是挺感兴趣的，于是阅读了一下，整体来说还是挺不错的，但是其中犯了一个验证可见性的问题。由于微信文章回复不方便讨论，于是我便把之前一些和群友的讨论在这里写出来。
如何测试可见性问题 因为在群里面我们习惯的有每周一问，也就由我或者群友发现一些由意思的问题然后提问给大家，让大家参与讨论，当时我提出了一个如何测试vlolatile可见性的问题，首先在Effective Java给出了一个测试volatile可见性的例子:
import java.util.concurrent.*; public class Test { private static /*volatile*/ boolean stop = false; public static void main(String[] args) throws Exception { Thread t = new Thread(new Runnable() { public void run() { int i = 0; while (!stop) { i++; // System.out.println(&amp;#34;hello&amp;#34;);  } } }); t.start(); Thread.sleep(1000); TimeUnit.SECONDS.sleep(1); System.out.println(&amp;#34;Stop Thread&amp;#34;); stop = true; } } 这里大家可以复制上面的代码，你会发现这里程序永远不会结束，在零度的那篇文章中也给出了一个测试可见性的例子:
public class ThreadSafeCache { int result; public int getResult() { return result; } public synchronized void setResult(int result) { this.</description>
    </item>
    
    <item>
      <title>一个SQL执行的很慢</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E4%B8%80%E4%B8%AAsql%E6%89%A7%E8%A1%8C%E7%9A%84%E5%BE%88%E6%85%A2/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E4%B8%80%E4%B8%AAsql%E6%89%A7%E8%A1%8C%E7%9A%84%E5%BE%88%E6%85%A2/</guid>
      <description>一个SQL执行的很慢，我们要分两种情况讨论：
大多数情况下很正常，偶尔很慢  数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。  当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在内存中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。写redo log是顺序io
 执行的时候，遇到锁，如表锁、行锁。  这条 SQL 语句一直执行的很慢  没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。 数据库选错了索引。  参考链接： 腾讯面试：一条SQL语句执行得很慢的原因有哪些？&amp;mdash;不看后悔系列</description>
    </item>
    
    <item>
      <title>为什么需要protobuf</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E6%8B%BE%E9%81%97/%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81protobuf/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E6%8B%BE%E9%81%97/%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81protobuf/</guid>
      <description>为什么需要protobuf  protobuf采用字节编码，而json, xml都是字符编码，字节编码更加节省空间 采用了varint编码，进一步降低了编码后的空间大小  Varint就是一种对数字进行编码的方法，编码后二进制数据是不定长的，数值越小的数字使用的字节数越少。例如对于int32_t，采用Varint编码后需要1~5个bytes，小的数字使用1个byte，大的数字使用5个bytes。基于实际场景中小数字的使用远远多于大数字，因此通过Varint编码对于大部分场景都可以起到一个压缩的效果。</description>
    </item>
    
    <item>
      <title>事务传播实战</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E5%AE%9E%E6%88%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E5%AE%9E%E6%88%98/</guid>
      <description>参考链接： 手把手带你实战下Spring的七种事务传播行为Spring的PROPAGATION_NESTED和PROPAGATION_REQUIRES_NEW的区别？</description>
    </item>
    
    <item>
      <title>事务隔离级别和实现原理</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</guid>
      <description>事务具有原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）四个特性，简称 ACID，缺一不可。今天要说的就是隔离性。
1. 概念说明 以下几个概念是事务隔离级别要实际解决的问题，所以需要搞清楚都是什么意思。
脏读 脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。
可重复读 可重复读指的是在一个事务内，最开始读到的数据和事务结束前的任意时刻读到的同一批数据都是一致的。通常针对数据**更新（UPDATE）**操作。
不可重复读 对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据**更新（UPDATE）**操作。
幻读 幻读是针对数据**插入（INSERT）**操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。
2. 事务隔离级别 SQL 标准定义了四种隔离级别，MySQL 全都支持。这四种隔离级别分别是：
 读未提交（READ UNCOMMITTED） 读提交 （READ COMMITTED） 可重复读 （REPEATABLE READ） 串行化 （SERIALIZABLE）  从上往下，隔离强度逐渐增强，性能逐渐变差。采用哪种隔离级别要根据系统需求权衡决定，其中，可重复读是 MySQL 的默认级别。
事务隔离其实就是为了解决上面提到的脏读、不可重复读、幻读这几个问题，下面展示了 4 种隔离级别对这三个问题的解决程度。
只有串行化的隔离级别解决了全部这 3 个问题，其他的 3 个隔离级别都有缺陷。
3. 标准SQL事务隔离级别实现原理 我们上面遇到的问题其实就是并发事务下的控制问题，解决并发事务的最常见方式就是悲观并发控制了（也就是数据库中的锁）。标准SQL事务隔离级别的实现是依赖锁的，我们来看下具体是怎么实现的：
   事务隔离级别 实现方式     未提交读（RU） 事务对当前被读取的数据不加锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级共享锁，直到事务结束才释放。   提交读（RC） 事务对当前被读取的数据加行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结束才释放。   可重复读（RR） 事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结束才释放。   序列化读（S） 事务在读取数据时，必须先对其加表级共享锁 ，直到事务结束才释放；事务在更新数据时，必须先对其加表级排他锁 ，直到事务结束才释放。    可以看到，在只使用锁来实现隔离级别的控制的时候，需要频繁的加锁解锁，而且很容易发生读写的冲突（例如在RC级别下，事务A更新了数据行1，事务B则在事务A提交前读取数据行1都要等待事务A提交并释放锁）。</description>
    </item>
    
    <item>
      <title>使用Xshell登录AWS的EC2云服务器和开启EC2上允许root&#43;密码方式登录</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/%E4%BD%BF%E7%94%A8xshell%E7%99%BB%E5%BD%95aws%E7%9A%84ec2%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%BC%80%E5%90%AFec2%E4%B8%8A%E5%85%81%E8%AE%B8root&#43;%E5%AF%86%E7%A0%81%E6%96%B9%E5%BC%8F%E7%99%BB%E5%BD%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/%E4%BD%BF%E7%94%A8xshell%E7%99%BB%E5%BD%95aws%E7%9A%84ec2%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%BC%80%E5%90%AFec2%E4%B8%8A%E5%85%81%E8%AE%B8root&#43;%E5%AF%86%E7%A0%81%E6%96%B9%E5%BC%8F%E7%99%BB%E5%BD%95/</guid>
      <description>使用Xshell登录AWS的EC2云服务器和开启EC2上允许root+密码方式登录 https://www.dwhd.org/20150525_182436.html</description>
    </item>
    
    <item>
      <title>全文索引</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E6%8B%BE%E9%81%97/%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E6%8B%BE%E9%81%97/%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</guid>
      <description>全文索引 参考 什么是全文索引，为什么要使用全文索引MySQL 之全文索引浅谈mysql fulltext全文索引优缺点</description>
    </item>
    
    <item>
      <title>几种注册中心对比</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/%E5%87%A0%E7%A7%8D%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/%E5%87%A0%E7%A7%8D%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E5%AF%B9%E6%AF%94/</guid>
      <description>前言 服务注册中心本质上是为了解耦服务提供者和服务消费者。对于任何一个微服务，原则上都应存在或者支持多个提供者，这是由微服务的分布式属性决定的。更进一步，为了支持弹性扩缩容特性，一个微服务的提供者的数量和分布往往是动态变化的，也是无法预先确定的。因此，原本在单体应用阶段常用的静态LB机制就不再适用了，需要引入额外的组件来管理微服务提供者的注册与发现，而这个组件就是服务注册中心。
CAP理论 CAP理论是分布式架构中重要理论
 一致性(Consistency) (所有节点在同一时间具有相同的数据) 可用性(Availability) (保证每个请求不管成功或者失败都有响应) 分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作)  关于
P的理解，我觉得是在整个系统中某个部分，挂掉了，或者宕机了，并不影响整个系统的运作或者说使用，而可用性是，某个系统的某个节点挂了，但是并不影响系统的接受或者发出请求，CAP 不可能都取，只能取其中2个
原因是
如果C是第一需求的话，那么会影响A的性能，因为要数据同步，不然请求结果会有差异，但是数据同步会消耗时间，期间可用性就会降低。
如果A是第一需求，那么只要有一个服务在，就能正常接受请求，但是对与返回结果变不能保证，原因是，在分布式部署的时候，数据一致的过程不可能想切线路那么快。
再如果，同事满足一致性和可用性，那么分区容错就很难保证了，也就是单点，也是分布式的基本核心，好了，明白这些理论，就可以在相应的场景选取服务注册与发现了
服务注册中心解决方案 设计或者选型一个服务注册中心，首先要考虑的就是服务注册与发现机制。纵观当下各种主流的服务注册中心解决方案，大致可归为三类：
  应用内：直接集成到应用中，依赖于应用自身完成服务的注册与发现，最典型的是Netflix提供的Eureka
  应用外：把应用当成黑盒，通过应用外的某种机制将服务注册到注册中心，最小化对应用的侵入性，比如Airbnb的SmartStack，HashiCorp的Consul
  DNS：将服务注册为DNS的SRV记录，严格来说，是一种特殊的应用外注册方式，SkyDNS是其中的代表
   注1：对于第一类注册方式，除了Eureka这种一站式解决方案，还可以基于ZooKeeper或者Etcd自行实现一套服务注册机制，这在大公司比较常见，但对于小公司而言显然性价比太低。
注2：由于DNS固有的缓存缺陷，本文不对第三类注册方式作深入探讨。
 除了基本的服务注册与发现机制，从开发和运维角度，至少还要考虑如下五个方面：
  测活：服务注册之后，如何对服务进行测活以保证服务的可用性？
  负载均衡：当存在多个服务提供者时，如何均衡各个提供者的负载？
  集成：在服务提供端或者调用端，如何集成注册中心？
  运行时依赖：引入注册中心之后，对应用的运行时环境有何影响？
  可用性：如何保证注册中心本身的可用性，特别是消除单点故障？
  主流注册中心产品  软件产品特性并非一成不变，如果发现功能特性有变更，欢迎评论指正
     Nacos Eureka Consul CoreDNS Zookeeper     一致性协议 CP+AP AP CP — CP   健康检查 TCP/HTTP/MYSQL/Client Beat Client Beat TCP/HTTP/gRPC/Cmd — Keep Alive   负载均衡策略 权重/ metadata/Selector Ribbon Fabio RoundRobin —   雪崩保护 有 有 无 无 无   自动注销实例 支持 支持 支持 不支持 支持   访问协议 HTTP/DNS HTTP HTTP/DNS DNS TCP   监听支持 支持 支持 支持 不支持 支持   多数据中心 支持 支持 支持 不支持 不支持   跨注册中心同步 支持 不支持 支持 不支持 不支持   SpringCloud集成 支持 支持 支持 不支持 支持   Dubbo集成 支持 不支持 支持 不支持 支持   K8S集成 支持 不支持 支持 支持 不支持      Consul是支持自动注销服务实例， 请见文档： https://www.</description>
    </item>
    
    <item>
      <title>分布式ID生成方式</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E6%96%B9%E5%BC%8F/</guid>
      <description>分布式ID生成方式 一、为什么要用分布式ID？ 在说分布式ID的具体实现之前，我们来简单分析一下为什么用分布式ID？分布式ID应该满足哪些特征？
1、什么是分布式ID？ 拿MySQL数据库举个栗子：
在我们业务数据量不大的时候，单库单表完全可以支撑现有业务，数据再大一点搞个MySQL主从同步读写分离也能对付。
但随着数据日渐增长，主从同步也扛不住了，就需要对数据库进行分库分表，但分库分表后需要有一个唯一ID来标识一条数据，数据库的自增ID显然不能满足需求；特别一点的如订单、优惠券也都需要有唯一ID做标识。此时一个能够生成全局唯一ID的系统是非常必要的。那么这个全局唯一ID就叫分布式ID。
2、那么分布式ID需要满足那些条件？  全局唯一：必须保证ID是全局性唯一的，基本要求 高性能：高可用低延时，ID生成响应要块，否则反倒会成为业务瓶颈 高可用：100%的可用性是骗人的，但是也要无限接近于100%的可用性 好接入：要秉着拿来即用的设计原则，在系统设计和实现上要尽可能的简单 趋势递增：最好趋势递增，这个要求就得看具体业务场景了，一般不严格要求  二、 分布式ID都有哪些生成方式？ 今天主要分析一下以下9种，分布式ID生成器方式以及优缺点：
 UUID 数据库自增ID 数据库多主模式 号段模式 Redis 雪花算法（SnowFlake） 滴滴出品（TinyID） 百度 （Uidgenerator） 美团（Leaf）  那么它们都是如何实现？以及各自有什么优缺点？我们往下看
 以上图片源自网络，如有侵权联系删除
 1、基于UUID 在Java的世界里，想要得到一个具有唯一性的ID，首先被想到可能就是UUID，毕竟它有着全球唯一的特性。那么UUID可以做分布式ID吗？答案是可以的，但是并不推荐！
public static void main(String[] args) { String uuid = UUID.randomUUID().toString().replaceAll(&amp;#34;-&amp;#34;,&amp;#34;&amp;#34;); System.out.println(uuid); } UUID的生成简单到只有一行代码，输出结果 c2b8c2b9e46c47e3b30dca3b0d447718，但UUID却并不适用于实际的业务需求。像用作订单号UUID这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务主键ID，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作分布式ID。
优点：
 生成足够简单，本地生成无网络消耗，具有唯一性  缺点：
 无序的字符串，不具备趋势自增特性 没有具体的业务含义 长度过长16 字节128位，36位长度的字符串，存储以及查询对MySQL的性能消耗较大，MySQL官方明确建议主键要尽量越短越好，作为数据库主键 UUID 的无序性会导致数据位置频繁变动，严重影响性能。  2、基于数据库自增ID 基于数据库的auto_increment自增ID完全可以充当分布式ID，具体实现：需要一个单独的MySQL实例用来生成ID，建表结构如下：
CREATE DATABASE `SEQ_ID`; CREATE TABLE SEQID.</description>
    </item>
    
    <item>
      <title>分布式事务</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</guid>
      <description>分布式事务 分布式事务顾名思义就是要在分布式系统中实现事务，它其实是由多个本地事务组合而成。对于分布式事务而言几乎满足不了 ACID，其实对于单机事务而言大部分情况下也没有满足 ACID，不然怎么会有四种隔离级别呢？所以更别说分布在不同数据库或者不同应用上的分布式事务了。
2PC 2PC（Two-phase commit protocol），中文叫二阶段提交。 二阶段提交是一种强一致性设计，2PC 引入一个事务协调者的角色来协调管理各参与者（也可称之为各本地资源）的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。
**准备阶段**协调者会给各参与者发送准备命令，你可以把准备命令理解成除了提交事务之外啥事都做完了。
同步等待所有资源的响应之后就进入第二阶段即**提交阶段**（注意提交阶段不一定是提交事务，也可能是回滚事务）。
假如在第一阶段所有参与者都返回准备成功，那么协调者则向所有参与者发送提交事务命令，然后等待所有事务都提交成功之后，返回事务执行成功。
假如在第一阶段有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。
首先 2PC 是一个同步阻塞协议，像第一阶段协调者会等待所有参与者响应才会进行下一步操作，当然第一阶段的协调者有超时机制，假设因为网络原因没有收到某参与者的响应或某参与者挂了，那么超时后就会判断事务失败，向所有参与者发送回滚命令。
在第二阶段协调者的没法超时，只能不断重试，这里有两种情况：
第一种是第二阶段执行的是回滚事务操作，那么答案是不断重试，直到所有参与者都回滚了，不然那些在第一阶段准备成功的参与者会一直阻塞着。
第二种是第二阶段执行的是提交事务操作，那么答案也是不断重试，因为有可能一些参与者的事务已经提交成功了，这个时候只有一条路，就是头铁往前冲，不断的重试，直到提交成功，到最后真的不行只能人工介入处理。
至此我们已经详细的分析的 2PC 的各种细节，我们来总结一下：
 2PC 是一种尽量保证强一致性的分布式事务，因此它是同步阻塞的，而同步阻塞就导致长久的资源锁定问题，总体而言效率低，并且存在单点故障问题，在极端条件下存在数据不一致的风险。
当然具体的实现可以变形，而且 2PC 也有变种，例如 Tree 2PC、Dynamic 2PC。
还有一点不知道你们看出来没，2PC 适用于数据库层面的分布式事务场景，而我们业务需求有时候不仅仅关乎数据库，也有可能是上传一张图片或者发送一条短信。
而且像 Java 中的 JTA 只能解决一个应用下多数据库的分布式事务问题，跨服务了就不能用了。
简单说下 Java 中 JTA，它是基于XA规范实现的事务接口，这里的 XA 你可以简单理解为基于数据库的 XA 规范来实现的 2PC。
 3PC 3PC 的出现是为了解决 2PC 的一些问题，相比于 2PC 它在参与者中也引入了超时机制，并且新增了一个阶段使得参与者可以利用这一个阶段统一各自的状态。
让我们来详细看一下。
3PC 包含了三个阶段，分别是**准备阶段、预提交阶段、提交阶段**，对应的英文就是：CanCommit、PreCommit 、 DoCommit。
看起来是把 2PC 的提交阶段变成了预提交阶段和提交阶段，但是 3PC 的准备阶段协调者只是询问参与者的自身状况，比如你现在还好吗？负载重不重？这类的。
而预提交阶段就是和 2PC 的准备阶段一样，除了事务的提交该做的都做了。
提交阶段和 2PC 的一样，让我们来看一下图。</description>
    </item>
    
    <item>
      <title>剑指 Offer 09. 用两个栈实现队列</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-09.-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-09.-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/</guid>
      <description>class CQueue { Stack&amp;lt;Integer&amp;gt; stack1; Stack&amp;lt;Integer&amp;gt; stack2; public CQueue() { stack1 = new Stack(); stack2 = new Stack(); } public void appendTail(int value) { stack1.push(value); } public int deleteHead() { if(stack2.isEmpty()){ while(stack1.isEmpty() == false){ stack2.push(stack1.pop()); } } if(stack2.isEmpty()){ return -1; }else{ return stack2.pop(); } } } /** * Your CQueue object will be instantiated and called as such: * CQueue obj = new CQueue(); * obj.appendTail(value); * int param_2 = obj.</description>
    </item>
    
    <item>
      <title>剑指 Offer 18. 删除链表的节点</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-18.-%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E8%8A%82%E7%82%B9/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-18.-%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E8%8A%82%E7%82%B9/</guid>
      <description>class CQueue { Stack&amp;lt;Integer&amp;gt; stack1; Stack&amp;lt;Integer&amp;gt; stack2; public CQueue() { stack1 = new Stack(); stack2 = new Stack(); } public void appendTail(int value) { stack1.push(value); } public int deleteHead() { if(stack2.isEmpty()){ while(stack1.isEmpty() == false){ stack2.push(stack1.pop()); } } if(stack2.isEmpty()){ return -1; }else{ return stack2.pop(); } } } /** * Your CQueue object will be instantiated and called as such: * CQueue obj = new CQueue(); * obj.appendTail(value); * int param_2 = obj.</description>
    </item>
    
    <item>
      <title>剑指 Offer 27. 二叉树的镜像</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-27.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-27.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/</guid>
      <description>剑指 Offer 27. 二叉树的镜像/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public TreeNode mirrorTree(TreeNode root) { if(root == null){ return root; } TreeNode posL = root.left; TreeNode posR = root.right; root.left = posR; root.right = posL; mirrorTree(root.left); mirrorTree(root.right); return root; } } </description>
    </item>
    
    <item>
      <title>剑指 Offer 28. 对称的二叉树</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-28.-%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-28.-%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/</guid>
      <description>/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public boolean isSymmetric(TreeNode root) { if(root == null){return true;} return isSymmetric2(root.left, root.right); } private boolean isSymmetric2(TreeNode left, TreeNode right){ if (left == null &amp;amp;&amp;amp; right == null){return true;} if ((left == null &amp;amp;&amp;amp; right !</description>
    </item>
    
    <item>
      <title>剑指 Offer 30. 包含min函数的栈</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-30.-%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-30.-%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/</guid>
      <description>剑指 Offer 30. 包含min函数的栈class MinStack { Stack&amp;lt;Integer&amp;gt; stack1; Stack&amp;lt;Integer&amp;gt; stack2; /** initialize your data structure here. */ public MinStack() { stack1 = new Stack(); stack2 = new Stack(); } public void push(int x) { stack1.push(x); if(stack2.isEmpty()){ stack2.push(x); }else if(x &amp;gt; stack2.peek()){ stack2.push(stack2.peek()); }else{ stack2.push(x); } } public void pop() { stack1.pop(); stack2.pop(); } public int top() { return stack1.peek(); } public int min() { return stack2.peek(); } } /** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.</description>
    </item>
    
    <item>
      <title>剑指 Offer 32 - I. 从上到下打印二叉树</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-32-i.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-32-i.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/</guid>
      <description>/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public static int[] levelOrder(TreeNode root) { ArrayList&amp;lt;Integer&amp;gt; list = new ArrayList&amp;lt;Integer&amp;gt;(); if (root == null) { return new int[]{}; } Queue&amp;lt;TreeNode&amp;gt; queue=new LinkedList&amp;lt;TreeNode&amp;gt;(); queue.add(root); while(!queue.isEmpty()){ TreeNode node=queue.poll(); list.add(node.val); if(node.left!=null){ queue.add(node.left); } if(node.right!=null){ queue.add(node.right); } } int[] res = new int[list.</description>
    </item>
    
    <item>
      <title>剑指 Offer 32 - II. 从上到下打印二叉树 II</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-32-ii.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91-ii/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-32-ii.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91-ii/</guid>
      <description>剑指 Offer 32 - II. 从上到下打印二叉树 IIclass Solution { public List&amp;lt;List&amp;lt;Integer&amp;gt;&amp;gt; levelOrder(TreeNode root) { Queue&amp;lt;TreeNode&amp;gt; queue = new LinkedList&amp;lt;&amp;gt;(); List&amp;lt;List&amp;lt;Integer&amp;gt;&amp;gt; res = new ArrayList&amp;lt;&amp;gt;(); if(root != null) queue.add(root); while(!queue.isEmpty()) { List&amp;lt;Integer&amp;gt; tmp = new ArrayList&amp;lt;&amp;gt;(); for(int i = queue.size(); i &amp;gt; 0; i--) { TreeNode node = queue.poll(); tmp.add(node.val); if(node.left != null) queue.add(node.left); if(node.right != null) queue.add(node.right); } res.add(tmp); } return res; } } </description>
    </item>
    
    <item>
      <title>剑指 Offer 32 - III. 从上到下打印二叉树 III</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-32-iii.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91-iii/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/leetcode/%E5%89%91%E6%8C%87-offer-32-iii.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91-iii/</guid>
      <description>剑指 Offer 32 - III. 从上到下打印二叉树 IIIpackage com.xzj; import java.util.*; /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public static List&amp;lt;List&amp;lt;Integer&amp;gt;&amp;gt; levelOrder(TreeNode root) { List&amp;lt;List&amp;lt;Integer&amp;gt;&amp;gt; res = new ArrayList&amp;lt;&amp;gt;(); if (root == null){return res;} Stack&amp;lt;TreeNode&amp;gt; stack1 = new Stack(); Stack&amp;lt;TreeNode&amp;gt; stack2 = new Stack(); stack1.push(root); while (stack1.</description>
    </item>
    
    <item>
      <title>卷积神经网络–CNN</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn/</guid>
      <description>在 CNN 出现之前，图像对于人工智能来说是一个难题，有2个原因：
 图像需要处理的数据量太大，导致成本很高，效率很低 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高  下面就详细说明一下这2个问题：
0.0、需要处理的数据量太大 图像是由像素构成的，每个像素又是由颜色构成的。
现在随随便便一张图片都是 1000×1000 像素以上的， 每个像素都有RGB 3个参数来表示颜色信息。
假如我们处理一张 1000×1000 像素的图片，我们就需要处理3百万个参数！
1000×1000×3=3,000,000
这么大量的数据处理起来是非常消耗资源的，而且这只是一张不算太大的图片！
卷积神经网络 – CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。
更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。
0.1、保留图像特征 图片数字化的传统方式我们简化一下，就类似下图的过程：
假如有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，图像的内容（本质）并没有发生变化，只是位置发生了变化。
所以当我们移动图像中的物体，用传统的方式的得出来的参数会差异很大！这是不符合图像处理的要求的。
而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。
那么卷积神经网络是如何实现的呢？在我们了解 CNN 原理之前，先来看看人类的视觉原理是什么？
1、人类的视觉原理 深度学习的许多研究成果，离不开对大脑认知原理的研究，尤其是视觉原理的研究。
1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”，可视皮层是分级的。
人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例：
对于不同的物体，人类视觉也是通过这样逐层分级，来进行认知的：
我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。
那么我们可以很自然的想到：可以不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？
答案是肯定的，这也是许多深度学习算法（包括CNN）的灵感来源。
2、卷积神经网络-CNN 的基本原理 典型的 CNN 由3个部分构成：
 卷积层 池化层 全连接层  如果简单来描述的话：卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。
下面的原理解释为了通俗易懂，忽略了很多技术细节，如果大家对详细的原理感兴趣，可以看这个视频《卷积神经网络基础》。
2.1、卷积——提取特征 卷积层的运算过程如下图，用一个卷积核扫完整张图片：
这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。
在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：
总结：卷积层的通过卷积核的过滤提取出图片中局部的特征，跟上面提到的人类视觉的特征提取类似。
2.2、池化层（下采样）——数据降维，避免过拟合 池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：</description>
    </item>
    
    <item>
      <title>去除请求path前缀</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/%E5%8E%BB%E9%99%A4%E8%AF%B7%E6%B1%82path%E5%89%8D%E7%BC%80/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/%E5%8E%BB%E9%99%A4%E8%AF%B7%E6%B1%82path%E5%89%8D%E7%BC%80/</guid>
      <description>1. proxy_pass后面加根路径/ location ^~/user/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; proxy_pass http://user/; } ^~/user/表示匹配前缀是user的请求，proxy_pass的结尾有/， 则会把/user/*后面的路径直接拼接到后面，即移除user。
2. 使用rewrite location ^~/user/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; rewrite ^/user/(.*)$ /$1 break; proxy_pass http://user; } 注意到proxy_pass结尾没有/， rewrite重写了url。
参考链接： Nginx代理proxy pass配置去除前缀Nginx 转发域名地址报 400 Bad Request</description>
    </item>
    
    <item>
      <title>反向传播—BP</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADbp/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADbp/</guid>
      <description>说到神经网络，大家看到这个图应该不陌生：
这是典型的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,&amp;hellip;,xn},输出也是一堆数据{y1,y2,y3,&amp;hellip;,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，我会专门再写一篇Auto-Encoder的文章来说明，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。
本文直接举一个例子，带入数值演示反向传播法的过程，公式的推导等到下次写Auto-Encoder的时候再写，其实也很简单，感兴趣的同学可以自己推导下试试：）（注：本文假设你已经懂得基本的神经网络构成，如果完全不懂，可以参考Poll写的笔记：[Mechine Learning &amp;amp; Algorithm] 神经网络基础）
假设，你有这样一个网络层：
第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。
现在对他们赋上初值，如下图：
其中，输入数据 i1=0.05，i2=0.10;
　输出数据 o1=0.01,o2=0.99;
　初始权重 w1=0.15,w2=0.20,w3=0.25,w4=0.30;
　w5=0.40,w6=0.45,w7=0.50,w8=0.55
目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。
Step 1 前向传播
1.输入层&amp;mdash;-&amp;gt;隐含层：
计算神经元h1的输入加权和： $$ \begin{array}{l} n e t_{h 1}=w_{1} * i_{1}+w_{2} * i_{2}+b_{1} * 1 \
{ net }_{h 1}=0.15 * 0.05+0.2 * 0.1+0.35 * 1=0.3775 \end{array} $$ 神经元h1的输出o1:(此处用到激活函数为sigmoid函数)： $$ { out }_{h 1}=\frac{1}{1+e^{-n e t_{h 1}}}=\frac{1}{1+e^{-0.3775}}=0.593269992 $$ 同理，可计算出神经元h2的输出o2： $$ { out }_{h 2}=0.</description>
    </item>
    
    <item>
      <title>基于aspect实现注解</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/%E5%9F%BA%E4%BA%8Easpect%E5%AE%9E%E7%8E%B0%E6%B3%A8%E8%A7%A3/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/%E5%9F%BA%E4%BA%8Easpect%E5%AE%9E%E7%8E%B0%E6%B3%A8%E8%A7%A3/</guid>
      <description>在工作中，我们有时候需要将一些公共的功能封装，比如操作日志的存储，防重复提交等等。这些功能有些接口会用到，为了便于其他接口和方法的使用，做成自定义注解，侵入性更低一点。别人用的话直接注解就好。下面就来讲讲自定义注解这些事情。
1. @Target、@Retention、@Documented简介 java自定义注解的注解位于包：java.lang.annotation下。包含三个元注解@Target、@Retention、@Documented，即注解的注解。
@Target @Target:注解的作用目标。和枚举ElementType共同起作用
根据源码知道，可以配置多个作用目标。
@Documented @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.ANNOTATION_TYPE) public @interface Target { /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value(); } 复制代码 ElementType的类型如下：
* @author Joshua Bloch * @since 1.5 * @jls 9.6.4.1 @Target * @jls 4.1 The Kinds of Types and Values */ public enum ElementType { /** 类, 接口 (包括注解类型), 或 枚举 声明 */ TYPE, /** 字段声明（包括枚举常量） */ FIELD, /** 方法声明(Method declaration) */ METHOD, /** 正式的参数声明 */ PARAMETER, /** 构造函数声明 */ CONSTRUCTOR, /** 局部变量声明 */ LOCAL_VARIABLE, /** 注解类型声明 */ ANNOTATION_TYPE, /** 包声明 */ PACKAGE, /** * 类型参数声明 * * @since 1.</description>
    </item>
    
    <item>
      <title>基于Redis的分布式锁实现</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E5%9F%BA%E4%BA%8Eredis%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E5%9F%BA%E4%BA%8Eredis%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0/</guid>
      <description>前言 本篇文章主要介绍基于Redis的分布式锁实现到底是怎么一回事，其中参考了许多大佬写的文章，算是对分布式锁做一个总结
分布式锁概览 在多线程的环境下，为了保证一个代码块在同一时间只能由一个线程访问，Java中我们一般可以使用synchronized语法和ReetrantLock去保证，这实际上是本地锁的方式。但是现在公司都是流行分布式架构，在分布式环境下，如何保证不同节点的线程同步执行呢？
实际上，对于分布式场景，我们可以使用分布式锁，它是控制分布式系统之间互斥访问共享资源的一种方式。
比如说在一个分布式系统中，多台机器上部署了多个服务，当客户端一个用户发起一个数据插入请求时，如果没有分布式锁机制保证，那么那多台机器上的多个服务可能进行并发插入操作，导致数据重复插入，对于某些不允许有多余数据的业务来说，这就会造成问题。而分布式锁机制就是为了解决类似这类问题，保证多个服务之间互斥的访问共享资源，如果一个服务抢占了分布式锁，其他服务没获取到锁，就不进行后续操作。大致意思如下图所示（不一定准确）：
分布式锁的特点 分布式锁一般有如下的特点：
 互斥性： 同一时刻只能有一个线程持有锁 可重入性： 同一节点上的同一个线程如果获取了锁之后能够再次获取锁 锁超时：和J.U.C中的锁一样支持锁超时，防止死锁 高性能和高可用： 加锁和解锁需要高效，同时也需要保证高可用，防止分布式锁失效 具备阻塞和非阻塞性：能够及时从阻塞状态中被唤醒  分布式锁的实现方式 我们一般实现分布式锁有以下几种方式：
 基于数据库 基于Redis 基于zookeeper  本篇文章主要介绍基于Redis如何实现分布式锁
Redis的分布式锁实现 1. 利用setnx+expire命令 (错误的做法) Redis的SETNX命令，setnx key value，将key设置为value，当键不存在时，才能成功，若键存在，什么也不做，成功返回1，失败返回0 。 SETNX实际上就是SET IF NOT Exists的缩写
因为分布式锁还需要超时机制，所以我们利用expire命令来设置，所以利用setnx+expire命令的核心代码如下：
public boolean tryLock(String key,String requset,int timeout) {Long result = jedis.setnx(key, requset);// result = 1时，设置成功，否则设置失败if (result == 1L) {return jedis.expire(key, timeout) == 1L;} else {return false;}}复制代码实际上上面的步骤是有问题的，setnx和expire是分开的两步操作，不具有原子性，如果执行完第一条指令应用异常或者重启了，锁将无法过期。</description>
    </item>
    
    <item>
      <title>如何解决MySQL主从同步的延时问题</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E7%9A%84%E5%BB%B6%E6%97%B6%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E7%9A%84%E5%BB%B6%E6%97%B6%E9%97%AE%E9%A2%98/</guid>
      <description>你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？
如何实现 MySQL 的读写分离？ 其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。
MySQL 主从复制原理的是啥？ 主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。
这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。
而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。
所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。
这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。
所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。
MySQL 主从同步延时问题（精华） 以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。
是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。
我们通过 MySQL 命令：
show slave status查看 Seconds_Behind_Master ，可以看到从库复制主库的数据落后了几 ms。
一般来说，如果主从延迟较为严重，有以下解决方案：
 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。  参考 你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？</description>
    </item>
    
    <item>
      <title>如何设计一个高并发系统</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/</guid>
      <description>其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？
我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。
当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。
所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。
那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题：
可以分为以下 6 点：
 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch  系统拆分 将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。
缓存 缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。
MQ MQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。
分库分表 分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。
读写分离 读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。
ElasticSearch Elasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。
参考 如何设计一个高并发系统？</description>
    </item>
    
    <item>
      <title>实现线程超时的几种方式</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E5%AE%9E%E7%8E%B0%E7%BA%BF%E7%A8%8B%E8%B6%85%E6%97%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E5%AE%9E%E7%8E%B0%E7%BA%BF%E7%A8%8B%E8%B6%85%E6%97%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</guid>
      <description>1. 基于线程的join(long millis)方法 其实这个方法比较牵强，因为它主要作用是用来多个线程之间进行同步的。但因为它提供了这个带参数的方法（所以这也给了我们一个更广泛的思路，就是一般带有超时参数的方法我们都可以尝试着用它来实现超时结束任务），所以我们可以用它来实现。注意这里的参数的单位是固定的毫秒，不同于接下来的带单位的函数。具体用法请看示例：
public class JoinTest { public static void main(String[] args) { Task task1 = new Task(&amp;#34;one&amp;#34;, 4); Task task2 = new Task(&amp;#34;two&amp;#34;, 2); Thread t1 = new Thread(task1); Thread t2 = new Thread(task2); t1.start(); try { t1.join(2000); // 在主线程中等待t1执行2秒  } catch (InterruptedException e) { System.out.println(&amp;#34;t1 interrupted when waiting join&amp;#34;); e.printStackTrace(); } t1.interrupt(); // 这里很重要，一定要打断t1,因为它已经执行了2秒。  t2.start(); try { t2.join(1000); } catch (InterruptedException e) { System.out.println(&amp;#34;t2 interrupted when waiting join&amp;#34;); e.</description>
    </item>
    
    <item>
      <title>对象初始化顺序</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E5%AF%B9%E8%B1%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E5%AF%B9%E8%B1%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F/</guid>
      <description>Java中类及方法的加载顺序Java类加载顺序</description>
    </item>
    
    <item>
      <title>常备知识点</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/kafka/%E5%B8%B8%E5%A4%87%E7%9F%A5%E8%AF%86%E7%82%B9/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/kafka/%E5%B8%B8%E5%A4%87%E7%9F%A5%E8%AF%86%E7%82%B9/</guid>
      <description>1. 生产消费模型 参考：Kafka生产者消费者模型</description>
    </item>
    
    <item>
      <title>常用的模型评估指标</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</guid>
      <description>“没有测量，就没有科学。”这是科学家门捷列夫的名言。在计算机科学中，特别是在机器学习的领域，对模型的测量和评估同样至关重要。只有选择与问题相匹配的评估方法，我们才能够快速的发现在模型选择和训练过程中可能出现的问题，迭代地对模型进行优化。本文将总结机器学习最常见的模型评估指标，其中包括：
 precision recall F1-score PRC ROC和AUC IOU  从混淆矩阵谈起 看一看下面这个例子：假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然我们可以使用错误率来衡量有多少比例的瓜被判别错误。但如果我们关心的是“挑出的西瓜中有多少比例是好瓜”，或者“所有好瓜中有多少比例被挑出来了”，那么错误率显然就不够用了，这时我们需要引入新的评估指标，比如“查准率”和查全率更适合此类需求的性能度量。
在引入查全率和查准率之前我们必须先理解到什么是混淆矩阵（Confusion matrix）。这个名字起得是真的好，初学者很容易被这个矩阵搞得晕头转向。下图a就是有名的混淆矩阵，而下图b则是由混淆矩阵推出的一些有名的评估指标。
我们首先好好解读一下混淆矩阵里的一些名词和其意思。根据混淆矩阵我们可以得到TP,FN,FP,TN四个值，显然TP+FP+TN+FN=样本总数。这四个值中都带两个字母，单纯记忆这四种情况很难记得牢，我们可以这样理解：第一个字母表示本次预测的正确性，T就是正确，F就是错误；第二个字母则表示由分类器预测的类别，P代表预测为正例，N代表预测为反例。比如TP我们就可以理解为分类器预测为正例（P），而且这次预测是对的（T），FN可以理解为分类器的预测是反例（N），而且这次预测是错误的（F），正确结果是正例，即一个正样本被错误预测为负样本。我们使用以上的理解方式来记住TP、FP、TN、FN的意思应该就不再困难了。，下面对混淆矩阵的四个值进行总结性讲解：
 True Positive （真正，TP）被模型预测为正的正样本 True Negative（真负 , TN）被模型预测为负的负样本 False Positive （假正, FP）被模型预测为正的负样本 False Negative（假负 , FN）被模型预测为负的正样本  Precision、Recall、PRC、F1-score Precision指标在中文里可以称为查准率或者是精确率，Recall指标在中卫里常被称为查全率或者是召回率，查准率 P和查全率 R分别定义为：
查准率P和查全率R的具体含义如下：
  查准率(Precision）是指在所有系统判定的“真”的样本中，确实是真的的占比
  查全率（Recall）是指在所有确实为真的样本中，被判为的“真”的占比
  这里想强调一点，precision和accuracy（正确率）不一样的，accuracy针对所有样本，precision针对部分样本，即正确的预测/总的正反例：
查准率和查全率是一对矛盾的度量，一般而言，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。我们从直观理解确实如此：我们如果希望好瓜尽可能多地选出来，则可以通过增加选瓜的数量来实现，如果将所有瓜都选上了，那么所有好瓜也必然被选上，但是这样查准率就会越低；若希望选出的瓜中好瓜的比例尽可能高，则只选最有把握的瓜，但这样难免会漏掉不少好瓜，导致查全率较低。通常只有在一些简单任务中，才可能使查全率和查准率都很高。
再说PRC， 其全称就是Precision Recall Curve，它以查准率为Y轴，、查全率为X轴做的图。它是综合评价整体结果的评估指标。所以，哪种类型（正或者负）样本多，权重就大。也就是通常说的『对样本不均衡敏感』，『容易被多的样品带走』。
上图就是一幅P-R图，它能直观地显示出学习器在样本总体上的查全率和查准率，显然它是一条总体趋势是递减的曲线。在进行比较时，若一个学习器的PR曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者，比如上图中A优于C。但是B和A谁更好呢？因为AB两条曲线交叉了，所以很难比较，这时比较合理的判据就是比较PR曲线下的面积，该指标在一定程度上表征了学习器在查准率和查全率上取得相对“双高”的比例。因为这个值不容易估算，所以人们引入“平衡点”(BEP)来度量，他表示“查准率=查全率”时的取值，值越大表明分类器性能越好，以此比较我们一下子就能判断A较B好。
BEP还是有点简化了，更常用的是F1度量：
F1-score 就是一个综合考虑precision和recall的指标，比BEP更为常用。
ROC &amp;amp; AUC ROC全称是“受试者工作特征”（Receiver Operating Characteristic）曲线，ROC曲线以“真正例率”（TPR）为Y轴，以“假正例率”（FPR）为X轴，对角线对应于“随机猜测”模型，而（0,1）则对应“理想模型”。ROC形式如下图所示。
TPR和FPR的定义如下：
从形式上看TPR就是我们上面提到的查全率Recall，而FPR的含义就是：所有确实为“假”的样本中，被误判真的样本。
进行学习器比较时，与PR图相似，若一个学习器的ROC曲线被另一个学习器的曲线包住，那么我们可以断言后者性能优于前者；若两个学习器的ROC曲线发生交叉，则难以一般性断言两者孰优孰劣。此时若要进行比较，那么可以比较ROC曲线下的面积，即AUC，面积大的曲线对应的分类器性能更好。
AUC（Area Under Curve）的值为ROC曲线下面的面积，若分类器的性能极好，则AUC为1。但现实生活中尤其是工业界不会有如此完美的模型，一般AUC均在0.5到1之间，AUC越高，模型的区分能力越好，上图AUC为0.81。若AUC=0.5，即与上图中红线重合，表示模型的区分能力与随机猜测没有差别。若AUC真的小于0.5，请检查一下是不是好坏标签标反了，或者是模型真的很差。
怎么选择评估指标？ 这种问题的答案当然是具体问题具体分析啦，单纯地回答谁好谁坏是没有意义的，我们需要结合实际场景给出合适的回答。
考虑下面是两个场景，由此看出不同场景下我们关注的点是不一样的：
 对于地震的预测，我们希望的是Recall非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲Precision。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。所以我们可以设定在合理的precision下，最高的recall作为最优点，找到这个对应的threshold点。 嫌疑人定罪基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。即使有时候放过了一些罪犯（Recall低），但也是值得的。  ROC和PRC在模型性能评估上效果都差不多，但需要注意的是，在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。在数据极度不平衡的情况下，譬如说1万封邮件中只有1封垃圾邮件，那么如果我挑出10封，50封，100&amp;hellip;封垃圾邮件（假设我们每次挑出的N封邮件中都包含真正的那封垃圾邮件），Recall都是100%，但是FPR分别是9/9999, 49/9999, 99/9999（数据都比较好看：FPR越低越好），而Precision却只有1/10，1/50， 1/100 （数据很差：Precision越高越好）。所以在数据非常不均衡的情况下，看ROC的AUC可能是看不出太多好坏的，而PR curve就要敏感的多。</description>
    </item>
    
    <item>
      <title>常用缓存淘汰算法</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E5%B8%B8%E7%94%A8%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E5%B8%B8%E7%94%A8%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/</guid>
      <description>常见类型包括LFU、LRU、ARC、FIFO、MRU。
 最不经常使用算法（LFU, Least Frequently Used）：  这个缓存算法使用一个计数器来记录条目被访问的频率。通过使用LFU缓存算法，最低访问数的条目首先被移除。这个方法并不经常使用，因为它无法对一个拥有最初高访问率之后长时间没有被访问的条目缓存负责。
 最近最少使用算法（LRU, Least Recently Used）  LRU是首先淘汰最长时间未被使用的页面。这种算法把近期最久没有被访问过的页面作为被替换的页面。它把LFU算法中要记录数量上的&amp;quot;多&amp;quot;与&amp;quot;少&amp;quot;简化成判断&amp;quot;有&amp;quot;与&amp;quot;无&amp;quot;，因此，实现起来比较容易。
注意：LRU的淘汰规则是基于访问时间，而LFU是基于访问次数的</description>
    </item>
    
    <item>
      <title>常见知识点</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/tcp/%E5%B8%B8%E8%A7%81%E7%9F%A5%E8%AF%86%E7%82%B9/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/tcp/%E5%B8%B8%E8%A7%81%E7%9F%A5%E8%AF%86%E7%82%B9/</guid>
      <description>前言 网络层只把分组发送到目的主机，但是真正通信的并不是主机而是主机中的进程。传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。
UDP 和 TCP 的特点与区别 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。
传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。
2、UDP 、TCP 首部格式 UDP 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。
TCP 首部格式比 UDP 复杂。
序号：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。
确认号：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。
数据偏移：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。
控制位：八位从左到右分别是 CWR，ECE，URG，ACK，PSH，RST，SYN，FIN。
CWR：CWR 标志与后面的 ECE 标志都用于 IP 首部的 ECN 字段，ECE 标志为 1 时，则通知对方已将拥塞窗口缩小；
ECE：若其值为 1 则会通知对方，从对方到这边的网络有阻塞。在收到数据包的 IP 首部中 ECN 为 1 时将 TCP 首部中的 ECE 设为 1；</description>
    </item>
    
    <item>
      <title>底层数据结构</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</guid>
      <description>什么是跳跃表？
答：跳表这种高效的数据结构，值得每一个程序员掌握为什么使用ziplist？
答：相较于双链表，节省了两个指针的空间。（pre_entry_length前驱数据项的大小。因为不用描述前驱的数据类型，描述较为简单）。相较于数组，ziplist的每个entry所占的内存大小可以不同，便于节省空间。
为什么list  hash  zset 在数据两大的时候不再使用ziplist？
答：难以获得大的连续的内存空间。
参考链接 图解redis五种数据结构底层实现(动图哦)Redis 数据结构 ziplistRedis源码分析-压缩列表ziplist</description>
    </item>
    
    <item>
      <title>悲观锁和乐观锁</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E6%82%B2%E8%A7%82%E9%94%81%E5%92%8C%E4%B9%90%E8%A7%82%E9%94%81/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/mysql/%E6%82%B2%E8%A7%82%E9%94%81%E5%92%8C%E4%B9%90%E8%A7%82%E9%94%81/</guid>
      <description>悲观锁 共享锁、排他锁
 共享锁，也就是其他事务可以并发读（其他事务也需要加共享锁），但是不能写。
排它锁，其他事务不能并发写也不能并发读。
 乐观锁 乐观锁其实也不是实际的锁，甚至没有用到锁来实现并发控制，而是采取其他方式来判断能否修改数据。乐观锁一般是用户自己实现的一种锁机制，虽然没有用到实际的锁，但是能产生加锁的效果。
乐观锁基本都是基于 CAS（Compare and swap）算法来实现的。
主要有以下几种方式：
 版本号标记：在表中新增一个字段：version，用于保存版本号。获取数据的时候同时获取版本号，然后更新数据的时候用以下命令:update xxx set version=version+1,… where … version=&amp;quot;old version&amp;quot; and ....。这时候通过判断返回结果的影响行数是否为0来判断是否更新成功，更新失败则说明有其他请求已经更新了数据了。 时间戳标记：和版本号一样，只是通过时间戳来判断。一般来说很多数据表都会有更新时间这一个字段，通过这个字段来判断就不用再新增一个字段了。 待更新字段：如果没有时间戳字段，而且不想新增字段，那可以考虑用待更新字段来判断，因为更新数据一般都会发生变化，那更新前可以拿要更新的字段的旧值和数据库的现值进行比对，没有变化则更新。 所有字段标记：数据表所有字段都用来判断。这种相当于就、不仅仅对某几个字段做加锁了，而是对整个数据行加锁，只要本行数据发生变化，就不进行更新。  总结 悲观锁和乐观锁大部分场景下差异不大，一些独特场景下有一些差别，一般我们可以从如下几个方面来判断：
1.响应速度：如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁
2.冲突频率：如果冲突频率非常高，建议采用悲观锁，保证成功率，如果冲突频率大，乐观锁会需要多次重试才能成功，代价比较大
3.重试代价：如果重试代价大，建议采用悲观锁
参考链接： 乐观锁与悲观锁各自适用场景是什么？一文读懂数据库中的乐观锁和悲观锁和MVCC</description>
    </item>
    
    <item>
      <title>指令重排</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92/</guid>
      <description>我们知道java在运行的时候有两个地方可能用到重排序，一个是编译器编译的的时候，一个是处理器运行的时候。
那么我们就应该问问为啥要用指令重排序呢？
编译期重排序有啥好处？
CPU计算的时候要访问值，如果常常利用到寄存器中已有的值就不用去内存读取了，比如说
int a = 1; int b = 1; a = a + 1; b = b +1 ; 就可能没有
int a = 1; a = a + 1; int b = 1; b = b +1 ; 性能好，因为后者的a或b可能在寄存器中了。
处理器为啥要重排序？
一条汇编指令的执行步骤：
  取指 IF 译码和取寄存器操作 ID 执行或者有效地址计算 EX 存储器访问 MEM 写回 WB   在CPU工作中汇编指令分多步完成，每一部涉及到的硬件（寄存器）可能不同，于是有了流水线技术来执行指令。
没有流水线技术前，如果同时两个指令过来执行 一个需要5秒，那么两个就需要10秒；有了流水线技术之后，可能就只要6秒。多个指令同时执行时性能显著提升。
 流水线技术是一种将指令分解为多步，并让不同指令的各步操作重叠，从而实现几条指令并行处理。
指令1 IF ID EX MEN WB
指令2 IF ID EX MEN WB</description>
    </item>
    
    <item>
      <title>数据库和缓存双写一致性</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E7%BC%93%E5%AD%98%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E7%BC%93%E5%AD%98%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>数据库和缓存双写一致性 首先，缓存由于其高并发和高性能的特性，已经在项目中被广泛使用。在读取缓存方面，大家没啥疑问，都是按照下图的流程来进行业务操作。
、
双写可分为：更新缓存-&amp;gt;更新db、更新db-&amp;gt;更新缓存、删除缓存-&amp;gt;更新db、更新db-&amp;gt;删除缓存 四种策略。
并发事务时，前两种策略不仅存在数据不一致的，而且在写多读少的场景下会白白消耗缓存的性能（因为数据还没被读就又被更新了，尤其是在更新的缓存需要通过复杂计算才能得到时，这种消耗更加严重）。后两种策略情况相似，只是更新db-&amp;gt;删除缓存相较于删除缓存-&amp;gt;更新db发生数据不一致的概率更低（因为只有在写操作先于读操作完成才会不一致，而一般来说一个写操作是要比读操作慢的），为防止不一致发生，它们都采用异步延迟双删+删除重试机制的策略，下面详述常见删除重试机制。
重试机制：方案一 流程如下所示 （1）更新数据库数据； （2）缓存因为种种问题删除失败 （3）将需要删除的key发送至消息队列 （4）自己消费消息，获得需要删除的key （5）继续重试删除操作，直到成功 然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。
重试机制：方案二 流程如下图所示： （1）更新数据库数据 （2）数据库会将操作信息写入binlog日志当中 （3）订阅程序提取出所需要的数据以及key （4）另起一段非业务代码，获得该信息 （5）尝试删除缓存操作，发现删除失败 （6）将这些信息发送至消息队列 （7）重新从消息队列中获得该数据，重试操作。
上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。
参考链接： 分布式之数据库和缓存双写一致性方案解析</description>
    </item>
    
    <item>
      <title>服务之间的调用为啥不直接用 HTTP 而用 RPC？</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E6%9C%8D%E5%8A%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B0%83%E7%94%A8%E4%B8%BA%E5%95%A5%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8-http-%E8%80%8C%E7%94%A8-rpc/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E6%9C%8D%E5%8A%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B0%83%E7%94%A8%E4%B8%BA%E5%95%A5%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8-http-%E8%80%8C%E7%94%A8-rpc/</guid>
      <description> 使用自定义 TCP 协议进行传输就会避免上面这个问题，极大地减轻了传输数据的开销。 这也就是为什么通常会采用自定义 TCP 协议的 RPC 来进行进行服务调用的真正原因。 除此之外，成熟的 RPC 框架还提供好了“服务自动注册与发现”、&amp;ldquo;智能负载均衡&amp;rdquo;、“可视化的服务治理和运维”、“运行期流量调度”等等功能，这些也算是选择 RPC 进行服务注册和发现的一方面原因吧！  </description>
    </item>
    
    <item>
      <title>查找重复元素</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E7%89%9B%E5%AE%A2/%E6%9F%A5%E6%89%BE%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E7%89%9B%E5%AE%A2/%E6%9F%A5%E6%89%BE%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0/</guid>
      <description>找出数组 arr 中重复出现过的元素（不用考虑返回顺序）
示例1
输入 [1, 2, 4, 4, 3, 3, 1, 5, 3] 输出 [1, 3, 4] 将传入的数组arr中的每一个元素value当作另外一个新数组b的key，然后遍历arr去访问b[value]，若b[value]不存在，则将b[value]设置为1，若b[value]存在，则将其加1。可以想象，若arr中数组没有重复的元素，则b数组中所有元素均为1；若arr数组中存在重复的元素，则在第二次访问该b[value]时，b[value]会加1，其值就为2了。最后遍历b数组，将其值大于1的元素的key存入另一个数组a中，就得到了arr中重复的元素。
function duplicates(arr) { //声明两个数组，a数组用来存放结果，b数组用来存放arr中每个元素的个数  var a = [],b = []; //遍历arr，如果以arr中元素为下标的的b元素已存在，则该b元素加1，否则设置为1  for(var i = 0; i &amp;lt; arr.length; i++){ if(!b[arr[i]]){ b[arr[i]] = 1; continue; } b[arr[i]]++; } //遍历b数组，将其中元素值大于1的元素下标存入a数组中  for(var i = 0; i &amp;lt; b.length; i++){ if(b[i] &amp;gt; 1){ a.push(i); } } return a; } 查找重复元素</description>
    </item>
    
    <item>
      <title>死记硬背</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E6%AD%BB%E8%AE%B0%E7%A1%AC%E8%83%8C/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8B%BE%E9%81%97/%E6%AD%BB%E8%AE%B0%E7%A1%AC%E8%83%8C/</guid>
      <description>1. redis可以容纳多少个键值对？单个键值的大小限制是？ 最多容纳 2^32 个key
redis的key和string类型value限制均为512MB。
string类型：一个String类型的value最大可以存储512M
list类型：list的元素个数最多为2^32-1个，也就是4294967295个。
set类型：元素个数最多为2^32-1个，也就是4294967295个。
hash类型：键值对个数最多为2^32-1个，也就是4294967295个。
zset类型：跟set类型相似。
2. mysql varchar最大长度？ 65532 byte = 65535 byte (max row size) - 1 byte (not null flag) - 2 byte (记录实际长度)
4.0版本以下，varchar(20)，指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） 5.0版本以上，varchar(20)，指的是20字符，无论存放的是数字、字母还是UTF8汉字（每个汉字3字节），都可以存放20个，最大大小是65532字节
MySQL中varchar最大长度是多少？3. mysql sql命令执行顺序 定位表` -&amp;gt; `笛卡尔积` -&amp;gt; `过滤` -&amp;gt; `选择 `-&amp;gt; `展示1.FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1
2.ON: 对虚表VT1进行ON筛选，只有那些符合的行才会被记录在虚表VT2中。
3.JOIN： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, rug from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止
4.WHERE： 对虚拟表VT3进行WHERE条件过滤。只有符合的记录才会被插入到虚拟表VT4中。
5.GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5.
6.CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6.
7.HAVING： 对虚拟表VT6应用having过滤，只有符合的记录才会被 插入到虚拟表VT7中。</description>
    </item>
    
    <item>
      <title>泛型</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E6%B3%9B%E5%9E%8B/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E6%B3%9B%E5%9E%8B/</guid>
      <description>泛型的好处  提供了一种类型安全检测机制 提升程序可读性  通配符 通配符的出现是为了指定泛型中的类型范围。
通配符有 3 种形式。
 &amp;lt;?&amp;gt;被称作无限定的通配符。 &amp;lt;? extends T&amp;gt;被称作有上限的通配符。 &amp;lt;? super T&amp;gt;被称作有下限的通配符。   ? 其实代表的是未知类型，所以涉及到 ? 时的操作，一定与具体类型无关。
有人说，&amp;lt;?&amp;gt;提供了只读的功能，也就是它删减了增加具体类型元素的能力，只保留与具体类型无关的功能。它不管装载在这个容器内的元素是什么类型，它只关心元素的数量、容器是否为空？我想这种需求还是很常见的吧。
有同学可能会想，&amp;lt;?&amp;gt;既然作用这么渺小，那么为什么还要引用它呢？ 
个人认为，提高了代码的可读性，程序员看到这段代码时，就能够迅速对此建立极简洁的印象，能够快速推断源码作者的意图。
类型擦除 在泛型类被类型擦除的时候，之前泛型类中的类型参数部分如果没有指定上限，如 &amp;lt;T&amp;gt;则会被转译成普通的 Object 类型，如果指定了上限如 &amp;lt;T extends String&amp;gt;则类型参数就被替换成类型上限。
类型擦除带来的局限性 正常情况下，因为泛型的限制，编译器不让最后一行代码编译通过，因为类似不匹配，但是，基于对类型擦除的了解，利用反射，我们可以绕过这个限制。
那么，利用反射，我们绕过编译器去调用 add 方法。
public class ToolTest { public static void main(String[] args) { List&amp;lt;Integer&amp;gt; ls = new ArrayList&amp;lt;&amp;gt;(); ls.add(23); //	ls.add(&amp;#34;text&amp;#34;);  try { Method method = ls.getClass().getDeclaredMethod(&amp;#34;add&amp;#34;,Object.class); method.invoke(ls,&amp;#34;test&amp;#34;); method.invoke(ls,42.9f); } catch (NoSuchMethodException e) { // TODO Auto-generated catch block  e.</description>
    </item>
    
    <item>
      <title>浮点数运算</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E6%B5%AE%E7%82%B9%E6%95%B0%E8%BF%90%E7%AE%97/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E6%B5%AE%E7%82%B9%E6%95%B0%E8%BF%90%E7%AE%97/</guid>
      <description>结论  Java中单精度和双精度采用IEEE 754表示，能有效运算的范围大致是小数点后7位和15位 如果Java中默认的float和double不能满足你的精度要求，可以用BigDecimal，理论上它的精度只受限制与机器内存 如果BigDecimal仍无法满足需求，例如是无限循环小数的运算，可考虑设计分数系统保证计算的精度  1. IEEE 754 精度上限 编程语言中的浮点数一般都是 32 位的单精度浮点数 float 和 64 位的双精度浮点数 double，部分语言会使用 float32 或者 float64 区分这两种不同精度的浮点数。想要使用有限的位数表示全部的实数是不可能的，不用说无限长度的小数和无理数，因为长度的限制，有限小数在浮点数中都无法精确的表示。
 单精度浮点数 float 总共包含 32 位，其中 1 位表示符号、8 位表示指数，最后 23 位表示小数； 双精度浮点数 double 总共包含 64 位，其中 1 位表示符号，11 位表示指数，最后 52 位表示小数；  我们以单精度浮点数 0.15625 为例:
通过上图中的公式 (sign * 2^{exp}* (1+fraction))可以将浮点数的二进制表示转换成十进制的小数。0.15625 虽然还可以用单精度的浮点数精确表示，但是 0.1 和 0.2 只能使用浮点数表示近似的值：
因为 0.2 和 0.1 只是指数稍有不同，所以上图中只展示了 0.1 对应的单精度浮点数，从上图的结果我们可以看出，0.1 和 0.2 在浮点数中只能用近似值来代替，精度十分有限，因为单精度浮点数的小数位为 23，双精度的小数位为 52，同时都隐式地包含首位的 1，所以它们的精度在十进制中分别是(log_{10}(2^{24})\approx 7.</description>
    </item>
    
    <item>
      <title>深度图解Redis Cluster原理</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E6%B7%B1%E5%BA%A6%E5%9B%BE%E8%A7%A3redis-cluster%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E6%B7%B1%E5%BA%A6%E5%9B%BE%E8%A7%A3redis-cluster%E5%8E%9F%E7%90%86/</guid>
      <description>前言 上文我们聊了基于Sentinel的Redis高可用架构，了解了Redis基于读写分离的主从架构，同时也知道当Redis的master发生故障之后，Sentinel集群是如何执行failover的，以及其执行failover的原理是什么。
这里大概再提一下，Sentinel集群会对Redis的主从架构中的Redis实例进行监控，一旦发现了master节点宕机了，就会选举出一个Sentinel节点来执行故障转移，从原来的slave节点中选举出一个，将其提升为master节点，然后让其他的节点去复制新选举出来的master节点。
你可能会觉得这样没有问题啊，甚至能够满足我们生产环境的使用需求了，那我们为什么还需要Redis Cluster呢？
1. 为什么需要Redis Cluster 的确，在数据上，有replication副本做保证；可用性上，master宕机会自动的执行failover。
 那问题在哪儿呢？
 首先Redis Sentinel说白了也是基于主从复制，在主从复制中slave的数据是完全来自于master。
假设master节点的内存只有4G，那slave节点所能存储的数据上限也只能是4G。而且在之前的跟随杠精的视角一起来了解Redis的主从复制文章中也说过，主从复制架构中是读写分离的，我们可以通过增加slave节点来扩展主从的读并发能力，但是写能力和存储能力是无法进行扩展的，就只能是master节点能够承载的上限。
所以，当你只需要存储4G的数据时候的，基于主从复制和基于Sentinel的高可用架构是完全够用的。
但是如果当你面临的是海量的数据的时候呢？16G、64G、256G甚至1T呢？现在互联网的业务里面，如果你的体量足够大，我觉得是肯定会面临缓存海量缓存数据的场景的。
这就是为什么我们需要引入Redis Cluster。
2. Redis Cluster是什么 知道了为什么需要Redis Cluster之后，我们就可以来对其一探究竟了。
 那什么是Redis Cluster呢？
 很简单，你就可以理解为n个主从架构组合在一起对外服务。Redis Cluster要求至少需要3个master才能组成一个集群，同时每个master至少需要有一个slave节点。
这样一来，如果一个主从能够存储32G的数据，如果这个集群包含了两个主从，则整个集群就能够存储64G的数据。
我们知道，主从架构中，可以通过增加slave节点的方式来扩展读请求的并发量，那Redis Cluster中是如何做的呢？虽然每个master下都挂载了一个slave节点，但是在Redis Cluster中的读、写请求其实都是在master上完成的。
slave节点只是充当了一个数据备份的角色，当master发生了宕机，就会将对应的slave节点提拔为master，来重新对外提供服务。
3. 节点负载均衡 知道了什么是Redis Cluster，我们就可以继续下面的讨论了。
不知道你思考过一个问题没，这么多的master节点。我存储的时候，到底该选择哪个节点呢？一般这种负载均衡算法，会选择哈希算法。哈希算法是怎么做的呢？
首先就是对key计算出一个hash值，然后用哈希值对master数量进行取模。由此就可以将key负载均衡到每一个Redis节点上去。这就是简单的哈希算法的实现。
那Redis Cluster是采取的上面的哈希算法吗？答案是没有。
Redis Cluster其实采取的是类似于一致性哈希的算法来实现节点选择的。那为什么不用哈希算法来进行实例选择呢？以及为什么说是类似的呢？我们继续讨论。
因为如果此时某一台master发生了宕机，那么此时会导致Redis中所有的缓存失效。为什么是所有的？假设之前有3个master，那么之前的算法应该是 hash % 3，但是如果其中一台master宕机了，则算法就会变成 hash % 2，会影响到之前存储的所有的key。而这对缓存后面保护的DB来说，是致命的打击。
4. 什么是一致性哈希 知道了通过传统哈希算法来实现对节点的负载均衡的弊端，我们就需要进一步了解什么是一致性哈希。
我们上面提过哈希算法是对master实例数量来取模，而一致性哈希则是对2^32取模，也就是值的范围在[0, 2^32 -1]。一致性哈希将其范围抽象成了一个圆环，使用CRC16算法计算出来的哈希值会落到圆环上的某个地方。
然后我们的Redis实例也分布在圆环上，我们在圆环上按照顺时针的顺序找到第一个Redis实例，这样就完成了对key的节点分配。我们举个例子。
假设我们有A、B、C三个Redis实例按照如图所示的位置分布在圆环上，此时计算出来的hash值，取模之后位置落在了位置D，那么我们按照顺时针的顺序，就能够找到我们这个key应该分配的Redis实例B。同理如果我们计算出来位置在E，那么对应选择的Redis的实例就是A。
即使这个时候Redis实例B挂了，也不会影响到实例A和C的缓存。
例如此时节点B挂了，那之前计算出来在位置D的key，此时会按照顺时针的顺序，找到节点C。相当于自动的把原来节点B的流量给转移到了节点C上去。而其他原本就在节点A和节点C的数据则完全不受影响。
这就是一致性哈希，能够在我们后续需要新增节点或者删除节点的时候，不影响其他节点的正常运行。
5. 虚拟节点机制 但是一致性哈希也存在自身的小问题，例如当我们的Redis节点分布如下时，就有问题了。
此时数据落在节点A上的概率明显是大于其他两个节点的，其次落在节点C上的概率最小。这样一来会导致整个集群的数据存储不平衡，AB节点压力较大，而C节点资源利用不充分。为了解决这个问题，一致性哈希算法引入了虚拟节点机制。
在圆环中，增加了对应节点的虚拟节点，然后完成了虚拟节点到真实节点的映射。假设现在计算得出了位置D，那么按照顺时针的顺序，我们找到的第一个节点就是C #1，最终数据实际还是会落在节点C上。</description>
    </item>
    
    <item>
      <title>生产问题定位</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/</guid>
      <description>排除应用之外的影响因素： top(cpu)、free(内存)、df(磁盘)、dstat(网络流量)、pstack、vmstat、st1race(底层系统调用)
  top 定位CPU 最高的进程
  top -Hp pid 定位使用 CPU 最高的线程（或者 ps -mp pid -o THREAD,tid,time）
  printf &#39;0x%x&#39; tid 线程 id 转化 16 进制
  jstack pid | grep tid 找到线程堆栈
5.1 gc线程（如下是查看gc情况的几种方式）
    查看gc 日志
  jstat -gcutil 进程号 统计间隔毫秒 统计次数（缺省代表一致统计
  如果所在公司有对应用进行监控的组件当然更方便（比如Prometheus + Grafana）
5.2 业务线程
  关注线程堆栈的lock字段
  jstack -l pid | grep BLOCKED 查看阻塞态线程堆栈</description>
    </item>
    
    <item>
      <title>系统性能优化</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</guid>
      <description>系统性能优化之并发编程提升系统的QPS和吞吐量</description>
    </item>
    
    <item>
      <title>线程安全的单例的几种实现方法</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%8D%95%E4%BE%8B%E7%9A%84%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%8D%95%E4%BE%8B%E7%9A%84%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95/</guid>
      <description>1. 使用synchronized 饱汉：双重检查锁定、饿汉、静态内部类、枚举 都属于利用synchronized同步原理实现
1.1 饱汉：双重检查锁定（double-checked locking） public class SingleTon { // 静态实例变量加上volatile  private static volatile SingleTon instance; // 私有化构造函数  private SingleTon() {} // 双重检查锁  public static SingleTon getInstance() { if (instance == null) { synchronized(SingleTon.class){ if(instance == null){ instance = new SingleTon(); } } } return instance; } 1.2 饿汉 public class Singleton { private static Singleton instance = new Singleton(); private Singleton() { } public static Singleton getInstance() { return instance; } } 1.</description>
    </item>
    
    <item>
      <title>线程状态</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81/</guid>
      <description>Linux线程的状态与调度 Java线程的6种状态及切换 参考 Linux线程的状态与调度</description>
    </item>
    
    <item>
      <title>缓存雪崩和缓存穿透</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/redis/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F/</guid>
      <description>1. 缓存雪崩 1.1 什么是缓存雪崩？ 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。
1.2 有哪些解决办法？  事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存  2. 缓存穿透 2.1 什么是缓存穿透？ 缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。
2.1 有哪些解决办法？ 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。
 缓存无效 key（解决请求的 key 变化不频繁的情况） : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建的不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器（解决请求的 key 变化频繁且key非法）：布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。总结一下就是下面这张图(这张图片不是我画的，为了省事直接在网上找的)：  但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是： 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。
 缓存预热（解决请求的 key 变化频繁且key合法）：提前将需要做缓存的数据放入redis，即缓存预热。  3.</description>
    </item>
    
    <item>
      <title>脚手架</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E8%84%9A%E6%89%8B%E6%9E%B6/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E8%84%9A%E6%89%8B%E6%9E%B6/</guid>
      <description>1. 什么是java脚手架 其实就是java工程模板，你可以把一些通用的组件抽象成一个模板，下次开发的时候基于这个模板开发，避免重复造轮子。像apache默认就提供了很多模板（archetype）
2. 创建archetype 假如你已经有了一个maven项目，想给该项目创建一个archetype模板。你需要cd 到项目根目录下执行(pom.xml同级目录)。
mvn archetype:create-from-project 执行完后，生成的target类似这样：
3. 生成archetype模板 先cd target/generated-sources/archetype/ 然后执行：
mvn install 执行成功后，执行crawl命令：
mvn archetype:crawl 在本地仓库的根目录生成archetype-catalog.xml骨架配置文件:
来看一看它里面的内容:
4. 使用archetype模板 执行mvn archetype:generate -DarchetypeCatalog=local从本地archetype模板中创建项目。
mvn archetype:generate -DarchetypeCatalog=local 然后会让你选择模板序号和groupId artifactId version和package信息：
至此，项目创建成功!
当然，也可以使用IDEA来帮我们用图形界面使用archetype模板创建项目：
这里的信息根据archetype-catalog.xml中的填写，如果是本地导入Repository可以不填或者填&amp;rsquo;local&#39;。既然提到本地，那么自然可以想到，可以将脚手架发布到nexus私服。发布到私服可以参看这里：https://www.cnblogs.com/woshimrf/p/maven-artifact-demo.html</description>
    </item>
    
    <item>
      <title>调整Spring HandlerMapping的顺序</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/%E8%B0%83%E6%95%B4spring-handlermapping%E7%9A%84%E9%A1%BA%E5%BA%8F/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/spring/%E8%B0%83%E6%95%B4spring-handlermapping%E7%9A%84%E9%A1%BA%E5%BA%8F/</guid>
      <description>当请求形如：/opendoc/jquery-1.10.2.min.js 的静态资源时，如果恰好存在匹配这个请求的Controller时，默认情况下，这个静态资源请求会被 RequestMappingHandlerMapping 分配给这个Controller处理，从而可能找不到静态资源，例如存在下面这样的Controller：
@RequestMapping(value = &amp;#34;/{name}/{version}&amp;#34;, method = {RequestMethod.POST, RequestMethod.GET}) public void rest3(@PathVariable(&amp;#34;name&amp;#34;) String name, @PathVariable(&amp;#34;version&amp;#34;) String version, HttpServletRequest request, HttpServletResponse response) { this.doRest(name, version, request, response); } 为了能正确匹配的静态资源，我们可以调整Spring HandlerMapping的顺序，让SimpleUrlHandlerMapping先于RequestMappingHandlerMapping去匹配请求，SimpleUrlHandlerMapping会返回一个用于加载静态资源的ResourceHttpRequestHandler
默认情况下RequestMappingHandlerMapping 先于SimpleUrlHandlerMapping匹配请求，DispatchServlet中初始化HandlerMapping的顺序源码如下所示：
/** * Initialize the HandlerMappings used by this class. * &amp;lt;p&amp;gt;If no HandlerMapping beans are defined in the BeanFactory for this namespace, * we default to BeanNameUrlHandlerMapping. */ private void initHandlerMappings(ApplicationContext context) { this.handlerMappings = null; if (this.</description>
    </item>
    
    <item>
      <title>部署vue项目</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/%E9%83%A8%E7%BD%B2vue%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/nginx/%E9%83%A8%E7%BD%B2vue%E9%A1%B9%E7%9B%AE/</guid>
      <description>时常我们想通过path来区分项目，例如通过 http://xxxx/admin 访问我们的后台，如果vue是的mode是history，请注意如下配置：
 修改vue-config.js文件配置  module.exports = {publicPath: &amp;#34;&amp;#34;}; 修改路由route/index  const router = new Router({ base: &amp;#39;/admin/&amp;#39;, //路由模式为history模式时，base必须要加上;路由模式为hash模式时，base可加可不加  mode: &amp;#39;history&amp;#39;, routes: [] } 此时，请求是形如 http://xxxx/admin/xxx.css，从而避免出现形如http://xxxx/xxx.css找不到资源的情况。</description>
    </item>
    
    <item>
      <title>锁</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E9%94%81/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E9%94%81/</guid>
      <description>阿里面试，问了我乐观锁、悲观锁、AQS、sync和Lock，这个回答让我拿了offer阿里面试官：说一下公平锁和非公平锁的区别？</description>
    </item>
    
    <item>
      <title>阿里云下配置MySQL远程连接</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%8B%E9%85%8D%E7%BD%AEmysql%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E5%85%B6%E4%BB%96/%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%8B%E9%85%8D%E7%BD%AEmysql%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/</guid>
      <description>1）use mysql 2）将host设置为%表示任何ip都能连接mysql update user set host=&amp;#39;%&amp;#39; where user=&amp;#39;root&amp;#39; and host=&amp;#39;localhost&amp;#39;; 3) 执行完以上语句,接着执行以下语句 ,刷新权限表,使配置生效 flush privileges; 参考链接： 阿里云下配置MySQL远程连接的步骤详解</description>
    </item>
    
    <item>
      <title>零拷贝</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E9%9B%B6%E6%8B%B7%E8%B4%9D/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/java/%E9%9B%B6%E6%8B%B7%E8%B4%9D/</guid>
      <description>1. 简介 零拷贝的“零”是指用户态和内核态间copy数据的次数为零。
传统的数据copy（文件到文件、client到server等）涉及到四次用户态内核态切换、四次copy。四次copy中，两次在用户态和内核态间copy需要CPU参与、两次在内核态与IO设备间copy为DMA方式不需要CPU参与。零拷贝避免了用户态和内核态间的copy、减少了Java零拷贝机制解析核态间的切换。
 DMA(Direct Memory Access，直接内存存取) 是所有现代电脑的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于CPU 的大量中断负载。 DMA控制器，接管了数据读写请求，减少CPU的负担。这样一来，CPU能高效工作了。 现代硬盘基本都支持DMA。
 使用Zero Copy前后对比：
使用前：
使用后：
Linux支持的(常见)零拷贝 mmap内存映射` `sendfile` `Sendfile With DMA Scatter/Gather Copy` `splice无论是传统IO方式，还是引入零拷贝之后，2次DMA copy 是都少不了的。因为两次DMA都是依赖硬件完成的。
实际上，零拷贝时有广义和狭义之分的。 广义零拷贝： 能减少拷贝次数，减少不必要的数据拷贝，就算作“零拷贝”。 这是目前，对零拷贝最为广泛的定义，我们需要知道的是，这是广义上的零拷贝，并不是操作系统 意义上的零拷贝。
Java零拷贝机制解析 Linux提供的领拷贝技术 Java并不是全支持，支持2种(内存映射mmap、sendfile)；
NIO提供的内存映射：MappedByteBuffer
NIO提供的sendfile：FileChannel.transferTo() FileChannel.transferFrom()
参考链接： Java中的零拷贝Java零拷贝</description>
    </item>
    
    <item>
      <title>高性能无锁队列Disruptor</title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E6%8B%BE%E9%81%97/%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97disruptor/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3/%E6%8B%BE%E9%81%97/%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97disruptor/</guid>
      <description>高性能无锁队列Disruptor 1. JDK中的队列 在jdk中的队列都实现了java.util.Queue接口，在队列中又分为两类，一类是线程不安全的，ArrayDeque，LinkedList等等，还有一类都在java.util.concurrent包下属于线程安全，而在我们真实的环境中，我们的机器都是属于多线程，当多线程对同一个队列进行排队操作的时候，如果使用线程不安全会出现，覆盖数据，数据丢失等无法预测的事情，所以我们这个时候只能选择线程安全的队列。在jdk中提供的线程安全的队列下面简单列举部分队列:
   队列名字 是否加锁 数据结构 关键技术点 是否有锁 是否有界     ArrayBlockingQueue 是 数组array ReentrantLock 有锁 有界   LinkedBlockingQueue 是 链表 ReentrantLock 有锁 有界   LinkedTransferQueue 否 链表 CAS 无锁 无界   ConcurrentLinkedQueue 否 链表 CAS 无锁 无界    我们可以看见，我们无锁的队列是无界的，有锁的队列是有界的，这里就会涉及到一个问题，我们在真正的线上环境中，无界的队列，对我们系统的影响比较大，有可能会导致我们内存直接溢出，所以我们首先得排除无界队列，当然并不是无界队列就没用了，只是在某些场景下得排除。其次还剩下ArrayBlockingQueue，LinkedBlockingQueue两个队列，他们两个都是用ReentrantLock控制的线程安全，他们两个的区别一个是数组，一个是链表，在队列中，一般获取这个队列元素之后紧接着会获取下一个元素，或者一次获取多个队列元素都有可能，而数组在内存中地址是连续的，在操作系统中会有缓存的优化(下面也会介绍缓存行)，所以访问的速度会略胜一筹，我们也会尽量去选择ArrayBlockingQueue。而事实证明在很多第三方的框架中，比如早期的log4j异步，都是选择的ArrayBlockingQueue。
当然ArrayBlockingQueue，也有自己的弊端，就是性能比较低，为什么jdk会增加一些无锁的队列，其实就是为了增加性能，很苦恼，又需要无锁，又需要有界，这个时候恐怕会忍不住说一句你咋不上天呢？但是还真有人上天了。
2.Disruptor Disruptor就是上面说的那个天，Disruptor是英国外汇交易公司LMAX开发的一个高性能队列，并且是一个开源的并发框架，并获得2011Duke’s程序框架创新奖。能够在无锁的情况下实现网络的Queue并发操作，基于Disruptor开发的系统单线程能支撑每秒600万订单。目前，包括Apache Storm、Camel、Log4j2等等知名的框架都在内部集成了Disruptor用来替代jdk的队列，以此来获得高性能。
3.1为什么这么牛逼？ 上面已经把Disruptor吹出了花了，你肯定会产生疑问，他真的能有这么牛逼吗，我的回答是当然的，在Disruptor中有三大杀器:
  CAS
  消除伪共享
  RingBuffer
有了这三大杀器，Disruptor才变得如此牛逼。
  3.1.1 锁和CAS CAS实现无锁队列可以参考👉无锁队列的实现我们ArrayBlockingQueue为什么会被抛弃的一点，就是因为用了重量级lock锁，在我们加锁过程中我们会把锁挂起，解锁后，又会把线程恢复,这一过程会有一定的开销，并且我们一旦没有获取锁，这个线程就只能一直等待，这个线程什么事也不能做。</description>
    </item>
    
    <item>
      <title>关于</title>
      <link>https://xuzhijvn.github.io/zh-cn/about/</link>
      <pubDate>Fri, 19 Feb 2016 09:30:15 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/about/</guid>
      <description>&lt;p&gt;我是 Razon Yang，一名全栈工程师。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/cicd/docker-compose%E7%9A%84%E6%96%B9%E5%BC%8F%E5%90%AF%E5%8A%A8jenkins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/cicd/docker-compose%E7%9A%84%E6%96%B9%E5%BC%8F%E5%90%AF%E5%8A%A8jenkins/</guid>
      <description>docker compose的方式启动jenkins
jenkins官网提供了多种安装方式，唯独没有提供docker compose的教程！咱也不知道为啥，所以自己动手丰衣足食。
1. 编写docker-compose.yml version: &amp;#39;3&amp;#39; services: jenkins: image: jenkinsci/blueocean container_name: jenkins user: root ports: - &amp;#39;8080:8080&amp;#39; - &amp;#39;50000:50000&amp;#39; volumes: - &amp;#39;/docker/volumes/jenkins:/var/jenkins_home&amp;#39; - &amp;#39;/var/run/docker.sock:/var/run/docker.sock&amp;#39; - &amp;#39;/usr/local/jdk1.8.0_241:/usr/local/jdk1.8.0_241&amp;#39; - &amp;#39;/usr/local/apache-maven-3.6.3:/usr/local/apache-maven-3.6.3&amp;#39; restart: always 2. 启动jenkins docker-compose up -d jenkins &amp;amp;&amp;amp; docker logs -f jenkins ==jenkins 启动时，一直处在 Please wait while Jenkins is getting ready to work &amp;hellip;== 需要更新hudson.model.UpdateCenter.xml的url：
&amp;lt;?xml version=&amp;#39;1.1&amp;#39; encoding=&amp;#39;UTF-8&amp;#39;?&amp;gt; &amp;lt;sites&amp;gt; &amp;lt;site&amp;gt; &amp;lt;id&amp;gt;default&amp;lt;/id&amp;gt; &amp;lt;url&amp;gt;https://updates.jenkins.io/update-center.json&amp;lt;/url&amp;gt; &amp;lt;/site&amp;gt; &amp;lt;/sites&amp;gt; 更新为：
&amp;lt;?xml version=&amp;#39;1.1&amp;#39; encoding=&amp;#39;UTF-8&amp;#39;?&amp;gt; &amp;lt;sites&amp;gt; &amp;lt;site&amp;gt; &amp;lt;id&amp;gt;default&amp;lt;/id&amp;gt; &amp;lt;url&amp;gt;http://mirror.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins&#43;docker&#43;%E8%85%BE%E8%AE%AF%E4%BA%91%E5%AE%B9%E5%99%A8%E4%BB%93%E5%BA%93&#43;github%E6%90%AD%E5%BB%BAci_cd%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins&#43;docker&#43;%E8%85%BE%E8%AE%AF%E4%BA%91%E5%AE%B9%E5%99%A8%E4%BB%93%E5%BA%93&#43;github%E6%90%AD%E5%BB%BAci_cd%E7%8E%AF%E5%A2%83/</guid>
      <description>jenkins+docker+腾讯云容器仓库+github搭建CI/CD环境
持续集成/持续部署的重要性不必多言，都什么年代了，没有哪个正紧项目还在人工构建/测试/部署。本文手把手教你搭建jenkins+docker+腾讯云容器仓库+github的CI/CD环境。妥妥的干活，绝对是解放生产力的利器。架构图如下：
step1: 程序猿git push代码到github step2: jenkins通过github webhooks触发任务（ jenkins构建项目，并制作docker镜像） step3: 推送镜像到容器仓库 step4: 应用服务器拉去镜像到本地运行
为了不让文章篇幅过长，本系列tony徐老师将文章分成两个小章节：
github webhooks+jenkins搭建持续集成环境 本文是jenkins+docker+腾讯云容器仓库+github搭建CI/CD环境的第一子章节。意在教您通过github webhooks+jenkins搭建持续集成环境，如果您已经完成webhooks+jenkins的配置，可以直接调整到第二子章节jenkins+腾讯云容器仓库+docker持续部署1. github配置Personal access tokens 没什么好说的，按图操作： 记下生成的token，待会儿要用的。
2. github配置webhooks 去到你的github项目仓库，按图操作： 这里的Payload URL = 你的jenkins地址+/github-webhook/
至此，github上的配置全部完成。接下来，我们配置jenkins。
 3. jenkins配置凭据 这里的Secret就是在第1节配置的Personal access tokens
 添加一个github登陆凭据，username和password对应github的登陆账号密码。 添加一个宿主机登陆凭据，username和password对应登陆的账号密码（因为小编的jenkins是容器的方式运行的，因此后面需要这个凭据ssh到宿主机上执行命令）。  4. 配置ssh sites 接下来配置一个ssh sites，用于后面需要ssh到宿主机。 5. 配置jenkins运行环境 这里的环境取决与你的项目需要的环境。 如果你的jenkins是直接运行在机器上，那么指定对应环境的路径即可。如果是运行在容器里面，容器启动的时候把宿主机的环境映射到容器即可。
6. 新建一个maven项目 本小节以一个maven项目为例子，下一个小节我还补充了vue项目的例子。 请按图操作： 请按图操作： 请按图操作： 这里对上图解释一下：上图的含义是，当maven构建完项目之后，制作docker镜像，并推送到远程容器仓库。当然，你也可以构建完项目之后，直接java -jar本地运行，这一切取决于你的需求！！！</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins%E4%BD%BF%E7%94%A8%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins%E4%BD%BF%E7%94%A8%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</guid>
      <description>Jenkins使用使用注意事项
1. 无法通过execute shell启动进程 这是因为Jenkins默认会在Build结束后Kill掉所有的衍生进程。
在执行shell前需要设置BUILD_ID=dontKillMe
BUILD_ID=dontKillMe cd /data/wwwroot/yxshop chmod +x ./*.sh nohup ./restart.sh &amp;gt;/data/wwwroot/yxshop/nohup.out 2&amp;gt;&amp;amp;1 &amp;amp; 2. github向jenkins deliver失败 一开始填写的Payload URL形如，
Payload URL = https://xxx.com/github-webhook无论怎么修改再redeliver都失败，后来修改为形如，
Payload URL = https://xxx.com/github-webhook/仅仅是在后面加了“/”。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%E6%8F%90%E9%80%9F/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%E6%8F%90%E9%80%9F/</guid>
      <description>Jenkins安装插件提速
国内安装Jenkins插件缓慢，这是我见到最完美的解决方案。 通过【插件&amp;ndash;&amp;gt;高级&amp;ndash;&amp;gt;更新网站】替换成清华的数据源（https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json）并不好使
需要按照如下步骤操作：
进入到工作目录 $ cd {你的Jenkins工作目录}/updates #进入更新配置位置 修改default.json 方法1 $ vim default.json 替换所有插件下载的url
:1,$s/http:\/\/updates.jenkins-ci.org\/download/https:\/\/mirrors.tuna.tsinghua.edu.cn\/jenkins/g 替换连接测试url
:1,$s/http:\/\/www.google.com/https:\/\/www.baidu.com/g 进入vim先输入：然后再粘贴上边的：后边的命令，注意不要写两个冒号！
修改完成保存退出:wq
方法2 使用sed
$ sed -i &amp;#39;s/http:\/\/updates.jenkins-ci.org\/download/https:\/\/mirrors.tuna.tsinghua.edu.cn\/jenkins/g&amp;#39; default.json &amp;amp;&amp;amp; sed -i &amp;#39;s/http:\/\/www.google.com/https:\/\/www.baidu.com/g&amp;#39; default.json 这是直接修改的配置文件，如果前边Jenkins用sudo启动的话，那么这里的两个sed前均需要加上sudo
重启Jenkins，安装插件试试，简直超速！！
https://www.cnblogs.com/hellxz/p/jenkins_install_plugins_faster.html</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/cicd/%E4%B8%BAdocker%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/cicd/%E4%B8%BAdocker%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85%E5%B7%A5%E5%85%B7/</guid>
      <description>为docker容器安装工具
因为我的docker jenkins需要node环境，默认是没有的，所以最初想法是和maven一样，将宿主机的maven和docker一起共享使用，但是不管怎么操作就不行，很诡异，同样的操作，maven可以执行，jdk和node却不行，如下所示：
version: &amp;#39;3&amp;#39; services: jenkins: image: jenkinsci/blueocean deploy: resources: limits: cpus: &amp;#39;0.50&amp;#39; memory: 1024M reservations: memory: 64M container_name: jenkins user: root ports: - &amp;#39;8080:8080&amp;#39; - &amp;#39;50000:50000&amp;#39; volumes: - &amp;#39;/docker/volumes/jenkins:/var/jenkins_home&amp;#39; - &amp;#39;/var/run/docker.sock:/var/run/docker.sock&amp;#39; - &amp;#39;/usr/local/jdk1.8.0_241:/usr/local/jdk1.8.0_241&amp;#39; - &amp;#39;/usr/local/apache-maven-3.6.3:/usr/local/apache-maven-3.6.3&amp;#39; - &amp;#39;/usr/bin/docker:/usr/bin/docker&amp;#39; - &amp;#39;/usr/local/node-v12.16.3-linux-x64:/usr/local/node-v12.16.3-linux-x64&amp;#39; restart: always  更诡异的是，虽然docker下无法执行node命令，jenkins却能调用：
这里暂且不管，请当作docker没有node环境，继续往下阅读。
 经过彻夜未眠的排除，基本能定位为题的原因：本质不是node命令找不到，而是node的依赖在docker容器里面找不到，node依赖如下：
所以要么进入到容器里面使用apk add，要么自己制作镜像（这样太麻烦了），我选择前者。
进入到容器里面安装：
apk add --no-cache nodejs  可能会遇到下载缓慢、找不到包等问题。
 解决办法如下：
 在 https://mirrors.alpinelinux.org/找到中国的镜像仓库，并添加到 etc/apk/repositories, 参考如下： A. 永久修改下载源
vi etc/apk/repositories http://mirrors.aliyun.com/alpine/v3.8/main/ http://mirrors.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/istio%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/istio%E7%81%B0%E5%BA%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/istio%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/istio%E7%81%B0%E5%BA%A6/</guid>
      <description>istio灰度
Gateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - &amp;#34;*&amp;#34; VirtualService 是在 Istio 服务网格内对服务的请求如何进行路由控制？VirtualService中就包含了这方面的定义。例如一个 Virtual Service 可以把请求路由到不同版本，甚至是可以路由到一个完全不同于请求要求的服务上去。路由可以用很多条件进行判断，例如请求的源和目的地、HTTP 路径和 Header 以及各个服务版本的权重等。
路由规则对应着一或多个用 VirtualService 配置指定的请求目的主机。这些主机可以是也可以不是实际的目标负载，甚至可以不是同一网格内可路由的服务。例如要给到 reviews 服务的请求定义路由规则，可以使用内部的名称 reviews，也可以用域名 bookinfo.com，VirtualService 可以定义这样的 host 字段：
hosts:- reviews- bookinfo.com复制代码host 字段用显示或者隐式的方式定义了一或多个完全限定名（FQDN）。上面的 reviews，会隐式的扩展成为特定的 FQDN，例如在 Kubernetes 环境中，全名会从 VirtualService 所在的集群和命名空间中继承而来（比如说 reviews.default.svc.cluster.local）。
--- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - &amp;#34;*&amp;#34; gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 所有的流量都去到reviews:v1</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/%E5%8E%9F%E7%90%86/%E5%8E%9F%E7%90%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/%E5%8E%9F%E7%90%86/%E5%8E%9F%E7%90%86/</guid>
      <description>原理</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC/%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC/%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC/</guid>
      <description>服务网格</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E5%90%AF%E7%94%A8%E7%AD%96%E7%95%A5%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E5%90%AF%E7%94%A8%E7%AD%96%E7%95%A5%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/</guid>
      <description>启用策略检查功能
不得不说，istio的官方文档真的很垃圾，操作手册没有随着版本同步更新不说，启用策略检查功能依照官方做法也不能生效。现先以istio-1.5.10为例总结启用策略检查功能的方法如下：
 默认安装istio之后disablePolicyChecks=true  [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system get cm Istio -o jsonpath=&amp;#34;{@.data.mesh}&amp;#34; | grep disablePolicyChecks disablePolicyChecks: true 编辑 istio configmap 以启用策略检查  [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system get cm Istio -o jsonpath=&amp;#34;{@.data.mesh}&amp;#34; | sed -e &amp;#34;s/disablePolicyChecks: true/disablePolicyChecks: false/&amp;#34; &amp;gt; /tmp/mesh.yaml [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system create cm Istio -o yaml --dry-run --from-file=mesh=/tmp/mesh.yaml | kubectl replace -f - W0902 17:00:37.208604 16538 helpers.go:535] --dry-run is deprecated and can be replaced with --dry-run=client.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E5%AE%89%E8%A3%85/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E5%AE%89%E8%A3%85/</guid>
      <description>安装
1. 下载 Istio 这里推荐直接下载tar.gz安装包，不推荐使用官网上的那个安装脚本（慢得一逼）。另外推荐下载istio-1.5.10，不推荐下载1.7.0。
解压：
tar -xzvf Istio-1.5.10-linux.tar.gz 拷贝istioctl到/usr/local/bin/
cd Istio-1.5.10/ cp bin/istioctl /usr/local/bin/ 查看版本：
istioctl version 2. 安装istio 基于demo的配置安装istio（除了demo，还有default等等，具体配置见istio-1.5.10/install/kubernetes/operator/profiles）
istioctl manifest apply --set profile=demo 查看svc：kubectl get svc -n istio-system
查看pod：kubectl get pods -n istio-system
这里需要注意，如果使用的是1.5.10以后的高版本，安装命令应该是：istioctl manifest install --set profile=demo
并且，最新版本1.7.0不再默认安装grafana ``kiali ``zipkin等等组件。
当使用 kubectl apply 来部署应用时，如果 pod 启动在标有 istio-injection=enabled 的命名空间中，那么，Istio sidecar 注入器将自动注入 Envoy 容器到应用的 pod 中：
kubectl label namespace &amp;lt;namespace&amp;gt; Istio-injection=enabled 3.卸载 卸载程序将删除 RBAC 权限、istio-system 命名空间和所有相关资源。可以忽略那些不存在的资源的报错，因为它们可能已经被删除掉了。
istioctl manifest generate --set profile=demo | kubectl delete -f - 参考 istio官网: 开始</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E6%9A%B4%E9%9C%B2grafana%E7%AD%89%E5%86%85%E9%83%A8%E7%BB%84%E4%BB%B6%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E6%9A%B4%E9%9C%B2grafana%E7%AD%89%E5%86%85%E9%83%A8%E7%BB%84%E4%BB%B6%E6%9C%8D%E5%8A%A1/</guid>
      <description>暴露grafana等内部组件服务
在安装完成istio后，默认状态下，集群外用户不能直接访问istio集群内的grafana等管理、监控服务。
有两种方法可以将集群内服务开放出来。一种是使用port-forward方式将本地端口流量转发到pod端口，实现集群内服务的访问；另一种方式是采用istio gateway方式，将集群内服务暴露到外网。
第一种方式（以暴露Prometheus为例，官方教程也是这种方式，这种方式极其反人类，推荐使用下面的第二种方式）：
kubectl -n Istio-system port-forward $(kubectl -n Istio-system get pod -l app=prometheus -o jsonpath=&amp;#39;{.items[0].metadata.name}&amp;#39;) 9090:9090 &amp;amp; 第二种方式需要将集群的默认网关服务ingressgateway的网络模式设置为LB/nodeport模式，作为代理实现对外服务。
1. 设置ingress gateway的工作模式 安装istio的时候默认就是LB的
2. 验证ingress gateway的网络模式 [root@k8s-master ~]# kubectl get svc -n Istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.101.122.19 &amp;lt;none&amp;gt; 3000/TCP 11h Istio-egressgateway ClusterIP 10.106.2.83 &amp;lt;none&amp;gt; 80/TCP,443/TCP,15443/TCP 11h Istio-ingressgateway LoadBalancer 10.110.238.41 &amp;lt;pending&amp;gt; 15020:31598/TCP,80:30299/TCP,443:31413/TCP,15029:30249/TCP,15030:32499/TCP,15031:31399/TCP,15032:32373/TCP,31400:30156/TCP,15443:30319/TCP 11h Istio-pilot ClusterIP 10.103.43.172 &amp;lt;none&amp;gt; 15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP 11h istiod ClusterIP 10.105.251.21 &amp;lt;none&amp;gt; 15012/TCP,443/TCP 11h jaeger-agent ClusterIP None &amp;lt;none&amp;gt; 5775/UDP,6831/UDP,6832/UDP 11h jaeger-collector ClusterIP 10.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.2-k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.2-k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid>
      <description>k8s基本概念
1⃣️ Container Container（容器）是一种便携式、轻量级的操作系统级虚拟化技术。它使用 namespace 隔离不同的软件运行环境，并通过镜像自包含软件的运行环境，从而使得容器可以很方便的在任何地方运行。
2⃣️ Pod Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。
Pod 是一组紧密关联的容器集合，它们共享 PID（同一个Pod中应用可以看到其它进程）、IPC（同一个Pod中的应用可以通过VPC或者POSIX进行通信）、Network （同一个Pod的中的应用对相同的IP地址和端口有权限）和 UTS namespace（同一个Pod中的应用共享一个主机名称），是 Kubernetes 调度的基本单位。Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。
在 Kubernetes 中，所有对象都使用 manifest（yaml 或 json）来定义，比如一个简单的 nginx 服务可以定义为 nginx.yaml，它包含一个镜像为 nginx 的容器：
apiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 3⃣️ Node Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 docker 或者 rkt）、kubelet 和 kube-proxy 服务。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.2-k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.2-k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</guid>
      <description>参考链接
https://feisky.gitbooks.io/kubernetes/content/introduction/concepts.html</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.2-k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.2-k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/</guid>
      <description>基础概念
Cluster
Cluster 是计算、存储和网络资源的集合，Kubernetes 利用这些资源运行各种基于容器的应用。
Master
Master 是 Cluster 的大脑，它的主要职责是调度，即决定将应用放在哪里运行。Master 运行 Linux 操作系统，可以是物理机或者虚拟机。为了实现高可用，可以运行多个 Master。
Node
Node 的职责是运行容器应用。Node 由 Master 管理，Node 负责监控并汇报容器的状态，并根据 Master 的要求管理容器的生命周期。Node 运行在 Linux 操作系统，可以是物理机或者是虚拟机。
Pod
Pod 是 Kubernetes 的最小工作单元。每个 Pod 包含一个或多个容器。Pod 中的容器会作为一个整体被 Master 调度到一个 Node 上运行，一个Pod里的所有容器共用一个namespaces
Controller
管理Pod的工具，kubernetes通过它来管理集群中的Pod
Deployment
Deployment 是最常用的 Controller，比如前面在线教程中就是通过创建 Deployment 来部署应用的。Deployment 可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。
ReplicaSet
ReplicaSet 实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。
DaemonSet
DaemonSet 用于每个 Node 最多只运行一个 Pod 副本的场景。正如其名称所揭示的，DaemonSet 通常用于运行 daemon。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.3-k8s%E6%9E%B6%E6%9E%84/k8s%E6%9E%B6%E6%9E%84/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.3-k8s%E6%9E%B6%E6%9E%84/k8s%E6%9E%B6%E6%9E%84/</guid>
      <description>k8s架构
Kubernetes 主要由以下几个核心组件组成:
 etcd 保存了整个集群的状态，就是一个数据库； apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet 负责维护容器的生命周期，同时也负责 Volume（CSI）和网络（CNI）的管理； Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）； kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡；  当然了除了上面的这些核心组件，还有一些推荐的插件：
 kube-dns 负责为整个集群提供 DNS 服务 Ingress Controller 为服务提供外网入口 Heapster 提供资源监控 Dashboard 提供 GUI  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.4-%E4%BA%91%E5%8E%82%E5%95%86%E9%9B%86%E6%88%90k8s/%E4%BA%91%E5%8E%82%E5%95%86%E9%9B%86%E6%88%90k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.4-%E4%BA%91%E5%8E%82%E5%95%86%E9%9B%86%E6%88%90k8s/%E4%BA%91%E5%8E%82%E5%95%86%E9%9B%86%E6%88%90k8s/</guid>
      <description>云厂商集成k8s
阿里云 ACK 腾讯云 TKE 华为云 CCE AWS EKS Google GKE Azure AKS DigitalOcean k8s Oracle OKE</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.5-k8s%E6%8E%A5%E5%8F%A3/k8s%E6%8E%A5%E5%8F%A3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.5-k8s%E6%8E%A5%E5%8F%A3/k8s%E6%8E%A5%E5%8F%A3/</guid>
      <description>k8s接口
几种接口解释：
  CRI（Container Runtime Interface）。容器运行时接口。Kubernetes项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现CRI接入到Kubernetes项目当中。Kubernetes是通过kubelet跟CRI进行通信。
  OCI（Open Container Initiative）：具体的容器运行时，比如Docker项目，则一般通过OCI这个容器运行时规范同底层的Linux操作系统进行交互，即：把CRI请求翻译成对Linux操作系统的调用（操作Linux Namespace和Cgroups等）。
 CRI是k8s对外暴露的抽象容器接口，OCI是开发容器倡议，最终CRI接口会调用符合OCI的系统内核接口
容器生态三层抽象:
 Orchestration API -&amp;gt; Container API -&amp;gt; Kernel API
  Orchestration API: kubernetes API标准就是这层的标准,无可非议 Container API: 标准就是CRI Kernel API: 标准就是OCI     CNI（Container Networking Interface）：调用网络插件做网络通信。
  CSI（Container Storage Interface）：调用存储插件配置持久化存储。
  kubelet还通过gRPC协议同一个叫作Device Plugin的插件进行交互。
   以上几种接口都是kubelet所要支持的功能，所以计算节点上最核心的组件就是kubelet。
 </description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.6-k8s%E7%BB%84%E4%BB%B6/k8s%E7%BB%84%E4%BB%B6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.6-k8s%E7%BB%84%E4%BB%B6/k8s%E7%BB%84%E4%BB%B6/</guid>
      <description>k8s组件
Master（控制平面）组件 1⃣️ kube-apiserver  提供集群管理的 REST API接口，包括认证授权、数据校验以及集群状态变更等 提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才直接操作 etcd）  2⃣️ etcd etcd 是 CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。
主要功能🔧包括：
 基本的 key-value 存储 监听机制 key 的过期及续约机制，用于监控和服务发现 原子 CAS 和 CAD，用于分布式锁和 leader 选举  Etcd，Zookeeper，Consul 比较 🆚：
 Etcd 和 Zookeeper 提供的能力非常相似，都是通用的一致性元信息存储，都提供 watch 机制用于变更通知和分发，也都被分布式系统用来作为共享信息存储，在软件生态中所处的位置也几乎是一样的，可以互相替代的。二者除了实现细节，语言，一致性协议上的区别，最大的区别在周边生态圈。Zookeeper 是 apache 下的，用 java 写的，提供 rpc 接口，最早从 hadoop 项目中孵化出来，在分布式系统中得到广泛使用（hadoop, solr, kafka, mesos 等）。Etcd 是 coreos 公司旗下的开源产品，比较新，以其简单好用的 rest 接口以及活跃的社区俘获了一批用户，在新的一些集群中得到使用（比如 kubernetes）。虽然 v3 为了性能也改成二进制 rpc 接口了，但其易用性上比 Zookeeper 还是好一些。 而 Consul 的目标则更为具体一些，Etcd 和 Zookeeper 提供的是分布式一致性存储能力，具体的业务场景需要用户自己实现，比如服务发现，比如配置变更。而 Consul 则以服务发现和配置变更为主要目标，同时附带了 kv 存储。  3⃣️ kube-scheduler kube-scheduler 负责分配调度 Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 NodeName 字段）。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.6-k8s%E7%BB%84%E4%BB%B6/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.6-k8s%E7%BB%84%E4%BB%B6/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</guid>
      <description>参考链接
https://feisky.gitbooks.io/kubernetes/content/components/components.html</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.7-k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.7-k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/</guid>
      <description>k8s组件间通信
Kubernetes 多组件之间的通信原理：
 apiserver 负责 etcd 存储的所有操作，且只有 apiserver 才直接操作 etcd 集群 apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件均通过 apiserver 进行通信  controller manager、scheduler、kube-proxy 和 kubelet 等均通过 apiserver watch API 监测资源变化情况，并对资源作相应的操作 所有需要更新资源状态的操作均通过 apiserver 的 REST API 进行   apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 --kubelet-certificate-authority 开启（而 GKE 通过 SSH 隧道保护它们之间的通信）  比如最典型的创建 Pod 的流程：
  通过 CLI 或者 UI 提交 Pod 部署请求给 Kubernetes API Server
  API Server 会把这个信息写入到它的存储系统 etcd</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.7-k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.7-k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</guid>
      <description>参考链接
https://www.qikqiak.com/k8s-book/docs/15.%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%BB%84%E4%BB%B6.htmlhttps://www.wumingx.com/k8s/kubernetes-introduction.html</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/configmap/</guid>
      <description>ConfigMap</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/cronjob/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/cronjob/</guid>
      <description>CronJob</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/daemonset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/daemonset/</guid>
      <description>DaemonSet</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/deployment/</guid>
      <description>Deployment</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/ingress/</guid>
      <description>Ingress</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/job/</guid>
      <description>Job</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/pv_pvc_storageclass/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/pv_pvc_storageclass/</guid>
      <description>PV/PVC/StorageClass
名词解释：PV/PVC/StorageClass 本文较老，建议查看新文档：http://docs.kubernetes.org.cn/429.html 介绍 PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。
PersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。
虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 群集管理员需要能够提供多种不同于PersistentVolumes的PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。
StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”
例子 https://kubernetes.io/docs/user-guide/persistent-volumes/walkthrough/Lifecycle of a volume and claim PV是集群中的资源。 PVC是对这些资源的请求，也是对资源的索赔检查。 PV和PVC之间的相互作用遵循这个生命周期:
Provisioning ——-&amp;gt; Binding ——–&amp;gt;Using——&amp;gt;Releasing——&amp;gt;Recycling
Provisioning 这里有两种PV的提供方式:静态或者动态
Static集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费。Dynamic当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置Binding 在动态配置的情况下，用户创建或已经创建了具有特定数量的存储请求和特定访问模式的PersistentVolumeClaim。 主机中的控制回路监视新的PVC，找到匹配的PV（如果可能），并将它们绑定在一起。 如果为新的PVC动态配置PV，则循环将始终将该PV绑定到PVC。 否则，用户总是至少得到他们要求的内容，但是卷可能超出了要求。 一旦绑定，PersistentVolumeClaim绑定是排他的，不管用于绑定它们的模式。
如果匹配的卷不存在，PVC将保持无限期。 随着匹配卷变得可用，PVC将被绑定。 例如，提供许多50Gi PV的集群将不匹配要求100Gi的PVC。 当集群中添加100Gi PV时，可以绑定PVC。
Using Pod使用PVC作为卷。 集群检查声明以找到绑定的卷并挂载该卷的卷。 对于支持多种访问模式的卷，用户在将其声明用作pod中的卷时指定所需的模式。
一旦用户有声明并且该声明被绑定，绑定的PV属于用户，只要他们需要它。 用户通过在其Pod的卷块中包含persistentVolumeClaim来安排Pods并访问其声明的PV。
Releasing 当用户完成卷时，他们可以从允许资源回收的API中删除PVC对象。 当声明被删除时，卷被认为是“释放的”，但是它还不能用于另一个声明。 以前的索赔人的数据仍然保留在必须根据政策处理的卷上.
Reclaiming PersistentVolume的回收策略告诉集群在释放其声明后，该卷应该如何处理。 目前，卷可以是保留，回收或删除。 保留可以手动回收资源。 对于那些支持它的卷插件，删除将从Kubernetes中删除PersistentVolume对象，以及删除外部基础架构（如AWS EBS，GCE PD，Azure Disk或Cinder卷）中关联的存储资产。 动态配置的卷始终被删除</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/service-account/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/service-account/</guid>
      <description>Service Account</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/services/</guid>
      <description>Services
名词解释 Services Overview（概述） Kubernetes Pod是平凡的，它门会被创建，也会死掉（生老病死），并且他们是不可复活的。 ReplicationControllers动态的创建和销毁Pods(比如规模扩大或者缩小，或者执行动态更新)。每个pod都由自己的ip，这些IP也随着时间的变化也不能持续依赖。这样就引发了一个问题：如果一些Pods（让我们叫它作后台，后端）提供了一些功能供其它的Pod使用（让我们叫作前台），在kubernete集群中是如何实现让这些前台能够持续的追踪到这些后台的？
答案是：Service
Kubernete Service 是一个定义了一组Pod的策略的抽象，我们也有时候叫做宏观服务。这些被服务标记的Pod都是（一般）通过label Selector决定的（下面我们会讲到我们为什么需要一个没有label selector的服务）
举个例子，我们假设后台是一个图形处理的后台，并且由3个副本。这些副本是可以相互替代的，并且前台并需要关心使用的哪一个后台Pod，当这个承载前台请求的pod发生变化时，前台并不需要直到这些变化，或者追踪后台的这些副本，服务是这些去耦
对于Kubernete原生的应用，Kubernete提供了一个简单的Endpoints API，这个Endpoints api的作用就是当一个服务中的pod发生变化时，Endpoints API随之变化，对于哪些不是原生的程序，Kubernetes提供了一个基于虚拟IP的网桥的服务，这个服务会将请求转发到对应的后台pod
Defining a service(定义一个服务) 一个Kubernete服务是一个最小的对象，类似pod,和其它的终端对象一样，我们可以朝paiserver发送请求来创建一个新的实例，比如，假设你拥有一些Pod,每个pod都开放了9376端口，并且均带有一个标签app=MyApp
{ &amp;#34;“kind”&amp;#34;: &amp;#34;“Service”&amp;#34;, &amp;#34;“apiVersion”&amp;#34;: &amp;#34;“v1”&amp;#34;, &amp;#34;“metadata”&amp;#34;: { &amp;#34;“name”&amp;#34;: &amp;#34;“my-service”&amp;#34; }, &amp;#34;“spec”&amp;#34;: { &amp;#34;“selector”&amp;#34;: { &amp;#34;“app”&amp;#34;: &amp;#34;“MyApp”&amp;#34; }, &amp;#34;“ports”&amp;#34;: [ { &amp;#34;“protocol”&amp;#34;: &amp;#34;“TCP”&amp;#34;, &amp;#34;“port”&amp;#34;: 80, &amp;#34;“targetPort”&amp;#34;: 9376 } ] } } 这段代码会创建一个新的服务对象，名称为：my-service，并且会连接目标端口9376，并且带有label app=MyApp的pod,这个服务会被分配一个ip地址，这个ip是给服务代理使用的（下面我们会看到），服务的选择器会持续的评估，并且结果会被发送到一个Endpoints 对象，这个Endpoints的对象的名字也叫“my-service”.
服务可以将一个“入端口”转发到任何“目标端口”，默认情况下targetPort的值会和port的值相同，更有趣的是，targetPort可以是字符串，可以指定到一个name,这个name是pod的一个端口。并且实际指派给这个name的端口可以是不同在不同的后台pod中，这样让我们能更加灵活的部署我们的服务，比如；我们可以在下一个更新版本中修改后台pod暴露的端口而不会影响客户的使用（更新过程不会打断）
 服务支持tcp和UDP，但是默认的是TCP Services without selectors（没有选择器的服务）  服务总体上抽象了对Pod的访问，但是服务也抽象了其它的内容，比如：
1：比如你希望有一个额外的数据库云在生产环境中，但是在测试的时候，我们希望使用自己的数据库
2：我们希望将服务指向其它的服务或者其它命名空间或者其它的云平台上的服务
3：我们正在向kubernete迁移，并且我们后台并没有在Kubernete中
如上的情况下，我们可以定义一个服务没有选择器
{ &amp;#34;“kind”&amp;#34;: &amp;#34;“Service”&amp;#34;, &amp;#34;“apiVersion”&amp;#34;: &amp;#34;“v1″&amp;#34;, &amp;#34;“metadata”&amp;#34;: { &amp;#34;“name”&amp;#34;: &amp;#34;“my-service”&amp;#34; }, &amp;#34;“spec”&amp;#34;: { &amp;#34;“ports”&amp;#34;: [ { &amp;#34;“protocol”&amp;#34;: &amp;#34;“TCP”&amp;#34;, &amp;#34;“port”&amp;#34;: 80, &amp;#34;“targetPort”&amp;#34;: 9376 } ] } } 因为没有选择器，所以相应的Endpoints对象就不会被创建，但是我们手动把我们的服务和Endpoints对应起来</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/statefulset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/statefulset/</guid>
      <description>StatefulSet</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.8-k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5/</guid>
      <description>参考链接
https://www.kubernetes.org.cn/http://docs.kubernetes.org.cn/</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.9-k8s%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/ingress%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.9-k8s%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/ingress%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/</guid>
      <description>ingress实现灰度
Nginx-ingress 架构和原理 迅速回顾一下 Nginx-ingress 的架构和实现原理：
Nginx-ingress 通过前置的 Loadbalancer 类型的 Service 接收集群流量，将流量转发至 Nginx-ingress Pod 内并对配置的策略进行检查，再转发至目标 Service，最终将流量转发至业务容器。
传统的 Nginx 需要我们配置 conf 文件策略。但 Nginx-ingress 通过实现 Nginx-ingress-Controller 将原生 conf 配置文件和 yaml 配置文件进行了转化，当我们配置 yaml 文件的策略后，Nginx-ingress-Controller 将对其进行转化，并且动态更新策略，动态 Reload Nginx Pod，实现自动管理。
那么 Nginx-ingress-Controller 如何能够动态感知集群的策略变化呢？方法有很多种，可以通过 webhook admission 拦截器，也可以通过 ServiceAccount 与 Kubernetes Api 进行交互，动态获取。Nginx-ingress-Controller 使用后者来实现。所以在部署 Nginx-ingress 我们会发现 Deployment 内指定了 Pod 的 ServiceAccount，以及实现了 RoleBinding ，最终达到 Pod 与 Kubernetes Api 交互的目标。
dev apiVersion: v1 kind: Namespace metadata: name: dev --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: dev spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wangweicoding-docker.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.9-k8s%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/%E5%A4%9Apod%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/1.%E5%9F%BA%E7%A1%80/1.2-k8s%E5%9F%BA%E7%A1%80/1.2.9-k8s%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/%E5%A4%9Apod%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/</guid>
      <description>多Pod实现灰度
如果您想使用 Deployment 将最新的应用程序版本发布给一部分用户（或服务器），您可以为每个版本创建一个 Deployment，此时，应用程序的新旧两个版本都可以同时获得生产上的流量。
实施方案   部署第一个版本
第一个版本的 Deployment 包含了 3 个Pod副本，Service 通过 label selector app: nginx 选择对应的 Pod，nginx 的标签为 1.7.9
apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 --- apiVersion: v1 kind: Service metadata: name: nginx-service labels: app: nginx spec: selector: app: nginx ports: - name: nginx-port protocol: TCP port: 80 nodePort: 32600 targetPort: 80 type: NodePort   假设此时想要发布新的版本 nginx 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-operator%E5%AE%9E%E6%88%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-operator%E5%AE%9E%E6%88%98/</guid>
      <description>operator实战 1⃣️ 需求 我们将定义一个 crd ，spec 包含以下信息：
Replicas	# 副本数Image	# 镜像Resources	# 资源限制Envs	# 环境变量Ports	# 服务端口2⃣️ 编码 初始化项目目录：
tony@192 k8s % pwd /Users/tony/workspace/k8s tony@192 k8s % mkdir -p app-operator/src/github.com/xuzhijvn/app cd app-operator/src/github.com/xuzhijvn/app 初始化operator项目结构，并创建api：
operator-sdk init --domain=huolala.cn --repo=github.com/xuzhijvn/app operator-sdk create api --group app --version v1 --kind App --resource=true --controller=true 修改 CRD 类型定义代码 api/v1/app_types.go:
/* Copyright 2021. Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;); you may not use this file except in compliance with the License.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0k8s%E8%AF%81%E4%B9%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0k8s%E8%AF%81%E4%B9%A6/</guid>
      <description>使用kubeadm更新k8s证书 今天操作k8s的时候，突然说证书无效：
Unable to connect to the server: x509: certificate has expired or is not yet valid 通过 kubeadm alpha certs check-expiration 查看，确实是过期了：
[root@k8s-master ~]# kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0627 11:21:35.745166 8754 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-%E5%90%8C%E6%AD%A5%E9%95%9C%E5%83%8F%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%93%E5%BA%93/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-%E5%90%8C%E6%AD%A5%E9%95%9C%E5%83%8F%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%93%E5%BA%93/</guid>
      <description>同步镜像到自己的仓库 经常有一些国外的镜像仓库，在国内无法拉取，此时我们可以先找台国外的服务器拉取下来，重新打tag后推送到自己的镜像仓库。
1⃣️ 免费服务器
https://labs.play-with-k8s.com/2⃣️ 拉取-打tag-推送
3⃣️ 验证
参考链接🔗 如何拉取k8s.grc.io、quay.io的镜像</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/_-%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>本地连接k8s集群 在日常开发的过程中，经常会需要在本地开发的程序需要在k8s中调试的场景，比如，写了一个operator。
如果此时，本地又没有可以直接可达的k8s集群，
比如k8s是在公有云的vpc环境内，外面无法直接访问。想要本地连接远程k8s集群，可以参考 本地连接远程的内网k8s集群再比如k8s集群是自己通过多台云服务器自行搭建的，master节点有自己的公网ip。想要本地连接远程k8s集群，可以参考本文。
1⃣️ 重新生成config文件 默认下，~/.kube/config 生成配置文件的时候只包含了k8s集群ip和这个节点的局域网ip，本地如果想远程操作k8s的话，必定要通过公网ip连接到k8s集群，所以我们需要把节点绑定的公网ip也放到证书里面去，即我们需要重新生成证书。如果不这样做，本地直接访问的话，会报如下提示：
tony@192 ~ % kubectl get pod Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 172.17.0.14, not 106.55.152.92 先备份证书：
[root@k8s-master .kube]# mkdir -p /etc/kubernetes/pki.bak [root@k8s-master .kube]# mv /etc/kubernetes/pki/apiserver.* /etc/kubernetes/pki.bak 重新生成证书：
[root@k8s-master .kube]# kubeadm init phase certs all --apiserver-advertise-address=0.0.0.0 --apiserver-cert-extra-sans=10.96.0.1,172.17.0.14,xxx.xxx.xxx.xxx(公网ip) I0627 15:10:39.069106 7777 version.go:252] remote version is much newer: v1.21.2; falling back to: stable-1.18 W0627 15:10:40.982380 7777 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/ingress/</guid>
      <description>ingress
1. ingress是什么 ingress是k8s一种资源对象，如k8s的Deployment、Service资源对象一样。它是一种集群维度暴露服务的方式，正如k8s的ClusterIP、NodePort、LoadBalance一样，但是ClusterIP的方式只能在集群内部访问，NodePort方式的话，测试环境使用还行，当有几十上百的服务在集群中运行时，NodePort的端口管理是灾难，LoadBalance方式受限于云平台，且通常在云平台部署ELB还需要额外的费用。ingress规则是很灵活的，可以根据不同域名、不同path转发请求到不同的service，并且支持https/http。 2. ingress与ingress-controller 要理解ingress，需要区分两个概念，ingress和ingress-controller：
 ingress：指的是k8s中的一个api对象，一般用yaml配置。作用是定义请求如何转发到service的规则，可以理解为配置模板。 ingress-controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发。  简单来说，ingress-controller才是负责具体转发的组件，通过各种方式将它暴露在集群入口，外部对集群的请求流量会先到ingress-controller，而ingress对象是用来告诉ingress-controller该如何转发请求，比如哪些域名哪些path要转发到哪些服务等等。
2.1 ingress ingress是一个API对象，和其他对象一样，通过yaml文件来配置。ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方向代理。ingress要依靠ingress-controller来具体实现以上功能。前一小节的图如果用ingress来表示，大概就是如下配置：
apiVersion: extensions/v1beta1 kind: Ingress metadata: name: abc-ingress annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; nginx.ingress.kubernetes.io/use-regex: &amp;#34;true&amp;#34; spec: tls: - hosts: - api.abc.com secretName: abc-tls rules: - host: api.abc.com http: paths: - backend: serviceName: apiserver servicePort: 80 - host: www.abc.com http: paths: - path: /image/* backend: serviceName: fileserver servicePort: 80 - host: www.abc.com http: paths: - backend: serviceName: feserver servicePort: 8080 2.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/java%E5%BA%94%E7%94%A8%E4%BB%8Enfs%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/java%E5%BA%94%E7%94%A8%E4%BB%8Enfs%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</guid>
      <description>java应用从nfs加载配置文件
背景 配置文件变化，无需重新构建镜像部署。
1. 准备nfs 1、准备好nfs服务器。参考：nfs安装的nfs服务端配置。 2、k8s node节点可以不启用rpcbind服务，但是必须安装nfs-utils（yum install nfs-utils），否则nfs-client-provisioner pod无法启动，因为nfs-client.yaml里面有nfs的相关配置，而这些nfs配置要生效需依赖nfs-utils。参考：nfs安装的nfs客户端配置。
2. 创建StorageClass对象 理论部分参考：StorageClass2.1 nfs-client.yaml kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: nfs - name: NFS_PATH value: /mnt/nfs/k8s volumes: - name: nfs-client-root nfs: server: nfs path: /mnt/nfs/k8s [v_warn]k8s node节点可以不启用rpcbind服务，但必须安装nfs-utils。[/v_warn]</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/k8s_-%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E5%89%8D%E7%AB%AF%E9%9D%99%E6%80%81/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/k8s_-%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E5%89%8D%E7%AB%AF%E9%9D%99%E6%80%81/</guid>
      <description>k8s: 部署应用(前端静态)
0. 准备条件  部署好了k8s集群，部署可以参考Kubernetes: 从零搭建K8S     名称 数量 IP 备注     master 1 172.17.0.14 操作系统: Linux(centos7, 其它操作系统也可, 安装过程类似, 可参考官方文档) 机器配置: 4C8G   node1 1 172.18.0.7 同上   node2 1 172.19.0.5 同上     应用已经容器化，并上传到了远程仓库，笔者是腾讯云容器仓库：   理解k8s基础概念，可以参考Kubernetes: 基础概念介绍  1. 控制器管理Pod 1.1 生成deployment配置文件 kubectl create deployment yshop-h5 --image=ccr.ccs.tencentyun.com/yshop/h5 --dry-run -o yaml &amp;gt; yshop-h5.yaml 这个时候k8s还不能拉取镜像，需要生成拉取镜像的密钥。
1.2 生成拉取镜像的密钥 kubectl create secret docker-registry registry-secret-tencent --docker-server=ccr.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/nfs%E5%AE%89%E8%A3%85/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/nfs%E5%AE%89%E8%A3%85/</guid>
      <description>nfs安装
1. 环境说明    角色 ip os     NFS 服务端 172.31.0.12 CentOS 7.6   NFS 客户端 172.18.0.7 CentOS 7.6    2. NFS 服务端 2.1 安装 nfs-utils # rpcbind 作为依赖会自动安装。 &amp;gt; yum install nfs-utils 2.2 配置并启动服务 允许rpcbind.service、nfs.service开机自启：
# 启动相关服务 &amp;gt; systemctl start rpcbind &amp;gt; systemctl start nfs 防火墙允许服务通过：
# 防火墙允许服务通过 &amp;gt; firewall-cmd --zone=public --permanent --add-service={rpc-bind,mountd,nfs} success &amp;gt; firewall-cmd --reload success 或者直接关闭防火墙：systemctl stop firewalld</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/pvc%E4%B8%8D%E4%BC%9A%E4%B8%BB%E5%8A%A8%E5%9B%9E%E6%94%B6%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/pvc%E4%B8%8D%E4%BC%9A%E4%B8%BB%E5%8A%A8%E5%9B%9E%E6%94%B6%E9%97%AE%E9%A2%98/</guid>
      <description>pvc不会主动回收问题
使用k8s部署nacos的时候发现，通过volumeClaimTemplates配置的pvc不会随着使用kubectl delete -f xxx.yaml 删除StatefulSet的时候一同被删除。nacos-pvc-nfs.yaml文件如下所示：
--- apiVersion: v1 kind: Service metadata: name: nacos-headless labels: app: nacos annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: &amp;#34;true&amp;#34; spec: ports: - port: 8848 name: server targetPort: 8848 - port: 7848 name: rpc targetPort: 7848 clusterIP: None selector: app: nacos --- apiVersion: v1 kind: ConfigMap metadata: name: nacos-cm data: mysql.db.name: &amp;#34;nacos_devtest&amp;#34; mysql.port: &amp;#34;3306&amp;#34; mysql.user: &amp;#34;nacos&amp;#34; mysql.password: &amp;#34;nacos&amp;#34; --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nacos spec: serviceName: nacos-headless replicas: 3 template: metadata: labels: app: nacos annotations: pod.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BAk8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BAk8s/</guid>
      <description>从零搭建K8S
机器准备    名称 数量 IP 备注     master 1 172.17.0.14 操作系统: Linux(centos7, 其它操作系统也可, 安装过程类似, 可参考官方文档) 机器配置: 4C8G   node1 1 172.18.0.7 同上   node2 1 172.19.0.5 同上    由于本人很穷，这几台机器是分别属于不同的腾讯云账号，不同的账号之间不能内网通信，不过可以通过建立“对等连接”实现通信，比直接用公网通信靠谱。
1. 修改hostname [root@k8s-master ~]$ vim /etc/hostname # 修改hostname [root@k8s-master ~]$ vim /etc/hosts	# 将本机IP指向hostname [root@k8s-master ~]$ reboot -h # 重启(可以做完全部前期准备后再重启) 修改后：
[root@k8s-master ~]# cat /etc/hosts ::1 VM_0_5_centos VM_0_5_centos ::1 localhost.localdomain localhost ::1 localhost6.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E5%88%87%E6%8D%A2namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E5%88%87%E6%8D%A2namespace/</guid>
      <description>切换namespace
0. 背景 k8s如何切换namespace？？而不必每次执行命令的时候在后面指定namespace？k8s并没有直接提供切换namespace的命令。不过可以通过：
 切换context达到切换namespace的目的，这需要提前创建好context和namespace的绑定关系（如：kubectl config set-context test --namespace=test）,然后使用 kubectl config use-context test 切换context，从而间接的达到切换namespace的目的。这方法属实是太别扭了，强迫症患者都不喜欢。 或者使用kubectx工具，其本质是动态的修改context和namespace的绑定关系。使用形如：kubens test 即可切换。这样才是切换namespace的正确打开方式，优雅多了。  参考资料： Kubernetes命名空间一条命令解决Kubernetes更改默认的namespaceKubernetes 切换context和namespacek8s集群namespace和context使用ahmetb / kubectx提高您的kubectl生产力（第三部分）：集群上下文切换、使用别名减少输入和插件扩展</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E5%9C%A8namespace%E4%B9%8B%E9%97%B4%E5%85%B1%E4%BA%ABsecret/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E5%9C%A8namespace%E4%B9%8B%E9%97%B4%E5%85%B1%E4%BA%ABsecret/</guid>
      <description>在namespace之间共享secret
有的时候在default空间下创建了拉取镜像的secret，当部署k8s的资源到其他namespace的时候，如果部署的的是deployment的之类的势必会要拉取镜像，这个时候必然失败，因为secret创建在default空间下，所以我们需要将secret复制一份到需要的namespace下面：
kubectl get secret coding-regcred --namespace=default -oyaml | grep -v &amp;#39;^\s*namespace:\s&amp;#39; | kubectl apply --namespace=tkb -f - 参考链接：Kubernetes - sharing secret across namespaces</description>
    </item>
    
    <item>
      <title></title>
      <link>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E9%83%A8%E7%BD%B2sentinel-dashboard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://xuzhijvn.github.io/zh-cn/posts/k8s/2.%E5%AE%9E%E6%88%98/%E9%83%A8%E7%BD%B2sentinel-dashboard/</guid>
      <description>部署sentinel-dashboard
1. 构建镜像 FROMopenjdk:8-jdk-alpineVOLUME/tmpADD sentinel-dashboard-1.8.0.jar sentinel-dashboard.jarCMD java ${JAVA_OPTS} -jar sentinel-dashboard.jarEXPOSE87182. 创建headless service apiVersion: v1 kind: Service metadata: name: sentinel-headless labels: app: sentinel annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: &amp;#34;true&amp;#34; spec: ports: - port: 8718 name: server targetPort: 8718 clusterIP: None selector: app: sentinel 3. 创建StatefulSet apiVersion: apps/v1 kind: StatefulSet metadata: name: sentinel spec: serviceName: sentinel replicas: 1 template: metadata: labels: app: sentinel annotations: pod.alpha.kubernetes.io/initialized: &amp;#34;true&amp;#34; spec: containers: - name: sentinel imagePullPolicy: IfNotPresent image: ccr.ccs.tencentyun.com/xuzhijun/sentinel-dashboard:latest resources: requests: memory: &amp;#34;512Mi&amp;#34; cpu: &amp;#34;200m&amp;#34; ports: - containerPort: 8719 name: client env: - name: TZ value: Asia/Shanghai - name: JAVA_OPTS value: &amp;#34;-Dserver.</description>
    </item>
    
  </channel>
</rss>
