<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s实战 on 🇨🇳🇨🇳🇨🇳🇨🇳🇨🇳</title>
    <link>https://xuzhijvn.github.io/en/series/k8s%E5%AE%9E%E6%88%98/</link>
    <description>Recent content in k8s实战 on 🇨🇳🇨🇳🇨🇳🇨🇳🇨🇳</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Razon Yang. All Rights Reserved.</copyright>
    <lastBuildDate>Sun, 15 May 2022 14:46:38 +0800</lastBuildDate><atom:link href="https://xuzhijvn.github.io/en/series/k8s%E5%AE%9E%E6%88%98/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Yshop K8s</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/yshop-k8s/</link>
      <pubDate>Sun, 15 May 2022 14:46:38 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/yshop-k8s/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ingress</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/ingress/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/ingress/</guid>
      <description>1. ingress是什么 ingress是k8s一种资源对象，如k8s的Deployment、Service资源对象一样。它是一种集群维度暴露服务的方式，正如k8s的ClusterIP、NodePort、LoadBalance一样，但是ClusterIP的方式只能在集群内部访问，NodePort方式的话，测试环境使用还行，当有几十上百的服务在集群中运行时，NodePort的端口管理是灾难，LoadBalance方式受限于云平台，且通常在云平台部署ELB还需要额外的费用。ingress规则是很灵活的，可以根据不同域名、不同path转发请求到不同的service，并且支持https/http。
2. ingress与ingress-controller 要理解ingress，需要区分两个概念，ingress和ingress-controller：
ingress：指的是k8s中的一个api对象，一般用yaml配置。作用是定义请求如何转发到service的规则，可以理解为配置模板。 ingress-controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发。 简单来说，ingress-controller才是负责具体转发的组件，通过各种方式将它暴露在集群入口，外部对集群的请求流量会先到ingress-controller，而ingress对象是用来告诉ingress-controller该如何转发请求，比如哪些域名哪些path要转发到哪些服务等等。
2.1 ingress ingress是一个API对象，和其他对象一样，通过yaml文件来配置。ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方向代理。ingress要依靠ingress-controller来具体实现以上功能。前一小节的图如果用ingress来表示，大概就是如下配置：
apiVersion: extensions/v1beta1 kind: Ingress metadata: name: abc-ingress annotations: kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34; nginx.ingress.kubernetes.io/use-regex: &amp;#34;true&amp;#34; spec: tls: - hosts: - api.abc.com secretName: abc-tls rules: - host: api.abc.com http: paths: - backend: serviceName: apiserver servicePort: 80 - host: www.abc.com http: paths: - path: /image/* backend: serviceName: fileserver servicePort: 80 - host: www.abc.com http: paths: - backend: serviceName: feserver servicePort: 8080 2.2 ingress-controller ingress-controller并不是k8s自带的组件，实际上ingress-controller只是一个统称，用户可以选择不同的ingress-controller实现，目前，由k8s维护的ingress-controller只有google云的GCE与ingress-nginx两个，其他还有很多第三方维护的ingress-controller，具体可以参考官方文档。但是不管哪一种ingress-controller，实现的机制都大同小异，只是在具体配置上有差异。一般来说，ingress-controller的形式都是一个pod，里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的变化，根据ingress对象生成配置并应用新配置到反向代理，比如nginx-ingress就是动态生成nginx配置，动态更新upstream，并在需要的时候reload程序应用新配置。为了方便，后面的例子都以k8s官方维护的nginx-ingress为例。</description>
    </item>
    
    <item>
      <title>Java应用从nfs加载配置文件</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/java%E5%BA%94%E7%94%A8%E4%BB%8Enfs%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/java%E5%BA%94%E7%94%A8%E4%BB%8Enfs%E5%8A%A0%E8%BD%BD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</guid>
      <description>Java应用从nfs加载配置文件
背景 配置文件变化，无需重新构建镜像部署。
1. 准备nfs 1、准备好nfs服务器。参考：nfs安装 的nfs服务端配置。 2、k8s node节点可以不启用rpcbind服务，但是必须安装nfs-utils（yum install nfs-utils），否则nfs-client-provisioner pod无法启动，因为nfs-client.yaml里面有nfs的相关配置，而这些nfs配置要生效需依赖nfs-utils。参考：nfs安装 的nfs客户端配置。
2. 创建StorageClass对象 理论部分参考：StorageClass
2.1 nfs-client.yaml kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: nfs - name: NFS_PATH value: /mnt/nfs/k8s volumes: - name: nfs-client-root nfs: server: nfs path: /mnt/nfs/k8s k8s node节点可以不启用rpcbind服务，但必须安装nfs-utils。</description>
    </item>
    
    <item>
      <title>k8s部署应用(前端静态)</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/k8s%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E5%89%8D%E7%AB%AF%E9%9D%99%E6%80%81/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/k8s%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E5%89%8D%E7%AB%AF%E9%9D%99%E6%80%81/</guid>
      <description>k8s部署应用(前端静态)
0. 准备条件 部署好了k8s集群，部署可以参考Kubernetes: 从零搭建K8S 名称 数量 IP 备注 master 1 172.17.0.14 操作系统: Linux(centos7, 其它操作系统也可, 安装过程类似, 可参考官方文档) 机器配置: 4C8G node1 1 172.18.0.7 同上 node2 1 172.19.0.5 同上 应用已经容器化，并上传到了远程仓库，笔者是腾讯云容器仓库： 理解k8s基础概念，可以参考Kubernetes: 基础概念介绍
1. 控制器管理Pod 1.1 生成deployment配置文件 kubectl create deployment yshop-h5 --image=ccr.ccs.tencentyun.com/yshop/h5 --dry-run -o yaml &amp;gt; yshop-h5.yaml 这个时候k8s还不能拉取镜像，需要生成拉取镜像的密钥。
1.2 生成拉取镜像的密钥 kubectl create secret docker-registry registry-secret-tencent --docker-server=ccr.ccs.tencentyun.com --docker-username=腾讯云账户ID --docker-password=腾讯云容器仓库密码 &amp;ndash;docker-server: 仓库地址 &amp;ndash;docker-username: 仓库登陆账号 &amp;ndash;docker-password: 仓库登陆密码 &amp;ndash;docker-email: 邮件地址(选填) -n 命名空间(选填)
可以运行：kubectl get secret registry-secret-tencent --output=yaml 查看生成的密钥</description>
    </item>
    
    <item>
      <title>nfs安装</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/nfs%E5%AE%89%E8%A3%85/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/nfs%E5%AE%89%E8%A3%85/</guid>
      <description>nfs安装
1. 环境说明 角色 ip os NFS 服务端 172.31.0.12 CentOS 7.6 NFS 客户端 172.18.0.7 CentOS 7.6 2. NFS 服务端 2.1 安装 nfs-utils # rpcbind 作为依赖会自动安装。 &amp;gt; yum install nfs-utils 2.2 配置并启动服务 允许rpcbind.service、nfs.service开机自启：
# 启动相关服务 &amp;gt; systemctl start rpcbind &amp;gt; systemctl start nfs 防火墙允许服务通过：
# 防火墙允许服务通过 &amp;gt; firewall-cmd --zone=public --permanent --add-service={rpc-bind,mountd,nfs} success &amp;gt; firewall-cmd --reload success 或者直接关闭防火墙：systemctl stop firewalld
2.3 配置共享目录 例如需要共享的目录为/mnt/nfs/：
# 创建 /mnt/kvm 并修改权限 &amp;gt; cd /mnt /mnt &amp;gt; mkdir nfs /mnt &amp;gt; chmod 755 nfs # 验证目录权限 /mnt &amp;gt; ls -l total 0 drwxr-xr-x 2 root root 59 Oct 17 17:49 nfs 之后修改/etc/exports，将/mnt/nfs/添加进去：</description>
    </item>
    
    <item>
      <title>operator实战</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/operator%E5%AE%9E%E6%88%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/operator%E5%AE%9E%E6%88%98/</guid>
      <description>operator实战 1⃣️ 需求 我们将定义一个 crd ，spec 包含以下信息：
Replicas	# 副本数 Image	# 镜像 Resources	# 资源限制 Envs	# 环境变量 Ports	# 服务端口 2⃣️ 编码 初始化项目目录：
tony@192 k8s % pwd /Users/tony/workspace/k8s tony@192 k8s % mkdir -p app-operator/src/github.com/xuzhijvn/app cd app-operator/src/github.com/xuzhijvn/app 初始化operator项目结构，并创建api：
operator-sdk init --domain=huolala.cn --repo=github.com/xuzhijvn/app operator-sdk create api --group app --version v1 --kind App --resource=true --controller=true 修改 CRD 类型定义代码 api/v1/app_types.go:
/* Copyright 2021. Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;); you may not use this file except in compliance with the License.</description>
    </item>
    
    <item>
      <title>pvc不会主动回收问题</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/pvc%E4%B8%8D%E4%BC%9A%E4%B8%BB%E5%8A%A8%E5%9B%9E%E6%94%B6%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/pvc%E4%B8%8D%E4%BC%9A%E4%B8%BB%E5%8A%A8%E5%9B%9E%E6%94%B6%E9%97%AE%E9%A2%98/</guid>
      <description>pvc不会主动回收问题
使用k8s部署nacos的时候发现，通过volumeClaimTemplates配置的pvc不会随着使用kubectl delete -f xxx.yaml 删除StatefulSet的时候一同被删除。nacos-pvc-nfs.yaml文件如下所示：
--- apiVersion: v1 kind: Service metadata: name: nacos-headless labels: app: nacos annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: &amp;#34;true&amp;#34; spec: ports: - port: 8848 name: server targetPort: 8848 - port: 7848 name: rpc targetPort: 7848 clusterIP: None selector: app: nacos --- apiVersion: v1 kind: ConfigMap metadata: name: nacos-cm data: mysql.db.name: &amp;#34;nacos_devtest&amp;#34; mysql.port: &amp;#34;3306&amp;#34; mysql.user: &amp;#34;nacos&amp;#34; mysql.password: &amp;#34;nacos&amp;#34; --- apiVersion: apps/v1 kind: StatefulSet metadata: name: nacos spec: serviceName: nacos-headless replicas: 3 template: metadata: labels: app: nacos annotations: pod.</description>
    </item>
    
    <item>
      <title>从零搭建K8S</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BAk8s/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BAk8s/</guid>
      <description>从零搭建K8S
机器准备 名称 数量 IP 备注 master 1 172.17.0.14 操作系统: Linux(centos7, 其它操作系统也可, 安装过程类似, 可参考官方文档) 机器配置: 4C8G node1 1 172.18.0.7 同上 node2 1 172.19.0.5 同上 由于本人很穷，这几台机器是分别属于不同的腾讯云账号，不同的账号之间不能内网通信，不过可以通过建立“对等连接”实现通信，比直接用公网通信靠谱。
1. 修改hostname [root@k8s-master ~]$ vim /etc/hostname # 修改hostname [root@k8s-master ~]$ vim /etc/hosts	# 将本机IP指向hostname [root@k8s-master ~]$ reboot -h # 重启(可以做完全部前期准备后再重启) 修改后：
[root@k8s-master ~]# cat /etc/hosts ::1 VM_0_5_centos VM_0_5_centos ::1 localhost.localdomain localhost ::1 localhost6.localdomain6 localhost6 127.0.0.1 localhost localhost.localdomain k8s-master 172.17.0.14 k8s-master 172.18.0.7 k8s-node1 172.19.0.5 k8s-node2 2. 配置防火墙 笔者图方便, 直接关闭了防火墙.</description>
    </item>
    
    <item>
      <title>使用kubeadm更新k8s证书</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0k8s%E8%AF%81%E4%B9%A6/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E4%BD%BF%E7%94%A8kubeadm%E6%9B%B4%E6%96%B0k8s%E8%AF%81%E4%B9%A6/</guid>
      <description>使用kubeadm更新k8s证书 今天操作k8s的时候，突然说证书无效：
Unable to connect to the server: x509: certificate has expired or is not yet valid 通过 kubeadm alpha certs check-expiration 查看，确实是过期了：
[root@k8s-master ~]# kubeadm alpha certs check-expiration [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with &amp;#39;kubectl -n kube-system get cm kubeadm-config -oyaml&amp;#39; [check-expiration] Error reading configuration from the Cluster. Falling back to default configuration W0627 11:21:35.745166 8754 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.</description>
    </item>
    
    <item>
      <title>切换namespace</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%88%87%E6%8D%A2namespace/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%88%87%E6%8D%A2namespace/</guid>
      <description>切换namespace
0. 背景 k8s如何切换namespace？？而不必每次执行命令的时候在后面指定namespace？k8s并没有直接提供切换namespace的命令。不过可以通过：
切换context达到切换namespace的目的，这需要提前创建好context和namespace的绑定关系（如：kubectl config set-context test --namespace=test）,然后使用 kubectl config use-context test 切换context，从而间接的达到切换namespace的目的。这方法属实是太别扭了，强迫症患者都不喜欢。 或者使用kubectx工具，其本质是动态的修改context和namespace的绑定关系。使用形如：kubens test 即可切换。这样才是切换namespace的正确打开方式，优雅多了。 参考资料： Kubernetes命名空间
一条命令解决Kubernetes更改默认的namespace
Kubernetes 切换context和namespace
k8s集群namespace和context使用
ahmetb / kubectx
提高您的kubectl生产力（第三部分）：集群上下文切换、使用别名减少输入和插件扩展</description>
    </item>
    
    <item>
      <title>同步镜像到自己的仓库</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%90%8C%E6%AD%A5%E9%95%9C%E5%83%8F%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%93%E5%BA%93/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%90%8C%E6%AD%A5%E9%95%9C%E5%83%8F%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%93%E5%BA%93/</guid>
      <description>同步镜像到自己的仓库 经常有一些国外的镜像仓库，在国内无法拉取，此时我们可以先找台国外的服务器拉取下来，重新打tag后推送到自己的镜像仓库。
1⃣️ 免费服务器
https://labs.play-with-k8s.com/
上面k8s的服务器硬盘不够大，可能会拉镜像失败，建议用下面这个：
https://labs.play-with-docker.com/
2⃣️ 拉取-打tag-推送
3⃣️ 验证
参考链接🔗 如何拉取k8s.grc.io、quay.io的镜像</description>
    </item>
    
    <item>
      <title>在namespace之间共享secret</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%9C%A8namespace%E4%B9%8B%E9%97%B4%E5%85%B1%E4%BA%ABsecret/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E5%9C%A8namespace%E4%B9%8B%E9%97%B4%E5%85%B1%E4%BA%ABsecret/</guid>
      <description>在namespace之间共享secret
有的时候在default空间下创建了拉取镜像的secret，当部署k8s的资源到其他namespace的时候，如果部署的的是deployment的之类的势必会要拉取镜像，这个时候必然失败，因为secret创建在default空间下，所以我们需要将secret复制一份到需要的namespace下面：
kubectl get secret coding-regcred --namespace=default -oyaml | grep -v &amp;#39;^\s*namespace:\s&amp;#39; | kubectl apply --namespace=tkb -f - 参考链接：Kubernetes - sharing secret across namespaces</description>
    </item>
    
    <item>
      <title>本地连接k8s集群</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5k8s%E9%9B%86%E7%BE%A4/</guid>
      <description>本地连接k8s集群 在日常开发的过程中，经常会需要在本地开发的程序需要在k8s中调试的场景，比如，写了一个operator。
如果此时，本地又没有可以直接可达的k8s集群，
比如k8s是在公有云的vpc环境内，外面无法直接访问。想要本地连接远程k8s集群，可以参考 本地连接远程的内网k8s集群
再比如k8s集群是自己通过多台云服务器自行搭建的，master节点有自己的公网ip。想要本地连接远程k8s集群，可以参考本文。
1⃣️ 重新生成config文件 默认下，~/.kube/config 生成配置文件的时候只包含了k8s集群ip和这个节点的局域网ip，本地如果想远程操作k8s的话，必定要通过公网ip连接到k8s集群，所以我们需要把节点绑定的公网ip也放到证书里面去，即我们需要重新生成证书。如果不这样做，本地直接访问的话，会报如下提示：
tony@192 ~ % kubectl get pod Unable to connect to the server: x509: certificate is valid for 10.96.0.1, 172.17.0.14, not 106.55.152.92 先备份证书：
[root@k8s-master .kube]# mkdir -p /etc/kubernetes/pki.bak [root@k8s-master .kube]# mv /etc/kubernetes/pki/apiserver.* /etc/kubernetes/pki.bak 重新生成证书：
[root@k8s-master .kube]# kubeadm init phase certs all --apiserver-advertise-address=0.0.0.0 --apiserver-cert-extra-sans=10.96.0.1,172.17.0.14,xxx.xxx.xxx.xxx(公网ip) I0627 15:10:39.069106 7777 version.go:252] remote version is much newer: v1.21.2; falling back to: stable-1.18 W0627 15:10:40.982380 7777 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.</description>
    </item>
    
    <item>
      <title>部署sentinel-dashboard</title>
      <link>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E9%83%A8%E7%BD%B2sentinel-dashboard/</link>
      <pubDate>Fri, 27 Aug 2021 11:15:10 +0800</pubDate>
      
      <guid>https://xuzhijvn.github.io/en/zh-cn/posts/k8s/%E5%AE%9E%E6%88%98/%E9%83%A8%E7%BD%B2sentinel-dashboard/</guid>
      <description>部署sentinel-dashboard
1. 构建镜像 FROM openjdk:8-jdk-alpine VOLUME /tmp ADD sentinel-dashboard-1.8.0.jar sentinel-dashboard.jar CMD java ${JAVA_OPTS} -jar sentinel-dashboard.jar EXPOSE 8718 2. 创建headless service apiVersion: v1 kind: Service metadata: name: sentinel-headless labels: app: sentinel annotations: service.alpha.kubernetes.io/tolerate-unready-endpoints: &amp;#34;true&amp;#34; spec: ports: - port: 8718 name: server targetPort: 8718 clusterIP: None selector: app: sentinel 3. 创建StatefulSet apiVersion: apps/v1 kind: StatefulSet metadata: name: sentinel spec: serviceName: sentinel replicas: 1 template: metadata: labels: app: sentinel annotations: pod.alpha.kubernetes.io/initialized: &amp;#34;true&amp;#34; spec: containers: - name: sentinel imagePullPolicy: IfNotPresent image: ccr.</description>
    </item>
    
  </channel>
</rss>
